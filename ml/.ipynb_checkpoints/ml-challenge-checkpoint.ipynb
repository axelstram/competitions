{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from bert_serving.client import BertClient\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "from os import listdir\n",
    "import re\n",
    "import fasttext\n",
    "import unicodedata\n",
    "import io\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "# import optuna\n",
    "# import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_momentum(optimizer, mom_val):\n",
    "    \"\"\"\n",
    "    Helper to set momentum of Keras optimizers.\n",
    "    :param optimizer: Keras optimizer\n",
    "    :param mom_val: value of momentum.\n",
    "    \"\"\"\n",
    "    keys = dir(optimizer)\n",
    "    if \"momentum\" in keys:\n",
    "        tf.keras.backend.set_value(optimizer.momentum, mom_val)\n",
    "    if \"rho\" in keys:\n",
    "        tf.keras.backend.set_value(optimizer.rho, mom_val)\n",
    "    if \"beta_1\" in keys:\n",
    "        tf.keras.backend.set_value(optimizer.beta_1, mom_val)\n",
    "\n",
    "\n",
    "def set_lr(optimizer, lr):\n",
    "    \"\"\"\n",
    "    Helper to set learning rate of Keras optimizers.\n",
    "    :param optimizer: Keras optimizer\n",
    "    :param lr: value of learning rate.\n",
    "    \"\"\"\n",
    "    tf.keras.backend.set_value(optimizer.lr, lr)\n",
    "\n",
    "class OneCycle(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A callback class for one-cycle policy training.\n",
    "    :param lr_range: a tuple of starting (usually minimum) lr value and maximum (peak) lr value.\n",
    "    :param momentum_range: a tuple of momentum values.\n",
    "    :param phase_one_fraction: a fraction for phase I (increasing lr) in one cycle. Must between 0 to 1.\n",
    "    :param reset_on_train_begin: True or False to reset counters when training begins.\n",
    "    :param record_frq: integer > 0, a frequency in batches to record training loss.\n",
    "    :param verbose: True or False to print progress.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            lr_range,\n",
    "            momentum_range=None,\n",
    "            phase_one_fraction=0.3,\n",
    "            reset_on_train_begin=True,\n",
    "            record_frq=10,\n",
    "            verbose=False):\n",
    "\n",
    "        super(OneCycle, self).__init__()\n",
    "\n",
    "        self.lr_range = lr_range\n",
    "\n",
    "        self.momentum_range = momentum_range\n",
    "        if momentum_range is not None:\n",
    "            err_msg = \"momentum_range must be a 2-numeric tuple (m1, m2).\"\n",
    "            if not isinstance(momentum_range, (tuple,)) or len(momentum_range) != 2:\n",
    "                raise ValueError(err_msg)\n",
    "\n",
    "        self.phase_one_fraction = phase_one_fraction\n",
    "        self.reset_on_train_begin = reset_on_train_begin\n",
    "        self.record_frq = record_frq\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # helper tracker\n",
    "        self.log = {}  # history in iterations\n",
    "        self.log_ep = {}  # history in epochs\n",
    "        self.stop_training = False\n",
    "\n",
    "        # counter\n",
    "        self.current_iter = 0\n",
    "\n",
    "    def get_current_lr(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        A helper function to calculate a current learning rate based on current iteration number.\n",
    "        :return lr: a current learning rate.\n",
    "        \"\"\"\n",
    "        if n_iter is None:\n",
    "            n_iter = self.n_iter\n",
    "\n",
    "        x = float(self.current_iter) / n_iter\n",
    "        if x < self.phase_one_fraction:\n",
    "            amp = self.lr_range[1] - self.lr_range[0]\n",
    "            lr = (np.cos(x * np.pi/self.phase_one_fraction - np.pi) + 1) * amp / 2.0 + self.lr_range[0]\n",
    "        if x >= self.phase_one_fraction:\n",
    "            amp = self.lr_range[1]\n",
    "            lr = (np.cos((x - self.phase_one_fraction) * np.pi/ (1-self.phase_one_fraction)) + 1) / 2.0 * amp\n",
    "        return lr\n",
    "\n",
    "    def get_current_momentum(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        A helper function to calculate a current momentum based on current iteration number.\n",
    "        :return momentum: a current momentum.\n",
    "        \"\"\"\n",
    "        if n_iter is None:\n",
    "            n_iter = self.n_iter\n",
    "        amp = self.momentum_range[1] - self.momentum_range[0]\n",
    "        # delta = (1 - np.abs(np.mod(self.current_iter, n_iter) * 2.0 / n_iter - 1)) * amplitude\n",
    "        x = float(self.current_iter) / n_iter\n",
    "        if x < self.phase_one_fraction:\n",
    "            delta = (np.cos(x * np.pi / self.phase_one_fraction - np.pi) + 1) * amp / 2.0\n",
    "        if x >= self.phase_one_fraction:\n",
    "            delta = (np.cos((x - self.phase_one_fraction) * np.pi / (1 - self.phase_one_fraction)) + 1) / 2.0 * amp\n",
    "        return delta + self.momentum_range[0]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def cycle_momentum(self):\n",
    "        return self.momentum_range is not None\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.n_epoch = self.params['epochs']\n",
    "\n",
    "        # find number of batches per epoch\n",
    "        if self.params['batch_size'] is not None:  # model.fit\n",
    "            self.n_bpe = int(np.ceil(self.params['samples'] / self.params['batch_size']))\n",
    "        if self.params['batch_size'] is None:  # model.fit_generator\n",
    "            self.n_bpe = self.params['samples']\n",
    "\n",
    "        self.n_iter = self.n_epoch * self.n_bpe\n",
    "        # this is a number of iteration in one cycle\n",
    "\n",
    "        self.current_iter = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs={}):\n",
    "        set_lr(self.model.optimizer, self.get_current_lr())\n",
    "        if self.cycle_momentum:\n",
    "            set_momentum(self.model.optimizer, self.get_current_momentum())\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"lr={:.2e}\".format(self.get_current_lr()), \",\", \"m={:.2e}\".format(self.get_current_momentum()))\n",
    "\n",
    "        # record according to record_frq\n",
    "        if np.mod(int(self.current_iter), self.record_frq) == 0:\n",
    "            self.log.setdefault('lr', []).append(self.get_current_lr())\n",
    "            if self.cycle_momentum:\n",
    "                self.log.setdefault('momentum', []).append(self.get_current_momentum())\n",
    "\n",
    "            for k, v in logs.items():\n",
    "                self.log.setdefault(k, []).append(v)\n",
    "\n",
    "            self.log.setdefault('iter', []).append(self.current_iter)\n",
    "\n",
    "        # update current iteration\n",
    "        self.current_iter += 1\n",
    "\n",
    "        # consider termination\n",
    "        if self.current_iter == self.n_iter:\n",
    "            self.stop_training = True\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.log_ep.setdefault('epoch', []).append(epoch)\n",
    "        self.log_ep.setdefault('lr', []).append(\n",
    "            tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.log_ep.setdefault(k, []).append(v)\n",
    "\n",
    "    def test_run(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        Visualize values of learning rate (and momentum) as a function of iteration (batch).\n",
    "        :param n_iter: a number of cycles. If None, 1000 is used.\n",
    "        \"\"\"\n",
    "\n",
    "        if hasattr(self, 'current_iter'):\n",
    "            original_it = self.current_iter\n",
    "\n",
    "        if n_iter is None:\n",
    "            if hasattr(self, 'n_iter'):\n",
    "                n_iter = self.n_iter\n",
    "            else:\n",
    "                n_iter = 1000\n",
    "        n_iter = int(n_iter)\n",
    "\n",
    "        lrs = np.zeros(shape=(n_iter,))\n",
    "        if self.momentum_range is not None:\n",
    "            moms = np.zeros_like(lrs)\n",
    "\n",
    "        for i in range(int(n_iter)):\n",
    "            self.current_iter = i\n",
    "            lrs[i] = self.get_current_lr(n_iter)\n",
    "            if self.cycle_momentum:\n",
    "                moms[i] = self.get_current_momentum(n_iter)\n",
    "        if not self.cycle_momentum:\n",
    "            plt.plot(lrs)\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('lr')\n",
    "        else:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(lrs)\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('lr')\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(moms)\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('momentum')\n",
    "\n",
    "        if hasattr(self, 'current_iter'):\n",
    "            self.current_iter = original_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,\n",
    "                 num_samples,\n",
    "                 batch_size,\n",
    "                 minimum_lr=1e-5,\n",
    "                 maximum_lr=10.,\n",
    "                 lr_scale='exp',\n",
    "                 validation_data=None,\n",
    "                 validation_sample_rate=5,\n",
    "                 stopping_criterion_factor=4.,\n",
    "                 loss_smoothing_beta=0.98,\n",
    "                 save_dir=None,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        This class uses the Cyclic Learning Rate history to find a\n",
    "        set of learning rates that can be good initializations for the\n",
    "        One-Cycle training proposed by Leslie Smith in the paper referenced\n",
    "        below.\n",
    "\n",
    "        A port of the Fast.ai implementation for Keras.\n",
    "\n",
    "        # Note\n",
    "        This requires that the model be trained for exactly 1 epoch. If the model\n",
    "        is trained for more epochs, then the metric calculations are only done for\n",
    "        the first epoch.\n",
    "\n",
    "        # Interpretation\n",
    "        Upon visualizing the loss plot, check where the loss starts to increase\n",
    "        rapidly. Choose a learning rate at somewhat prior to the corresponding\n",
    "        position in the plot for faster convergence. This will be the maximum_lr lr.\n",
    "        Choose the max value as this value when passing the `max_val` argument\n",
    "        to OneCycleLR callback.\n",
    "\n",
    "        Since the plot is in log-scale, you need to compute 10 ^ (-k) of the x-axis\n",
    "\n",
    "        # Arguments:\n",
    "            num_samples: Integer. Number of samples in the dataset.\n",
    "            batch_size: Integer. Batch size during training.\n",
    "            minimum_lr: Float. Initial learning rate (and the minimum).\n",
    "            maximum_lr: Float. Final learning rate (and the maximum).\n",
    "            lr_scale: Can be one of ['exp', 'linear']. Chooses the type of\n",
    "                scaling for each update to the learning rate during subsequent\n",
    "                batches. Choose 'exp' for large range and 'linear' for small range.\n",
    "            validation_data: Requires the validation dataset as a tuple of\n",
    "                (X, y) belonging to the validation set. If provided, will use the\n",
    "                validation set to compute the loss metrics. Else uses the training\n",
    "                batch loss. Will warn if not provided to alert the user.\n",
    "            validation_sample_rate: Positive or Negative Integer. Number of batches to sample from the\n",
    "                validation set per iteration of the LRFinder. Larger number of\n",
    "                samples will reduce the variance but will take longer time to execute\n",
    "                per batch.\n",
    "\n",
    "                If Positive > 0, will sample from the validation dataset\n",
    "                If Megative, will use the entire dataset\n",
    "            stopping_criterion_factor: Integer or None. A factor which is used\n",
    "                to measure large increase in the loss value during training.\n",
    "                Since callbacks cannot stop training of a model, it will simply\n",
    "                stop logging the additional values from the epochs after this\n",
    "                stopping criterion has been met.\n",
    "                If None, this check will not be performed.\n",
    "            loss_smoothing_beta: Float. The smoothing factor for the moving\n",
    "                average of the loss function.\n",
    "            save_dir: Optional, String. If passed a directory path, the callback\n",
    "                will save the running loss and learning rates to two separate numpy\n",
    "                arrays inside this directory. If the directory in this path does not\n",
    "                exist, they will be created.\n",
    "            verbose: Whether to print the learning rate after every batch of training.\n",
    "\n",
    "        # References:\n",
    "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
    "        \"\"\"\n",
    "        super(LRFinder, self).__init__()\n",
    "\n",
    "        if lr_scale not in ['exp', 'linear']:\n",
    "            raise ValueError(\"`lr_scale` must be one of ['exp', 'linear']\")\n",
    "\n",
    "        if validation_data is not None:\n",
    "            self.validation_data = validation_data\n",
    "            self.use_validation_set = True\n",
    "\n",
    "            if validation_sample_rate > 0 or validation_sample_rate < 0:\n",
    "                self.validation_sample_rate = validation_sample_rate\n",
    "            else:\n",
    "                raise ValueError(\"`validation_sample_rate` must be a positive or negative integer other than o\")\n",
    "        else:\n",
    "            self.use_validation_set = False\n",
    "            self.validation_sample_rate = 0\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.initial_lr = minimum_lr\n",
    "        self.final_lr = maximum_lr\n",
    "        self.lr_scale = lr_scale\n",
    "        self.stopping_criterion_factor = stopping_criterion_factor\n",
    "        self.loss_smoothing_beta = loss_smoothing_beta\n",
    "        self.save_dir = save_dir\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.num_batches_ = num_samples // batch_size\n",
    "        self.current_lr_ = minimum_lr\n",
    "\n",
    "        if lr_scale == 'exp':\n",
    "            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (\n",
    "                1. / float(self.num_batches_))\n",
    "        else:\n",
    "            extra_batch = int((num_samples % batch_size) != 0)\n",
    "            self.lr_multiplier_ = np.linspace(\n",
    "                minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n",
    "\n",
    "        # If negative, use entire validation set\n",
    "        if self.validation_sample_rate < 0:\n",
    "            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n",
    "\n",
    "        self.current_batch_ = 0\n",
    "        self.current_epoch_ = 0\n",
    "        self.best_loss_ = 1e6\n",
    "        self.running_loss_ = 0.\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "\n",
    "        self.current_epoch_ = 1\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, self.initial_lr)\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_batch_ = 0\n",
    "\n",
    "        if self.current_epoch_ > 1:\n",
    "            warnings.warn(\n",
    "                \"\\n\\nLearning rate finder should be used only with a single epoch. \"\n",
    "                \"Hereafter, the callback will not measure the losses.\\n\\n\")\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.current_batch_ += 1\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.current_epoch_ > 1:\n",
    "            return\n",
    "\n",
    "        if self.use_validation_set:\n",
    "            X, Y = self.validation_data[0], self.validation_data[1]\n",
    "\n",
    "            # use 5 random batches from test set for fast approximate of loss\n",
    "            num_samples = self.batch_size * self.validation_sample_rate\n",
    "\n",
    "            if num_samples > X.shape[0]:\n",
    "                num_samples = X.shape[0]\n",
    "\n",
    "            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n",
    "            x = X[idx]\n",
    "            y = Y[idx]\n",
    "\n",
    "            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n",
    "            loss = values[0]\n",
    "        else:\n",
    "            loss = logs['loss']\n",
    "\n",
    "        # smooth the loss value and bias correct\n",
    "        running_loss = self.loss_smoothing_beta * loss + (\n",
    "            1. - self.loss_smoothing_beta) * loss\n",
    "        running_loss = running_loss / (\n",
    "            1. - self.loss_smoothing_beta**self.current_batch_)\n",
    "\n",
    "        # stop logging if loss is too large\n",
    "        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n",
    "                running_loss >\n",
    "                self.stopping_criterion_factor * self.best_loss_):\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)\"\n",
    "                      % (self.stopping_criterion_factor, self.best_loss_))\n",
    "            return\n",
    "\n",
    "        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n",
    "            self.best_loss_ = running_loss\n",
    "\n",
    "        current_lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        self.history.setdefault('running_loss_', []).append(running_loss)\n",
    "        if self.lr_scale == 'exp':\n",
    "            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n",
    "        else:\n",
    "            self.history.setdefault('log_lrs', []).append(current_lr)\n",
    "\n",
    "        # compute the lr for the next batch and update the optimizer lr\n",
    "        if self.lr_scale == 'exp':\n",
    "            current_lr *= self.lr_multiplier_\n",
    "        else:\n",
    "            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n",
    "\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, current_lr)\n",
    "\n",
    "        # save the other metrics as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        if self.verbose:\n",
    "            if self.use_validation_set:\n",
    "                print(\" - LRFinder: val_loss: %1.4f - lr = %1.8f \" %\n",
    "                      (values[0], current_lr))\n",
    "            else:\n",
    "                print(\" - LRFinder: lr = %1.8f \" % current_lr)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.save_dir is not None and self.current_epoch_ <= 1:\n",
    "            if not os.path.exists(self.save_dir):\n",
    "                os.makedirs(self.save_dir)\n",
    "\n",
    "            losses_path = os.path.join(self.save_dir, 'losses.npy')\n",
    "            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n",
    "\n",
    "            np.save(losses_path, self.losses)\n",
    "            np.save(lrs_path, self.lrs)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"\\tLR Finder : Saved the losses and learning rate values in path : {%s}\"\n",
    "                      % (self.save_dir))\n",
    "\n",
    "        self.current_epoch_ += 1\n",
    "\n",
    "        warnings.simplefilter(\"default\")\n",
    "\n",
    "    def plot_schedule(self, clip_beginning=None, clip_endding=None):\n",
    "        \"\"\"\n",
    "        Plots the schedule from the callback itself.\n",
    "\n",
    "        # Arguments:\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.style.use('seaborn-white')\n",
    "        except ImportError:\n",
    "            print(\n",
    "                \"Matplotlib not found. Please use `pip install matplotlib` first.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if clip_beginning is not None and clip_beginning < 0:\n",
    "            clip_beginning = -clip_beginning\n",
    "\n",
    "        if clip_endding is not None and clip_endding > 0:\n",
    "            clip_endding = -clip_endding\n",
    "\n",
    "        losses = self.losses\n",
    "        lrs = self.lrs\n",
    "\n",
    "        if clip_beginning:\n",
    "            losses = losses[clip_beginning:]\n",
    "            lrs = lrs[clip_beginning:]\n",
    "\n",
    "        if clip_endding:\n",
    "            losses = losses[:clip_endding]\n",
    "            lrs = lrs[:clip_endding]\n",
    "\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.title('Learning rate vs Loss')\n",
    "        plt.xlabel('learning rate')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def restore_schedule_from_dir(cls,\n",
    "                                  directory,\n",
    "                                  clip_beginning=None,\n",
    "                                  clip_endding=None):\n",
    "        \"\"\"\n",
    "        Loads the training history from the saved numpy files in the given directory.\n",
    "\n",
    "        # Arguments:\n",
    "            directory: String. Path to the directory where the serialized numpy\n",
    "                arrays of the loss and learning rates are saved.\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "\n",
    "        Returns:\n",
    "            tuple of (losses, learning rates)\n",
    "        \"\"\"\n",
    "        if clip_beginning is not None and clip_beginning < 0:\n",
    "            clip_beginning = -clip_beginning\n",
    "\n",
    "        if clip_endding is not None and clip_endding > 0:\n",
    "            clip_endding = -clip_endding\n",
    "\n",
    "        losses_path = os.path.join(directory, 'losses.npy')\n",
    "        lrs_path = os.path.join(directory, 'lrs.npy')\n",
    "\n",
    "        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n",
    "            print(\"%s and %s could not be found at directory : {%s}\" %\n",
    "                  (losses_path, lrs_path, directory))\n",
    "\n",
    "            losses = None\n",
    "            lrs = None\n",
    "\n",
    "        else:\n",
    "            losses = np.load(losses_path)\n",
    "            lrs = np.load(lrs_path)\n",
    "\n",
    "            if clip_beginning:\n",
    "                losses = losses[clip_beginning:]\n",
    "                lrs = lrs[clip_beginning:]\n",
    "\n",
    "            if clip_endding:\n",
    "                losses = losses[:clip_endding]\n",
    "                lrs = lrs[:clip_endding]\n",
    "\n",
    "        return losses, lrs\n",
    "\n",
    "    @classmethod\n",
    "    def plot_schedule_from_file(cls,\n",
    "                                directory,\n",
    "                                clip_beginning=None,\n",
    "                                clip_endding=None):\n",
    "        \"\"\"\n",
    "        Plots the schedule from the saved numpy arrays of the loss and learning\n",
    "        rate values in the specified directory.\n",
    "\n",
    "        # Arguments:\n",
    "            directory: String. Path to the directory where the serialized numpy\n",
    "                arrays of the loss and learning rates are saved.\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.style.use('seaborn-white')\n",
    "        except ImportError:\n",
    "            print(\"Matplotlib not found. Please use `pip install matplotlib` first.\")\n",
    "            return\n",
    "\n",
    "        losses, lrs = cls.restore_schedule_from_dir(\n",
    "            directory,\n",
    "            clip_beginning=clip_beginning,\n",
    "            clip_endding=clip_endding)\n",
    "\n",
    "        if losses is None or lrs is None:\n",
    "            return\n",
    "        else:\n",
    "            plt.plot(lrs, losses)\n",
    "            plt.title('Learning rate vs Loss')\n",
    "            plt.xlabel('learning rate')\n",
    "            plt.ylabel('loss')\n",
    "            plt.show()\n",
    "\n",
    "    @property\n",
    "    def lrs(self):\n",
    "        return np.array(self.history['log_lrs'])\n",
    "\n",
    "    @property\n",
    "    def losses(self):\n",
    "        return np.array(self.history['running_loss_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead(tf.keras.optimizers.Optimizer):\n",
    "    \"\"\"The lookahead mechanism for optimizers.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        optimizer: An existed optimizer.\n",
    "        sync_period: int > 0. The synchronization period.\n",
    "        slow_step: float, 0 < alpha < 1. The step size of slow weights.\n",
    "    # References\n",
    "        - [Lookahead Optimizer: k steps forward, 1 step back]\n",
    "          (https://arxiv.org/pdf/1907.08610v1.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, sync_period=5, slow_step=0.5, **kwargs):\n",
    "        super(Lookahead, self).__init__(**kwargs)\n",
    "        self.optimizer = tf.keras.optimizers.get(optimizer)\n",
    "        with tf.keras.backend.name_scope(self.__class__.__name__):\n",
    "            self.sync_period = tf.keras.backend.variable(sync_period, dtype='int64', name='sync_period')\n",
    "            self.slow_step = tf.keras.backend.variable(slow_step, name='slow_step')\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self.optimizer.lr\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, lr):\n",
    "        self.optimizer.lr = lr\n",
    "\n",
    "    @property\n",
    "    def iterations(self):\n",
    "        return self.optimizer.iterations\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        sync_cond = tf.keras.backend.equal((self.iterations + 1) % self.sync_period, 0)\n",
    "        if True:\n",
    "            slow_params = [tf.keras.backend.variable(tf.keras.backend.get_value(p), name='sp_{}'.format(i)) for i, p in enumerate(params)]\n",
    "            self.updates = self.optimizer.get_updates(loss, params)\n",
    "            slow_updates = []\n",
    "            for p, sp in zip(params, slow_params):\n",
    "                sp_t = sp + self.slow_step * (p - sp)\n",
    "                slow_updates.append(tf.keras.backend.update(sp, tf.keras.backend.switch(\n",
    "                    sync_cond,\n",
    "                    sp_t,\n",
    "                    sp,\n",
    "                )))\n",
    "                slow_updates.append(tf.keras.backend.update_add(p, tf.keras.backend.switch(\n",
    "                    sync_cond,\n",
    "                    sp_t - p,\n",
    "                    tf.keras.backend.zeros_like(p),\n",
    "                )))\n",
    "        else:\n",
    "            slow_params = {p.name: tf.keras.backend.variable(tf.keras.backend.get_value(p), name='sp_{}'.format(i)) for i, p in enumerate(params)}\n",
    "            update_names = ['update', 'update_add', 'update_sub']\n",
    "            original_updates = [getattr(K, name) for name in update_names]\n",
    "            setattr(K, 'update', lambda x, new_x: ('update', x, new_x))\n",
    "            setattr(K, 'update_add', lambda x, new_x: ('update_add', x, new_x))\n",
    "            setattr(K, 'update_sub', lambda x, new_x: ('update_sub', x, new_x))\n",
    "            self.updates = self.optimizer.get_updates(loss, params)\n",
    "            for name, original_update in zip(update_names, original_updates):\n",
    "                setattr(K, name, original_update)\n",
    "            slow_updates = []\n",
    "            for i, update in enumerate(self.updates):\n",
    "                if isinstance(update, tuple):\n",
    "                    name, x, new_x, adjusted = update + (update[-1],)\n",
    "                    update_func = getattr(K, name)\n",
    "                    if name == 'update_add':\n",
    "                        adjusted = x + new_x\n",
    "                    if name == 'update_sub':\n",
    "                        adjusted = x - new_x\n",
    "                    if x.name not in slow_params:\n",
    "                        self.updates[i] = update_func(x, new_x)\n",
    "                    else:\n",
    "                        slow_param = slow_params[x.name]\n",
    "                        slow_param_t = slow_param + self.slow_step * (adjusted - slow_param)\n",
    "                        slow_updates.append(tf.keras.backend.update(slow_param, tf.keras.backend.switch(\n",
    "                            sync_cond,\n",
    "                            slow_param_t,\n",
    "                            slow_param,\n",
    "                        )))\n",
    "                        self.updates[i] = tf.keras.backend.update(x, tf.keras.backend.switch(\n",
    "                            sync_cond,\n",
    "                            slow_param_t,\n",
    "                            adjusted,\n",
    "                        ))\n",
    "            slow_params = list(slow_params.values())\n",
    "        self.updates += slow_updates\n",
    "        self.weights = self.optimizer.weights + slow_params\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'optimizer': tf.keras.optimizers.serialize(self.optimizer),\n",
    "            'sync_period': int(tf.keras.backend.get_value(self.sync_period)),\n",
    "            'slow_step': float(tf.keras.backend.get_value(self.slow_step)),\n",
    "        }\n",
    "        base_config = super(Lookahead, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        optimizer = tf.keras.optimizers.deserialize(config.pop('optimizer'))\n",
    "        return cls(optimizer, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ported from https://github.com/LiyuanLucasLiu/RAdam/blob/master/radam.py\n",
    "class RectifiedAdam(tf.keras.optimizers.Optimizer):\n",
    "    \"\"\"RectifiedAdam optimizer.\n",
    "\n",
    "    Default parameters follow those provided in the original paper.\n",
    "\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        final_lr: float >= 0. Final learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        gamma: float >= 0. Convergence speed of the bound function.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `tf.keras.backend.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: Weight decay weight.\n",
    "        amsbound: boolean. Whether to apply the AMSBound variant of this\n",
    "            algorithm.\n",
    "\n",
    "    # References\n",
    "        - [On the Variance of the Adaptive Learning Rate and Beyond]\n",
    "          (https://arxiv.org/abs/1908.03265)\n",
    "        - [Adam - A Method for Stochastic Optimization]\n",
    "          (https://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond]\n",
    "          (https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., weight_decay=0.0, **kwargs):\n",
    "        super(RectifiedAdam, self).__init__(**kwargs)\n",
    "\n",
    "        with tf.keras.backend.name_scope(self.__class__.__name__):\n",
    "            self.iterations = tf.keras.backend.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = tf.keras.backend.variable(lr, name='lr')\n",
    "            self.beta_1 = tf.keras.backend.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = tf.keras.backend.variable(beta_2, name='beta_2')\n",
    "            self.decay = tf.keras.backend.variable(decay, name='decay')\n",
    "\n",
    "        if epsilon is None:\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "        self.weight_decay = float(weight_decay)\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [tf.keras.backend.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * tf.keras.backend.cast(self.iterations,\n",
    "                                                      tf.keras.backend.dtype(self.decay))))\n",
    "\n",
    "        t = tf.keras.backend.cast(self.iterations, tf.keras.backend.floatx()) + 1\n",
    "\n",
    "        ms = [tf.keras.backend.zeros(tf.keras.backend.int_shape(p), dtype=tf.keras.backend.dtype(p)) for p in params]\n",
    "        vs = [tf.keras.backend.zeros(tf.keras.backend.int_shape(p), dtype=tf.keras.backend.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * tf.keras.backend.square(g)\n",
    "\n",
    "            beta2_t = self.beta_2 ** t\n",
    "            N_sma_max = 2 / (1 - self.beta_2) - 1\n",
    "            N_sma = N_sma_max - 2 * t * beta2_t / (1 - beta2_t)\n",
    "\n",
    "            # apply weight decay\n",
    "            if self.weight_decay != 0.:\n",
    "                p_wd = p - self.weight_decay * lr * p\n",
    "            else:\n",
    "                p_wd = None\n",
    "\n",
    "            if p_wd is None:\n",
    "                p_ = p\n",
    "            else:\n",
    "                p_ = p_wd\n",
    "\n",
    "            def gt_path():\n",
    "                step_size = lr * tf.keras.backend.sqrt(\n",
    "                    (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max /\n",
    "                    (N_sma_max - 2)) / (1 - self.beta_1 ** t)\n",
    "\n",
    "                denom = tf.keras.backend.sqrt(v_t) + self.epsilon\n",
    "                p_t = p_ - step_size * (m_t / denom)\n",
    "\n",
    "                return p_t\n",
    "\n",
    "            def lt_path():\n",
    "                step_size = lr / (1 - self.beta_1 ** t)\n",
    "                p_t = p_ - step_size * m_t\n",
    "\n",
    "                return p_t\n",
    "\n",
    "            p_t = tf.keras.backend.switch(N_sma > 5, gt_path, lt_path)\n",
    "\n",
    "            self.updates.append(tf.keras.backend.update(m, m_t))\n",
    "            self.updates.append(tf.keras.backend.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(tf.keras.backend.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(tf.keras.backend.get_value(self.lr)),\n",
    "                  'beta_1': float(tf.keras.backend.get_value(self.beta_1)),\n",
    "                  'beta_2': float(tf.keras.backend.get_value(self.beta_2)),\n",
    "                  'decay': float(tf.keras.backend.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'weight_decay': self.weight_decay}\n",
    "        base_config = super(RectifiedAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, labels, encoder, test_size=0.2):\n",
    "    encodings = np.load(path)\n",
    "    X = np.expand_dims(encodings, 2)\n",
    "    y = encoder.transform(labels)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=16)\n",
    "    \n",
    "    del encodings\n",
    "    del X\n",
    "    gc.collect()\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_balanced_accuracy(X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "    \n",
    "    cl = classification_report(y_val, y_pred, output_dict=True)\n",
    "\n",
    "    recall_c = 0\n",
    "\n",
    "    for key in cl.keys():\n",
    "        recall_c += cl[key]['recall']\n",
    "\n",
    "    return recall_c / categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_balanced_accuracy_chunks(model, X_val, y_val, mc_dropout=False):\n",
    "    y_preds = np.array([])\n",
    "    \n",
    "    if mc_dropout is False:\n",
    "        for chunk in chunks(X_val, 1000000):\n",
    "            y_pred = model.predict(chunk)\n",
    "            y_pred = np.argmax(y_pred, axis=-1)\n",
    "            y_preds = np.append(y_preds, y_pred)\n",
    "    else:\n",
    "        num_models = 4\n",
    "        for chunk in chunks(X_val, int(1000000/num_models)):\n",
    "            y_probas = np.stack([model.predict(chunk) for sample in range(num_models)])\n",
    "            y_proba = y_probas.mean(axis=0)\n",
    "            y_proba = np.argmax(y_proba, axis=-1)\n",
    "            y_preds = np.append(y_preds, y_proba)\n",
    "            del y_probas\n",
    "            gc.collect()\n",
    "        \n",
    "    cl = classification_report(y_val, y_preds, output_dict=True)\n",
    "\n",
    "    recall_c = 0\n",
    "\n",
    "    for key in cl.keys():\n",
    "        recall_c += cl[key]['recall']\n",
    "\n",
    "    return recall_c / categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(tf.keras.callbacks.Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency.\n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored\n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or\n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "\n",
    "    # Example for CIFAR-10 w/ batch size 100:\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "\n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "\n",
    "    # References\n",
    "\n",
    "      - [Cyclical Learning Rates for Training Neural Networks](\n",
    "      https://arxiv.org/abs/1506.01186)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_lr=0.001,\n",
    "            max_lr=0.006,\n",
    "            step_size=2000.,\n",
    "            mode='triangular',\n",
    "            gamma=1.,\n",
    "            scale_fn=None,\n",
    "            scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2',\n",
    "                        'exp_range']:\n",
    "            raise KeyError(\"mode must be one of 'triangular', \"\n",
    "                           \"'triangular2', or 'exp_range'\")\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1 / (2.**(x - 1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma ** x\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr is not None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr is not None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size is not None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
    "                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
    "                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "        self.history.setdefault(\n",
    "            'lr', []).append(\n",
    "            tf.keras.backend.get_value(\n",
    "                self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = tf.keras.backend.get_value(self.model.optimizer.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname, vocab_size=None):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    i = 0\n",
    "    \n",
    "    for line in fin:\n",
    "        #tokens = line.rstrip().split(' ')\n",
    "        #data[tokens[0]] = map(float, tokens[1:])\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float16')\n",
    "        data[word] = coefs\n",
    "        \n",
    "        i+= 1\n",
    "        \n",
    "        if vocab_size is not None and i == vocab_size:\n",
    "            break\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_title(title):\n",
    "    return unicodedata.normalize('NFKD', title.lower()).encode('ASCII', 'ignore').decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, name):\n",
    "    with open(name + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(name):\n",
    "    with open(name + '.pickle', 'rb') as handle:\n",
    "        obj = pickle.load(handle)\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    model.save('model_' + str(name))\n",
    "    model.save_weights('model_weights_' + str(name))\n",
    "    \n",
    "def load_model(model, name):\n",
    "    model.load_weights('model_weights_' + str(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/axel/ssd/ml-challenge/'\n",
    "df = pd.read_csv(path + 'train_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = set(stopwords.words('spanish')).union(set(stopwords.words('portuguese')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(path + 'train.csv')\n",
    "# df['title'] = df.title.apply(normalize_title)\n",
    "# df = df[~df.title.isna() & (df.title != 'nan') & (df.title != '')]\n",
    "\n",
    "# df['title'] = df['title'].str.split(' ').apply(lambda sentence: ' '.join(word for word in sentence if word not in stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(path + 'test_processed.csv')\n",
    "# df_test = pd.read_csv(path + 'test.csv')\n",
    "# df_test['title'] = df_test.title.apply(normalize_title)\n",
    "# df_test['title'] = df_test['title'].str.split(' ').apply(lambda sentence: ' '.join(word for word in sentence if word not in stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc = BertClient()\n",
    "# titles = df['title'].iloc[19000000:].str.lower().values.tolist()\n",
    "# encodings = bc.encode(titles)\n",
    "# np.save('/media/axel/ssd/ml-challenge/large/encodings_3_layers_19', encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_1 = 'encodings_0.npy'\n",
    "# enc_2 = 'encodings_1.npy'\n",
    "# new_enc = 'encodings_3_layers_0-2.npy'\n",
    "# encodings1 = np.load('/media/axel/ssd/ml-challenge/' + enc_1)\n",
    "# encodings2 = np.load('/media/axel/ssd/ml-challenge/' + enc_2)\n",
    "# encodings = np.vstack((encodings1, encodings2))\n",
    "# np.save('/media/axel/ssd/ml-challenge/' + new_enc, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('/media/axel/ssd/ml-challenge/encodings_1', encodings)\n",
    "#  encodings = np.load('/media/axel/ssd/ml-challenge/encodings_0.npy')\n",
    "# encodings = np.expand_dims(encodings, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc = BertClient()\n",
    "# titles = df['title'].iloc[:50000].str.lower().values.tolist()\n",
    "# encodings = bc.encode(titles, show_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings_post = []\n",
    "\n",
    "# for token_list in encodings[1]:\n",
    "#     enc_idx = 0\n",
    "#     i = 0\n",
    "#     embeddings = []\n",
    "    \n",
    "#     for token in token_list:\n",
    "#         if token not in ['[CLS]', '-', ',', '/', '[UNK]', '[SEP]']:\n",
    "#             #print(token)\n",
    "#             #print(enc[0][0][i])\n",
    "#             embeddings.append(encodings[0][enc_idx][i])\n",
    "#         i += 1\n",
    "    \n",
    "#     embeddings = np.array(embeddings)\n",
    "#     encodings_post.append(embeddings)\n",
    "#     enc_idx += 1\n",
    "    \n",
    "# encodings = encodings_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 120000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic_es = load_vectors(path+'cc.es.300.vec')\n",
    "# dic_pt = load_vectors(path+'cc.pt.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = list(dic_es.keys())\n",
    "\n",
    "# for key in keys:\n",
    "#     if key in stopwords:\n",
    "#         del dic_es[key]\n",
    "        \n",
    "# keys = list(dic_pt.keys())\n",
    "\n",
    "# for key in keys:\n",
    "#     if key in stopwords:\n",
    "#         del dic_pt[key]\n",
    "        \n",
    "# common_keys = set(dic_es.keys()).intersection(set(dic_pt.keys()))\n",
    "\n",
    "# for key in dic_pt.keys():\n",
    "#     if key in common_keys:\n",
    "#         dic_pt[key + '_pt'] = dic_pt.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = {**dic_es, **dic_pt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df.language == 'portuguese', 'title'] = df[df.language == 'portuguese']['title'].str.split(' ').apply(lambda sentence: ' '.join(word if word not in common_keys else word + '_pt' for word in sentence))\n",
    "# df_test.loc[df_test.language == 'portuguese', 'title'] = df_test[df_test.language == 'portuguese']['title'].str.split(' ').apply(lambda sentence: ' '.join(word if word not in common_keys else word + '_pt' for word in sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(path + 'train_processed.csv', index=None)\n",
    "# df_test.to_csv(path + 'test_processed.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderCat = LabelEncoder()\n",
    "encoderCat.fit(df['category'])\n",
    "y = df['category']\n",
    "y = encoderCat.transform(y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(df, y, test_size=0.2, random_state=16)\n",
    "# _, df_sample = train_test_split(df, test_size=2000000, random_state=42, stratify=df.category)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(df_sample, df_sample['category'], test_size=0.2, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles_train = df['title'].astype(str).values.tolist()\n",
    "# titles_test = df_test['title'].astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "# tokenizer.fit_on_texts(titles_train + titles_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_seq_train = tokenizer.texts_to_sequences(X_train['title'].astype(str))\n",
    "# word_seq_val = tokenizer.texts_to_sequences(X_val['title'].astype(str))\n",
    "# word_seq_test = tokenizer.texts_to_sequences(df_test['title'])\n",
    "\n",
    "# word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "# word_seq_val = sequence.pad_sequences(word_seq_val, maxlen=max_seq_len)\n",
    "# word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_object(tokenizer, name='tokenizer_sample')\n",
    "# save_object(word_seq_test, name='word_seq_test_sample')\n",
    "# save_object(word_seq_train, name='word_seq_train_sample')\n",
    "# save_object(word_seq_val, name='word_seq_val_sample')\n",
    "\n",
    "tokenizer = load_object(name='tokenizer_proc')\n",
    "word_seq_test = load_object(name='word_seq_test_proc')\n",
    "word_seq_train = load_object(name='word_seq_train_proc')\n",
    "word_seq_val = load_object(name='word_seq_val_proc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 300\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.random.normal(0.0, 0.1, (nb_words, EMB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word, i in word_index.items():\n",
    "#     if i >= nb_words:\n",
    "#         continue\n",
    "#     embedding_vector = dic.get(word)\n",
    "#     if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_object(embedding_matrix, 'embedding_matrix_proc')\n",
    "\n",
    "embedding_matrix = load_object('embedding_matrix_proc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_train, X_val, df\n",
    "# del dic, dic_es, dic_pt\n",
    "# del titles_train, titles_test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoderCat = LabelEncoder()\n",
    "\n",
    "# encoderCat.fit(df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings = np.load('/media/axel/ssd/ml-challenge/encodings_3_layers_0-2.npy')\n",
    "# encodings = np.expand_dims(encodings, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoderCat = LabelEncoder()\n",
    "# y = df['category'].iloc[:len(encodings)]\n",
    "# y = encoderCat.transform(y)\n",
    "# #y2 = df['label_quality'].iloc[:len(encodings)]\n",
    "# #y2 = LabelEncoder().fit_transform(y2)\n",
    "# #y = np.vstack((y1, y2)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[df.language == 'spanish']\n",
    "\n",
    "# for i in range(len(X)):\n",
    "#     X[i] = tf.keras.preprocessing.sequence.pad_sequences(X[i].T, maxlen=50, padding='post', dtype='float16').T\n",
    "    \n",
    "# X = np.array(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del encodings\n",
    "# del X\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df.copy()\n",
    "# df2 = pd.concat([df2, pd.get_dummies(df['label_quality'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {}\n",
    "# prcts = []\n",
    "# for cat in df.category:\n",
    "#     cat_reliable = df2[(df2['category'] == cat)]['reliable'].sum()\n",
    "#     cat_reliable_percentage = cat_reliable / len(df2[(df2['category'] == cat)])\n",
    "#     prcts.append(cat_reliable_percentage)\n",
    "    \n",
    "# cats = encoderCat.transform(df.category)\n",
    "\n",
    "# for cat in cats:\n",
    "#     d[cat] = prcts[0]\n",
    "#     prcts.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(tf.keras.layers.Layer):\n",
    "   \n",
    "    def __init__(self):\n",
    "        super(Mish, self).__init__()\n",
    "    \n",
    "    \n",
    "    def call(self, x):\n",
    "        return x * tf.math.tanh(tf.math.softplus(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(tf.keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_encodings = 768*3\n",
    "categories = 1588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_train(num_filters1, num_filters2, num_filters3, dropout, spatial_dropout, num_dense, balanced_accuracy):\n",
    "    inp = tf.keras.Input(shape=(max_seq_len,))\n",
    "    emb = tf.keras.layers.Embedding(nb_words, EMB_SIZE, weights=[embedding_matrix], input_length=max_seq_len,\n",
    "                                    trainable=False)(inp)\n",
    "    emb = tf.keras.layers.SpatialDropout1D(spatial_dropout)(emb)\n",
    "    x = tf.keras.layers.Conv1D(filters=num_filters1, kernel_size=3)(emb)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Mish()(x)\n",
    "    # x = tf.keras.layers.PReLU()(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=num_filters2, kernel_size=3)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Mish()(x)\n",
    "    # x = tf.keras.layers.PReLU()(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=num_filters3, kernel_size=3)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Mish()(x)\n",
    "    # x = tf.keras.layers.PReLU()(x)\n",
    "    x1 = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    x = tf.keras.layers.SeparableConv1D(filters=num_filters1, kernel_size=3)(emb)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Mish()(x)\n",
    "    # x = tf.keras.layers.PReLU()(x)\n",
    "    x = tf.keras.layers.SeparableConv1D(filters=num_filters2, kernel_size=3)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Mish()(x)\n",
    "    # x = tf.keras.layers.PReLU()(x)\n",
    "    x = tf.keras.layers.SeparableConv1D(filters=num_filters3, kernel_size=3)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Mish()(x)\n",
    "    # x = tf.keras.layers.PReLU()(x)\n",
    "    x2 = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([x1, x2])\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    # x = MCDropout(0.2)(x1)\n",
    "    x = tf.keras.layers.Dense(num_dense)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Mish()(x)\n",
    "    # x = tf.keras.layers.PReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    # x = MCDropout(0.2)(x)\n",
    "    output1 = tf.keras.layers.Dense(categories, activation=tf.nn.softmax, name='output1')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = inp, outputs = output1)\n",
    "    optimizer = Lookahead(RectifiedAdam())\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})\n",
    "    \n",
    "    clr = OneCycle(lr_range=(0.001, 0.002),\n",
    "               momentum_range=(0.95, 0.85))\n",
    "    \n",
    "    model.fit(word_seq_train[:1000000], y_train[:1000000], validation_data=None,\n",
    "           batch_size=1024, epochs=1, verbose=0, callbacks=[clr])\n",
    "    \n",
    "    balanced_accuracy = calculate_balanced_accuracy_chunks(model, word_seq_val[:1000000], y_val[:1000000])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/axel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/axel/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "# inp = tf.keras.Input(shape=(max_seq_len,))\n",
    "# emb = tf.keras.layers.Embedding(nb_words, EMB_SIZE, weights=[embedding_matrix], input_length=max_seq_len,\n",
    "#                                 trainable=False)(inp)\n",
    "# # emb = tf.keras.layers.SpatialDropout1D(0.1)(emb)\n",
    "# x = tf.keras.layers.Conv1D(filters=256, kernel_size=3)(emb)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = Mish()(x)\n",
    "# # x = tf.keras.layers.PReLU()(x)\n",
    "# x = tf.keras.layers.Conv1D(filters=512, kernel_size=3)(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = Mish()(x)\n",
    "# # x = tf.keras.layers.PReLU()(x)\n",
    "# x = tf.keras.layers.Conv1D(filters=1024, kernel_size=3)(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = Mish()(x)\n",
    "# # x = tf.keras.layers.PReLU()(x)\n",
    "# x1 = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "# x = tf.keras.layers.SeparableConv1D(filters=256, kernel_size=3)(emb)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = Mish()(x)\n",
    "# # x = tf.keras.layers.PReLU()(x)\n",
    "# x = tf.keras.layers.SeparableConv1D(filters=512, kernel_size=3)(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = Mish()(x)\n",
    "# # x = tf.keras.layers.PReLU()(x)\n",
    "# x = tf.keras.layers.SeparableConv1D(filters=1024, kernel_size=3)(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = Mish()(x)\n",
    "# # x = tf.keras.layers.PReLU()(x)\n",
    "# x2 = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "# x = tf.keras.layers.Concatenate()([x1, x2])\n",
    "# x = tf.keras.layers.Dropout(0.4)(x)\n",
    "# # x = MCDropout(0.2)(x1)\n",
    "# x = tf.keras.layers.Dense(800)(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = Mish()(x)\n",
    "# # x = tf.keras.layers.PReLU()(x)\n",
    "# x = tf.keras.layers.Dropout(0.4)(x)\n",
    "# # x = MCDropout(0.2)(x)\n",
    "# output1 = tf.keras.layers.Dense(categories, activation=tf.nn.softmax, name='output1')(x)\n",
    "\n",
    "inp = tf.keras.Input(shape=(max_seq_len,))\n",
    "emb = tf.keras.layers.Embedding(nb_words, EMB_SIZE, weights=[embedding_matrix], input_length=max_seq_len,\n",
    "                                trainable=False)(inp)\n",
    "emb = tf.keras.layers.SpatialDropout1D(0.114)(emb)\n",
    "x = tf.keras.layers.Conv1D(filters=456, kernel_size=3)(emb)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x = tf.keras.layers.Conv1D(filters=465, kernel_size=3)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x = tf.keras.layers.Conv1D(filters=1454, kernel_size=3)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x1 = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "x = tf.keras.layers.SeparableConv1D(filters=456, kernel_size=3)(emb)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x = tf.keras.layers.SeparableConv1D(filters=465, kernel_size=3)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x = tf.keras.layers.SeparableConv1D(filters=1454, kernel_size=3)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x2 = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "x = tf.keras.layers.Concatenate()([x1, x2])\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "# x = MCDropout(0.2)(x1)\n",
    "x = tf.keras.layers.Dense(942)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "# x = MCDropout(0.2)(x)\n",
    "output1 = tf.keras.layers.Dense(categories, activation=tf.nn.softmax, name='output1')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.Model(inputs = inp, outputs = [output1, output2])\n",
    "model = tf.keras.Model(inputs = inp, outputs = output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = {\n",
    "#     \"output1\": \"sparse_categorical_crossentropy\",\n",
    "#     \"output2\": \"binary_crossentropy\",\n",
    "# }\n",
    "# lossWeights = {\"output1\": 1.0, \"output2\": 1.0}\n",
    "# optimizer = Lookahead(RectifiedAdam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})\n",
    "# model.compile(optimizer=optimizer, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})\n",
    "# model.compile(optimizer=RectifiedAdam(), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, batch_size = 256):\n",
    "    indices = np.arange(len(X)) \n",
    "    batch=[]\n",
    "    while True:\n",
    "            # it might be a good idea to shuffle your data before each epoch\n",
    "            np.random.shuffle(indices) \n",
    "            for i in indices:\n",
    "                batch.append(i)\n",
    "                if len(batch)==batch_size:\n",
    "                    yield X[batch], Y[batch]\n",
    "                    batch=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1024\n",
    "# train_generator = batch_generator(word_seq_train, y_train, batch_size = batch_size)\n",
    "# val_generator = batch_generator(word_seq_val, y_val, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study()\n",
    "# study.optimize(objective, n_trials=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder.plot_loss(n_skip_beginning=20, n_skip_end=5)\n",
    "# reduce_lr =tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "#                               patience=1)\n",
    "\n",
    "decay=1\n",
    "# clr = CyclicLR(base_lr=0.001*decay, max_lr=0.002*decay, step_size=30000., mode='triangular2')\n",
    "clr = OneCycle(lr_range=(0.001, 0.002),\n",
    "               momentum_range=(0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15999998 samples, validate on 4000000 samples\n",
      "WARNING:tensorflow:From /home/axel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "15999998/15999998 [==============================] - 4260s 266us/sample - loss: 1.0426 - sparse_categorical_accuracy: 0.7826 - val_loss: 0.7278 - val_sparse_categorical_accuracy: 0.8415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4868570470>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 2000000\n",
    "model.fit(word_seq_train, y_train, validation_data=(word_seq_val, y_val),\n",
    "           batch_size=1700, epochs=1, verbose=1,)# callbacks=[clr])\n",
    "\n",
    "# model.fit(word_seq_train[:size], y_train[:size], validation_data=None,\n",
    "#            batch_size=2048, epochs=1, verbose=0,)# callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_balanced_accuracy_chunks(model, word_seq_val[:size], y_val[:size]) #0.411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8114201934578901"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss: 1.0868 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.7825 - val_sparse_categorical_accuracy: 0.8334 prelu + la\n",
    "#loss: 1.1268 - sparse_categorical_accuracy: 0.7744 - val_loss: 0.8179 - val_sparse_categorical_accuracy: 0.8259 mish + la\n",
    "\n",
    "#loss: 1.2513 - sparse_categorical_accuracy: 0.7497 - val_loss: 0.8113 - val_sparse_categorical_accuracy: 0.8263 clr viejo\n",
    "calculate_balanced_accuracy_chunks(model, word_seq_val, y_val) #0.7953 con prelu, 0.8020 lookahead+radam, 0.8182 clr nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model, 'vec3')\n",
    "load_model(model, 'vec3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].trainable = True\n",
    "#model.compile(optimizer=optimizer, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_callback = LRFinder(num_samples=len(word_seq_train[:3000000]), batch_size=1024, lr_scale='exp', verbose=False)\n",
    "\n",
    "# model.fit(word_seq_train[:3000000], y_train[:3000000], validation_data=None,\n",
    "#            batch_size=1024, epochs=1, verbose=1, callbacks=[lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(np.power(10, lr_callback.lrs[20:200]), lr_callback.losses[20:200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_value(model.optimizer.lr, 0.001*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15999998 samples, validate on 4000000 samples\n",
      "WARNING:tensorflow:From /home/axel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "15999998/15999998 [==============================] - 4763s 298us/sample - loss: 0.5881 - sparse_categorical_accuracy: 0.8678 - val_loss: 0.5573 - val_sparse_categorical_accuracy: 0.8749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f35df61a518>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train, validation_data=(word_seq_val, y_val),\n",
    "           batch_size=1200, epochs=1, verbose=1,)# callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.852839552795989"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss: 0.6866 - sparse_categorical_accuracy: 0.8513 - val_loss: 0.5838 - val_sparse_categorical_accuracy: 0.8706\n",
    "#loss: 0.6197 - sparse_categorical_accuracy: 0.8631 - val_loss: 0.5577 - val_sparse_categorical_accuracy: 0.8750 prelu + la\n",
    "#loss: 0.6337 - sparse_categorical_accuracy: 0.8605 - val_loss: 0.5601 - val_sparse_categorical_accuracy: 0.8747\n",
    "\n",
    "calculate_balanced_accuracy_chunks(model, word_seq_val, y_val) #0.8505 prelu + la, 0.8528 mish + la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model, 'vec3')\n",
    "# load_model(model, 'vec3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_value(model.optimizer.lr, 0.001*0.2*0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15999998 samples, validate on 4000000 samples\n",
      "15999998/15999998 [==============================] - 4762s 298us/sample - loss: 0.4656 - sparse_categorical_accuracy: 0.8911 - val_loss: 0.5342 - val_sparse_categorical_accuracy: 0.8806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f35dcd586d8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train, validation_data=(word_seq_val, y_val),\n",
    "           batch_size=1200, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8607321302577564"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "#0.8799 train - 0.8792 val antes\n",
    "#loss: 0.5019 - sparse_categorical_accuracy: 0.8844 - val_loss: 0.5296 - val_sparse_categorical_accuracy: 0.8810 prelu + la\n",
    "calculate_balanced_accuracy_chunks(model, word_seq_val, y_val) #0.8567 antes, 0.8590, 0.8593 mish + la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model, 'vec3')\n",
    "# load_model(model, 'vec3') #seguir desde aca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15999998 samples, validate on 4000000 samples\n",
      "15999998/15999998 [==============================] - 3978s 249us/sample - loss: 0.5180 - sparse_categorical_accuracy: 0.8809 - val_loss: 0.5449 - val_sparse_categorical_accuracy: 0.8781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6b0e5b16d8>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.keras.backend.set_value(model.optimizer.lr, 0.0003*0.5*0.2)\n",
    "model.fit(word_seq_train, y_train, validation_data=(word_seq_val, y_val),\n",
    "           batch_size=1024, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss: 0.4526 - sparse_categorical_accuracy: 0.8938 - val_loss: 0.5213 - val_sparse_categorical_accuracy: 0.8833, prelu + lookahead\n",
    "calculate_balanced_accuracy_chunks(model, word_seq_val, y_val) # 0.8592 antes, 0.8622 prelu + lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model, 'vec3')\n",
    "# load_model(model, 'vec3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15999998 samples, validate on 4000000 samples\n",
      "15999998/15999998 [==============================] - 4006s 250us/sample - loss: 0.4506 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.5228 - val_sparse_categorical_accuracy: 0.8830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc05edee5c0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.set_value(model.optimizer.lr, tf.keras.backend.get_value(model.optimizer.lr)*0.3)\n",
    "model.fit(word_seq_train, y_train, validation_data=(word_seq_val, y_val),\n",
    "           batch_size=1024, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8627204686986996"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_balanced_accuracy_chunks(model, word_seq_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo word_embeddings:\n",
    "\n",
    "# 0.8472 val - 0.8852 test\n",
    "# 0.8545 val - 0.8869 test\n",
    "# 0.8592 val - 0.8915 test\n",
    "# 0.8625 val - 0.8944 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
    "tfidf_df = tfidf_vect.fit_transform(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split('(\\d+)',text) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodings_3_layers_0.npy\n",
      "lr before: 0.0002\n",
      "Train on 900000 samples, validate on 100000 samples\n",
      "900000/900000 [==============================] - 1074s 1ms/sample - loss: 0.9977 - sparse_categorical_accuracy: 0.8001 - val_loss: 0.8735 - val_sparse_categorical_accuracy: 0.8239\n",
      "lr after: 0.0002\n",
      "balanced_accuracy: 0.7878131573558538\n",
      "encodings_3_layers_1.npy\n",
      "lr before: 0.0002\n",
      "Train on 900000 samples, validate on 100000 samples\n",
      "900000/900000 [==============================] - 1083s 1ms/sample - loss: 0.9617 - sparse_categorical_accuracy: 0.8064 - val_loss: 0.8439 - val_sparse_categorical_accuracy: 0.8290\n",
      "lr after: 0.0002\n",
      "balanced_accuracy: 0.7945393969624746\n",
      "encodings_3_layers_2.npy\n",
      "lr before: 0.0002\n",
      "Train on 900000 samples, validate on 100000 samples\n",
      "900000/900000 [==============================] - 1089s 1ms/sample - loss: 0.9340 - sparse_categorical_accuracy: 0.8116 - val_loss: 0.8435 - val_sparse_categorical_accuracy: 0.8289\n",
      "lr after: 0.0002\n",
      "balanced_accuracy: 0.7977145227319922\n",
      "encodings_3_layers_3.npy\n",
      "lr before: 0.0002\n",
      "Train on 900000 samples, validate on 100000 samples\n",
      "900000/900000 [==============================] - 1068s 1ms/sample - loss: 0.9131 - sparse_categorical_accuracy: 0.8145 - val_loss: 0.8507 - val_sparse_categorical_accuracy: 0.8272\n",
      "lr after: 0.0002\n",
      "balanced_accuracy: 0.7931988681934606\n",
      "encodings_3_layers_4.npy\n",
      "lr before: 1e-04\n",
      "Train on 900000 samples, validate on 100000 samples\n",
      "900000/900000 [==============================] - 1083s 1ms/sample - loss: 0.8871 - sparse_categorical_accuracy: 0.8192 - val_loss: 0.8374 - val_sparse_categorical_accuracy: 0.8305\n",
      "lr after: 1e-04\n",
      "balanced_accuracy: 0.7969944545986862\n",
      "encodings_3_layers_5.npy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-fe786c6e1f95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter_num\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen_data\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/large/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoderCat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr before: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4a051cdf89ec>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(path, labels, encoder, test_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2212\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2212\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filenames = [f for f in listdir(path+'large')]\n",
    "filenames.sort(key=natural_keys)\n",
    "epochs = 1\n",
    "len_data = 1000000\n",
    "decay = 1\n",
    "best_balanced_accuracy = 0\n",
    "\n",
    "for _ in range(epochs):\n",
    "    iter_num = 0\n",
    "    \n",
    "#     clr = CyclicLR(base_lr=0.001*decay, max_lr=0.002*decay, step_size=20000., mode='triangular2')\n",
    "    tf.keras.backend.set_value(model.optimizer.lr, 0.001*0.2)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        labels = df['category'].iloc[iter_num*len_data : (iter_num+1)*len_data]\n",
    "        X_train, X_val, y_train, y_val = get_data(path + '/large/' + filename, labels, encoderCat, test_size=0.1)\n",
    "        \n",
    "        print('lr before: ' + str(tf.keras.backend.get_value(model.optimizer.lr)))\n",
    "        \n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "               batch_size=256, epochs=1, verbose=1,)# callbacks=[clr])\n",
    "        \n",
    "        print('lr after: ' + str(tf.keras.backend.get_value(model.optimizer.lr)))\n",
    "        balanced_accuracy = calculate_balanced_accuracy(X_val, y_val)\n",
    "        print('balanced_accuracy: ' + str(balanced_accuracy))\n",
    "        \n",
    "        if balanced_accuracy > best_balanced_accuracy:\n",
    "            best_balanced_accuracy = balanced_accuracy\n",
    "        else:\n",
    "            tf.keras.backend.set_value(model.optimizer.lr, tf.keras.backend.get_value(model.optimizer.lr)*0.5)\n",
    "        \n",
    "        del X_train, X_val, y_train, y_val\n",
    "        gc.collect()\n",
    "\n",
    "        iter_num += 1\n",
    "        \n",
    "    decay = decay*0.2\n",
    "    tf.keras.backend.set_value(model.optimizer.lr, tf.keras.backend.get_value(model.optimizer.lr)*decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.set_value(model.optimizer.lr, 0.001*0.2)\n",
    "# tf.keras.backend.get_value(model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7fde1a3ebcf8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.save('model2')\n",
    "# model.save_weights('model_weights2')\n",
    "# model.load_weights('model_weights2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triangular: (max_lr=0.003, Adam)\n",
    "# it 8: 0.718 60k step_size, 0.707 30k step_size, 0.722 20k step_size, 0.7154 10k step_size \n",
    "# it 12:                     0.737 30k step_size, 0.754 20k step_size, 0.7515 10k step_size\n",
    "# it 19:                                          0.738 20k step_size\n",
    "\n",
    "# triangular2:\n",
    "\n",
    "# it 8:  0.723 20k step\n",
    "# it 12: 0.752 20k step\n",
    "# it 19: 0.752 20k step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.6752 val - 0.709 test\n",
    "#0.7335 val - 0.770 test\n",
    "#0.7540 val - 0.794 test\n",
    "#0.7793 val - 0.819 test\n",
    "#0.7850 val - 0.823 test\n",
    "#0.8136 val - 0.838 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv(path + 'test.csv')\n",
    "# bc = BertClient()\n",
    "# titles = df_test['title'].str.lower().values.tolist()\n",
    "# encodings = bc.encode(titles)\n",
    "# np.save('/media/axel/ssd/ml-challenge/encodings_test', encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodingsTest = np.load('/media/axel/ssd/ml-challenge/encodings_test.npy')\n",
    "# encodingsTest = np.expand_dims(encodingsTest, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_chunks = np.array([])\n",
    "\n",
    "num_models = 50\n",
    "for chunk in chunks(word_seq_test, int(50000/num_models)):\n",
    "    y_probas = np.stack([model.predict(chunk) for sample in range(num_models)])\n",
    "    y_proba = y_probas.mean(axis=0)\n",
    "    y_proba = np.argmax(y_proba, axis=-1)\n",
    "    y_preds_chunks = np.append(y_preds_chunks, y_proba)\n",
    "    del y_probas\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_preds.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del y_preds\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(encodingsTest)\n",
    "y_pred = model.predict(word_seq_test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = encoderCat.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['category'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('../../sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
