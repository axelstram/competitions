{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from bert_serving.client import BertClient\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "from os import listdir\n",
    "import re\n",
    "import fasttext\n",
    "import unicodedata\n",
    "import io\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "# import optuna\n",
    "# import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_momentum(optimizer, mom_val):\n",
    "    \"\"\"\n",
    "    Helper to set momentum of Keras optimizers.\n",
    "    :param optimizer: Keras optimizer\n",
    "    :param mom_val: value of momentum.\n",
    "    \"\"\"\n",
    "    keys = dir(optimizer)\n",
    "    if \"momentum\" in keys:\n",
    "        tf.keras.backend.set_value(optimizer.momentum, mom_val)\n",
    "    if \"rho\" in keys:\n",
    "        tf.keras.backend.set_value(optimizer.rho, mom_val)\n",
    "    if \"beta_1\" in keys:\n",
    "        tf.keras.backend.set_value(optimizer.beta_1, mom_val)\n",
    "\n",
    "\n",
    "def set_lr(optimizer, lr):\n",
    "    \"\"\"\n",
    "    Helper to set learning rate of Keras optimizers.\n",
    "    :param optimizer: Keras optimizer\n",
    "    :param lr: value of learning rate.\n",
    "    \"\"\"\n",
    "    tf.keras.backend.set_value(optimizer.lr, lr)\n",
    "\n",
    "class OneCycle(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A callback class for one-cycle policy training.\n",
    "    :param lr_range: a tuple of starting (usually minimum) lr value and maximum (peak) lr value.\n",
    "    :param momentum_range: a tuple of momentum values.\n",
    "    :param phase_one_fraction: a fraction for phase I (increasing lr) in one cycle. Must between 0 to 1.\n",
    "    :param reset_on_train_begin: True or False to reset counters when training begins.\n",
    "    :param record_frq: integer > 0, a frequency in batches to record training loss.\n",
    "    :param verbose: True or False to print progress.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            lr_range,\n",
    "            momentum_range=None,\n",
    "            phase_one_fraction=0.3,\n",
    "            reset_on_train_begin=True,\n",
    "            record_frq=10,\n",
    "            verbose=False):\n",
    "\n",
    "        super(OneCycle, self).__init__()\n",
    "\n",
    "        self.lr_range = lr_range\n",
    "\n",
    "        self.momentum_range = momentum_range\n",
    "        if momentum_range is not None:\n",
    "            err_msg = \"momentum_range must be a 2-numeric tuple (m1, m2).\"\n",
    "            if not isinstance(momentum_range, (tuple,)) or len(momentum_range) != 2:\n",
    "                raise ValueError(err_msg)\n",
    "\n",
    "        self.phase_one_fraction = phase_one_fraction\n",
    "        self.reset_on_train_begin = reset_on_train_begin\n",
    "        self.record_frq = record_frq\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # helper tracker\n",
    "        self.log = {}  # history in iterations\n",
    "        self.log_ep = {}  # history in epochs\n",
    "        self.stop_training = False\n",
    "\n",
    "        # counter\n",
    "        self.current_iter = 0\n",
    "\n",
    "    def get_current_lr(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        A helper function to calculate a current learning rate based on current iteration number.\n",
    "        :return lr: a current learning rate.\n",
    "        \"\"\"\n",
    "        if n_iter is None:\n",
    "            n_iter = self.n_iter\n",
    "\n",
    "        x = float(self.current_iter) / n_iter\n",
    "        if x < self.phase_one_fraction:\n",
    "            amp = self.lr_range[1] - self.lr_range[0]\n",
    "            lr = (np.cos(x * np.pi/self.phase_one_fraction - np.pi) + 1) * amp / 2.0 + self.lr_range[0]\n",
    "        if x >= self.phase_one_fraction:\n",
    "            amp = self.lr_range[1]\n",
    "            lr = (np.cos((x - self.phase_one_fraction) * np.pi/ (1-self.phase_one_fraction)) + 1) / 2.0 * amp\n",
    "        return lr\n",
    "\n",
    "    def get_current_momentum(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        A helper function to calculate a current momentum based on current iteration number.\n",
    "        :return momentum: a current momentum.\n",
    "        \"\"\"\n",
    "        if n_iter is None:\n",
    "            n_iter = self.n_iter\n",
    "        amp = self.momentum_range[1] - self.momentum_range[0]\n",
    "        # delta = (1 - np.abs(np.mod(self.current_iter, n_iter) * 2.0 / n_iter - 1)) * amplitude\n",
    "        x = float(self.current_iter) / n_iter\n",
    "        if x < self.phase_one_fraction:\n",
    "            delta = (np.cos(x * np.pi / self.phase_one_fraction - np.pi) + 1) * amp / 2.0\n",
    "        if x >= self.phase_one_fraction:\n",
    "            delta = (np.cos((x - self.phase_one_fraction) * np.pi / (1 - self.phase_one_fraction)) + 1) / 2.0 * amp\n",
    "        return delta + self.momentum_range[0]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def cycle_momentum(self):\n",
    "        return self.momentum_range is not None\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.n_epoch = self.params['epochs']\n",
    "\n",
    "        # find number of batches per epoch\n",
    "        if self.params['batch_size'] is not None:  # model.fit\n",
    "            self.n_bpe = int(np.ceil(self.params['samples'] / self.params['batch_size']))\n",
    "        if self.params['batch_size'] is None:  # model.fit_generator\n",
    "            self.n_bpe = self.params['samples']\n",
    "\n",
    "        self.n_iter = self.n_epoch * self.n_bpe\n",
    "        # this is a number of iteration in one cycle\n",
    "\n",
    "        self.current_iter = 0\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs={}):\n",
    "        set_lr(self.model.optimizer, self.get_current_lr())\n",
    "        if self.cycle_momentum:\n",
    "            set_momentum(self.model.optimizer, self.get_current_momentum())\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"lr={:.2e}\".format(self.get_current_lr()), \",\", \"m={:.2e}\".format(self.get_current_momentum()))\n",
    "\n",
    "        # record according to record_frq\n",
    "        if np.mod(int(self.current_iter), self.record_frq) == 0:\n",
    "            self.log.setdefault('lr', []).append(self.get_current_lr())\n",
    "            if self.cycle_momentum:\n",
    "                self.log.setdefault('momentum', []).append(self.get_current_momentum())\n",
    "\n",
    "            for k, v in logs.items():\n",
    "                self.log.setdefault(k, []).append(v)\n",
    "\n",
    "            self.log.setdefault('iter', []).append(self.current_iter)\n",
    "\n",
    "        # update current iteration\n",
    "        self.current_iter += 1\n",
    "\n",
    "        # consider termination\n",
    "        if self.current_iter == self.n_iter:\n",
    "            self.stop_training = True\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.log_ep.setdefault('epoch', []).append(epoch)\n",
    "        self.log_ep.setdefault('lr', []).append(\n",
    "            tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.log_ep.setdefault(k, []).append(v)\n",
    "\n",
    "    def test_run(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        Visualize values of learning rate (and momentum) as a function of iteration (batch).\n",
    "        :param n_iter: a number of cycles. If None, 1000 is used.\n",
    "        \"\"\"\n",
    "\n",
    "        if hasattr(self, 'current_iter'):\n",
    "            original_it = self.current_iter\n",
    "\n",
    "        if n_iter is None:\n",
    "            if hasattr(self, 'n_iter'):\n",
    "                n_iter = self.n_iter\n",
    "            else:\n",
    "                n_iter = 1000\n",
    "        n_iter = int(n_iter)\n",
    "\n",
    "        lrs = np.zeros(shape=(n_iter,))\n",
    "        if self.momentum_range is not None:\n",
    "            moms = np.zeros_like(lrs)\n",
    "\n",
    "        for i in range(int(n_iter)):\n",
    "            self.current_iter = i\n",
    "            lrs[i] = self.get_current_lr(n_iter)\n",
    "            if self.cycle_momentum:\n",
    "                moms[i] = self.get_current_momentum(n_iter)\n",
    "        if not self.cycle_momentum:\n",
    "            plt.plot(lrs)\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('lr')\n",
    "        else:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(lrs)\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('lr')\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(moms)\n",
    "            plt.xlabel('iterations')\n",
    "            plt.ylabel('momentum')\n",
    "\n",
    "        if hasattr(self, 'current_iter'):\n",
    "            self.current_iter = original_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "\n",
    "# class Lookahead(OptimizerV2):\n",
    "#     \"\"\"This class allows to extend optimizers with the lookahead mechanism.\n",
    "\n",
    "#     The mechanism is proposed by Michael R. Zhang et.al in the paper\n",
    "#     [Lookahead Optimizer: k steps forward, 1 step back]\n",
    "#     (https://arxiv.org/abs/1907.08610v1). The optimizer iteratively updates two\n",
    "#     sets of weights: the search directions for weights are chosen by the inner\n",
    "#     optimizer, while the \"slow weights\" are updated each `k` steps based on the\n",
    "#     directions of the \"fast weights\" and the two sets of weights are\n",
    "#     synchronized. This method improves the learning stability and lowers the\n",
    "#     variance of its inner optimizer.\n",
    "\n",
    "#     Example of usage:\n",
    "\n",
    "#     ```python\n",
    "#     opt = tf.keras.optimizers.SGD(learning_rate)\n",
    "#     opt = tfa.optimizers.Lookahead(opt)\n",
    "#     ```\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self,\n",
    "#                  optimizer,\n",
    "#                  sync_period=6,\n",
    "#                  slow_step_size=0.5,\n",
    "#                  name=\"Lookahead\",\n",
    "#                  **kwargs):\n",
    "#         r\"\"\"Wrap optimizer with the lookahead mechanism.\n",
    "\n",
    "#         Args:\n",
    "#             optimizer: The original optimizer that will be used to compute\n",
    "#                 and apply the gradients.\n",
    "#             sync_period: An integer. The synchronization period of lookahead.\n",
    "#                 Enable lookahead mechanism by setting it with a positive value.\n",
    "#             slow_step_size: A floating point value.\n",
    "#                 The ratio for updating the slow weights.\n",
    "#             name: Optional name for the operations created when applying\n",
    "#                 gradients. Defaults to \"Lookahead\".\n",
    "#             **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n",
    "#                 `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients\n",
    "#                 by norm; `clipvalue` is clip gradients by value, `decay` is\n",
    "#                 included for backward compatibility to allow time inverse\n",
    "#                 decay of learning rate. `lr` is included for backward\n",
    "#                 compatibility, recommended to use `learning_rate` instead.\n",
    "#         \"\"\"\n",
    "#         super(Lookahead, self).__init__(name, **kwargs)\n",
    "\n",
    "#         if isinstance(optimizer, str):\n",
    "#             optimizer = tf.keras.optimizers.get(optimizer)\n",
    "#         if not isinstance(optimizer, tf.keras.optimizers.Optimizer):\n",
    "#             raise TypeError(\n",
    "#                 \"optimizer is not an object of tf.keras.optimizers.Optimizer\")\n",
    "\n",
    "#         self._optimizer = optimizer\n",
    "#         self._set_hyper('sync_period', sync_period)\n",
    "#         self._set_hyper('slow_step_size', slow_step_size)\n",
    "#         self._initialized = False\n",
    "\n",
    "#     @property\n",
    "#     def lr(self):\n",
    "#       return self._optimizer.lr\n",
    "\n",
    "#     @lr.setter\n",
    "#     def lr(self, lr):\n",
    "#       self._optimizer.lr = lr\n",
    "      \n",
    "#     def _create_slots(self, var_list):\n",
    "#         self._optimizer._create_slots(var_list=var_list)  # pylint: disable=protected-access\n",
    "#         for var in var_list:\n",
    "#             self.add_slot(var, 'slow')\n",
    "\n",
    "#     def _create_hypers(self):\n",
    "#         self._optimizer._create_hypers()  # pylint: disable=protected-access\n",
    "\n",
    "#     def _prepare(self, var_list):\n",
    "#         return self._optimizer._prepare(var_list=var_list)  # pylint: disable=protected-access\n",
    "\n",
    "#     def apply_gradients(self, grads_and_vars, name=None):\n",
    "#         self._optimizer._iterations = self.iterations  # pylint: disable=protected-access\n",
    "#         return super(Lookahead, self).apply_gradients(grads_and_vars, name)\n",
    "\n",
    "#     def _init_op(self, var):\n",
    "#         slow_var = self.get_slot(var, 'slow')\n",
    "#         return slow_var.assign(\n",
    "#             tf.where(\n",
    "#                 tf.equal(self.iterations,\n",
    "#                          tf.constant(0, dtype=self.iterations.dtype)),\n",
    "#                 var,\n",
    "#                 slow_var,\n",
    "#             ),\n",
    "#             use_locking=self._use_locking)\n",
    "\n",
    "#     def _look_ahead_op(self, var):\n",
    "#         var_dtype = var.dtype.base_dtype\n",
    "#         slow_var = self.get_slot(var, 'slow')\n",
    "#         local_step = tf.cast(self.iterations + 1, tf.dtypes.int64)\n",
    "#         sync_period = self._get_hyper('sync_period', tf.dtypes.int64)\n",
    "#         slow_step_size = self._get_hyper('slow_step_size', var_dtype)\n",
    "#         step_back = slow_var + slow_step_size * (var - slow_var)\n",
    "#         sync_cond = tf.equal(\n",
    "#             tf.math.floordiv(local_step, sync_period) * sync_period,\n",
    "#             local_step)\n",
    "#         with tf.control_dependencies([step_back]):\n",
    "#             slow_update = slow_var.assign(\n",
    "#                 tf.where(\n",
    "#                     sync_cond,\n",
    "#                     step_back,\n",
    "#                     slow_var,\n",
    "#                 ),\n",
    "#                 use_locking=self._use_locking)\n",
    "#             var_update = var.assign(\n",
    "#                 tf.where(\n",
    "#                     sync_cond,\n",
    "#                     step_back,\n",
    "#                     var,\n",
    "#                 ),\n",
    "#                 use_locking=self._use_locking)\n",
    "#         return tf.group(slow_update, var_update)\n",
    "\n",
    "#     @property\n",
    "#     def weights(self):\n",
    "#         return self._weights + self._optimizer.weights\n",
    "\n",
    "#     def _resource_apply_dense(self, grad, var):\n",
    "#         init_op = self._init_op(var)\n",
    "#         with tf.control_dependencies([init_op]):\n",
    "#             train_op = self._optimizer._resource_apply_dense(grad, var)  # pylint: disable=protected-access\n",
    "#             with tf.control_dependencies([train_op]):\n",
    "#                 look_ahead_op = self._look_ahead_op(var)\n",
    "#         return tf.group(init_op, train_op, look_ahead_op)\n",
    "\n",
    "#     def _resource_apply_sparse(self, grad, var, indices):\n",
    "#         init_op = self._init_op(var)\n",
    "#         with tf.control_dependencies([init_op]):\n",
    "#             train_op = self._optimizer._resource_apply_sparse(  # pylint: disable=protected-access\n",
    "#                 grad, var, indices)\n",
    "#             with tf.control_dependencies([train_op]):\n",
    "#                 look_ahead_op = self._look_ahead_op(var)\n",
    "#         return tf.group(init_op, train_op, look_ahead_op)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = {\n",
    "#             'optimizer': tf.keras.optimizers.serialize(self._optimizer),\n",
    "#             'sync_period': self._serialize_hyperparameter('sync_period'),\n",
    "#             'slow_step_size': self._serialize_hyperparameter('slow_step_size'),\n",
    "#         }\n",
    "#         base_config = super(Lookahead, self).get_config()\n",
    "#         return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config, custom_objects=None):\n",
    "#         optimizer = tf.keras.optimizers.deserialize(\n",
    "#             config.pop('optimizer'),\n",
    "#             custom_objects=custom_objects,\n",
    "#         )\n",
    "#         return cls(optimizer, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,\n",
    "                 num_samples,\n",
    "                 batch_size,\n",
    "                 minimum_lr=1e-5,\n",
    "                 maximum_lr=10.,\n",
    "                 lr_scale='exp',\n",
    "                 validation_data=None,\n",
    "                 validation_sample_rate=5,\n",
    "                 stopping_criterion_factor=4.,\n",
    "                 loss_smoothing_beta=0.98,\n",
    "                 save_dir=None,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        This class uses the Cyclic Learning Rate history to find a\n",
    "        set of learning rates that can be good initializations for the\n",
    "        One-Cycle training proposed by Leslie Smith in the paper referenced\n",
    "        below.\n",
    "\n",
    "        A port of the Fast.ai implementation for Keras.\n",
    "\n",
    "        # Note\n",
    "        This requires that the model be trained for exactly 1 epoch. If the model\n",
    "        is trained for more epochs, then the metric calculations are only done for\n",
    "        the first epoch.\n",
    "\n",
    "        # Interpretation\n",
    "        Upon visualizing the loss plot, check where the loss starts to increase\n",
    "        rapidly. Choose a learning rate at somewhat prior to the corresponding\n",
    "        position in the plot for faster convergence. This will be the maximum_lr lr.\n",
    "        Choose the max value as this value when passing the `max_val` argument\n",
    "        to OneCycleLR callback.\n",
    "\n",
    "        Since the plot is in log-scale, you need to compute 10 ^ (-k) of the x-axis\n",
    "\n",
    "        # Arguments:\n",
    "            num_samples: Integer. Number of samples in the dataset.\n",
    "            batch_size: Integer. Batch size during training.\n",
    "            minimum_lr: Float. Initial learning rate (and the minimum).\n",
    "            maximum_lr: Float. Final learning rate (and the maximum).\n",
    "            lr_scale: Can be one of ['exp', 'linear']. Chooses the type of\n",
    "                scaling for each update to the learning rate during subsequent\n",
    "                batches. Choose 'exp' for large range and 'linear' for small range.\n",
    "            validation_data: Requires the validation dataset as a tuple of\n",
    "                (X, y) belonging to the validation set. If provided, will use the\n",
    "                validation set to compute the loss metrics. Else uses the training\n",
    "                batch loss. Will warn if not provided to alert the user.\n",
    "            validation_sample_rate: Positive or Negative Integer. Number of batches to sample from the\n",
    "                validation set per iteration of the LRFinder. Larger number of\n",
    "                samples will reduce the variance but will take longer time to execute\n",
    "                per batch.\n",
    "\n",
    "                If Positive > 0, will sample from the validation dataset\n",
    "                If Megative, will use the entire dataset\n",
    "            stopping_criterion_factor: Integer or None. A factor which is used\n",
    "                to measure large increase in the loss value during training.\n",
    "                Since callbacks cannot stop training of a model, it will simply\n",
    "                stop logging the additional values from the epochs after this\n",
    "                stopping criterion has been met.\n",
    "                If None, this check will not be performed.\n",
    "            loss_smoothing_beta: Float. The smoothing factor for the moving\n",
    "                average of the loss function.\n",
    "            save_dir: Optional, String. If passed a directory path, the callback\n",
    "                will save the running loss and learning rates to two separate numpy\n",
    "                arrays inside this directory. If the directory in this path does not\n",
    "                exist, they will be created.\n",
    "            verbose: Whether to print the learning rate after every batch of training.\n",
    "\n",
    "        # References:\n",
    "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
    "        \"\"\"\n",
    "        super(LRFinder, self).__init__()\n",
    "\n",
    "        if lr_scale not in ['exp', 'linear']:\n",
    "            raise ValueError(\"`lr_scale` must be one of ['exp', 'linear']\")\n",
    "\n",
    "        if validation_data is not None:\n",
    "            self.validation_data = validation_data\n",
    "            self.use_validation_set = True\n",
    "\n",
    "            if validation_sample_rate > 0 or validation_sample_rate < 0:\n",
    "                self.validation_sample_rate = validation_sample_rate\n",
    "            else:\n",
    "                raise ValueError(\"`validation_sample_rate` must be a positive or negative integer other than o\")\n",
    "        else:\n",
    "            self.use_validation_set = False\n",
    "            self.validation_sample_rate = 0\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.initial_lr = minimum_lr\n",
    "        self.final_lr = maximum_lr\n",
    "        self.lr_scale = lr_scale\n",
    "        self.stopping_criterion_factor = stopping_criterion_factor\n",
    "        self.loss_smoothing_beta = loss_smoothing_beta\n",
    "        self.save_dir = save_dir\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.num_batches_ = num_samples // batch_size\n",
    "        self.current_lr_ = minimum_lr\n",
    "\n",
    "        if lr_scale == 'exp':\n",
    "            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (\n",
    "                1. / float(self.num_batches_))\n",
    "        else:\n",
    "            extra_batch = int((num_samples % batch_size) != 0)\n",
    "            self.lr_multiplier_ = np.linspace(\n",
    "                minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n",
    "\n",
    "        # If negative, use entire validation set\n",
    "        if self.validation_sample_rate < 0:\n",
    "            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n",
    "\n",
    "        self.current_batch_ = 0\n",
    "        self.current_epoch_ = 0\n",
    "        self.best_loss_ = 1e6\n",
    "        self.running_loss_ = 0.\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "\n",
    "        self.current_epoch_ = 1\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, self.initial_lr)\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_batch_ = 0\n",
    "\n",
    "        if self.current_epoch_ > 1:\n",
    "            warnings.warn(\n",
    "                \"\\n\\nLearning rate finder should be used only with a single epoch. \"\n",
    "                \"Hereafter, the callback will not measure the losses.\\n\\n\")\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.current_batch_ += 1\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.current_epoch_ > 1:\n",
    "            return\n",
    "\n",
    "        if self.use_validation_set:\n",
    "            X, Y = self.validation_data[0], self.validation_data[1]\n",
    "\n",
    "            # use 5 random batches from test set for fast approximate of loss\n",
    "            num_samples = self.batch_size * self.validation_sample_rate\n",
    "\n",
    "            if num_samples > X.shape[0]:\n",
    "                num_samples = X.shape[0]\n",
    "\n",
    "            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n",
    "            x = X[idx]\n",
    "            y = Y[idx]\n",
    "\n",
    "            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n",
    "            loss = values[0]\n",
    "        else:\n",
    "            loss = logs['loss']\n",
    "\n",
    "        # smooth the loss value and bias correct\n",
    "        running_loss = self.loss_smoothing_beta * loss + (\n",
    "            1. - self.loss_smoothing_beta) * loss\n",
    "        running_loss = running_loss / (\n",
    "            1. - self.loss_smoothing_beta**self.current_batch_)\n",
    "\n",
    "        # stop logging if loss is too large\n",
    "        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n",
    "                running_loss >\n",
    "                self.stopping_criterion_factor * self.best_loss_):\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)\"\n",
    "                      % (self.stopping_criterion_factor, self.best_loss_))\n",
    "            return\n",
    "\n",
    "        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n",
    "            self.best_loss_ = running_loss\n",
    "\n",
    "        current_lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        self.history.setdefault('running_loss_', []).append(running_loss)\n",
    "        if self.lr_scale == 'exp':\n",
    "            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n",
    "        else:\n",
    "            self.history.setdefault('log_lrs', []).append(current_lr)\n",
    "\n",
    "        # compute the lr for the next batch and update the optimizer lr\n",
    "        if self.lr_scale == 'exp':\n",
    "            current_lr *= self.lr_multiplier_\n",
    "        else:\n",
    "            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n",
    "\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, current_lr)\n",
    "\n",
    "        # save the other metrics as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        if self.verbose:\n",
    "            if self.use_validation_set:\n",
    "                print(\" - LRFinder: val_loss: %1.4f - lr = %1.8f \" %\n",
    "                      (values[0], current_lr))\n",
    "            else:\n",
    "                print(\" - LRFinder: lr = %1.8f \" % current_lr)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.save_dir is not None and self.current_epoch_ <= 1:\n",
    "            if not os.path.exists(self.save_dir):\n",
    "                os.makedirs(self.save_dir)\n",
    "\n",
    "            losses_path = os.path.join(self.save_dir, 'losses.npy')\n",
    "            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n",
    "\n",
    "            np.save(losses_path, self.losses)\n",
    "            np.save(lrs_path, self.lrs)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"\\tLR Finder : Saved the losses and learning rate values in path : {%s}\"\n",
    "                      % (self.save_dir))\n",
    "\n",
    "        self.current_epoch_ += 1\n",
    "\n",
    "        warnings.simplefilter(\"default\")\n",
    "\n",
    "    def plot_schedule(self, clip_beginning=None, clip_endding=None):\n",
    "        \"\"\"\n",
    "        Plots the schedule from the callback itself.\n",
    "\n",
    "        # Arguments:\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.style.use('seaborn-white')\n",
    "        except ImportError:\n",
    "            print(\n",
    "                \"Matplotlib not found. Please use `pip install matplotlib` first.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if clip_beginning is not None and clip_beginning < 0:\n",
    "            clip_beginning = -clip_beginning\n",
    "\n",
    "        if clip_endding is not None and clip_endding > 0:\n",
    "            clip_endding = -clip_endding\n",
    "\n",
    "        losses = self.losses\n",
    "        lrs = self.lrs\n",
    "\n",
    "        if clip_beginning:\n",
    "            losses = losses[clip_beginning:]\n",
    "            lrs = lrs[clip_beginning:]\n",
    "\n",
    "        if clip_endding:\n",
    "            losses = losses[:clip_endding]\n",
    "            lrs = lrs[:clip_endding]\n",
    "\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.title('Learning rate vs Loss')\n",
    "        plt.xlabel('learning rate')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def restore_schedule_from_dir(cls,\n",
    "                                  directory,\n",
    "                                  clip_beginning=None,\n",
    "                                  clip_endding=None):\n",
    "        \"\"\"\n",
    "        Loads the training history from the saved numpy files in the given directory.\n",
    "\n",
    "        # Arguments:\n",
    "            directory: String. Path to the directory where the serialized numpy\n",
    "                arrays of the loss and learning rates are saved.\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "\n",
    "        Returns:\n",
    "            tuple of (losses, learning rates)\n",
    "        \"\"\"\n",
    "        if clip_beginning is not None and clip_beginning < 0:\n",
    "            clip_beginning = -clip_beginning\n",
    "\n",
    "        if clip_endding is not None and clip_endding > 0:\n",
    "            clip_endding = -clip_endding\n",
    "\n",
    "        losses_path = os.path.join(directory, 'losses.npy')\n",
    "        lrs_path = os.path.join(directory, 'lrs.npy')\n",
    "\n",
    "        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n",
    "            print(\"%s and %s could not be found at directory : {%s}\" %\n",
    "                  (losses_path, lrs_path, directory))\n",
    "\n",
    "            losses = None\n",
    "            lrs = None\n",
    "\n",
    "        else:\n",
    "            losses = np.load(losses_path)\n",
    "            lrs = np.load(lrs_path)\n",
    "\n",
    "            if clip_beginning:\n",
    "                losses = losses[clip_beginning:]\n",
    "                lrs = lrs[clip_beginning:]\n",
    "\n",
    "            if clip_endding:\n",
    "                losses = losses[:clip_endding]\n",
    "                lrs = lrs[:clip_endding]\n",
    "\n",
    "        return losses, lrs\n",
    "\n",
    "    @classmethod\n",
    "    def plot_schedule_from_file(cls,\n",
    "                                directory,\n",
    "                                clip_beginning=None,\n",
    "                                clip_endding=None):\n",
    "        \"\"\"\n",
    "        Plots the schedule from the saved numpy arrays of the loss and learning\n",
    "        rate values in the specified directory.\n",
    "\n",
    "        # Arguments:\n",
    "            directory: String. Path to the directory where the serialized numpy\n",
    "                arrays of the loss and learning rates are saved.\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.style.use('seaborn-white')\n",
    "        except ImportError:\n",
    "            print(\"Matplotlib not found. Please use `pip install matplotlib` first.\")\n",
    "            return\n",
    "\n",
    "        losses, lrs = cls.restore_schedule_from_dir(\n",
    "            directory,\n",
    "            clip_beginning=clip_beginning,\n",
    "            clip_endding=clip_endding)\n",
    "\n",
    "        if losses is None or lrs is None:\n",
    "            return\n",
    "        else:\n",
    "            plt.plot(lrs, losses)\n",
    "            plt.title('Learning rate vs Loss')\n",
    "            plt.xlabel('learning rate')\n",
    "            plt.ylabel('loss')\n",
    "            plt.show()\n",
    "\n",
    "    @property\n",
    "    def lrs(self):\n",
    "        return np.array(self.history['log_lrs'])\n",
    "\n",
    "    @property\n",
    "    def losses(self):\n",
    "        return np.array(self.history['running_loss_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead(tf.keras.optimizers.Optimizer):\n",
    "    \"\"\"The lookahead mechanism for optimizers.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        optimizer: An existed optimizer.\n",
    "        sync_period: int > 0. The synchronization period.\n",
    "        slow_step: float, 0 < alpha < 1. The step size of slow weights.\n",
    "    # References\n",
    "        - [Lookahead Optimizer: k steps forward, 1 step back]\n",
    "          (https://arxiv.org/pdf/1907.08610v1.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, sync_period=5, slow_step=0.5, **kwargs):\n",
    "        super(Lookahead, self).__init__(**kwargs)\n",
    "        self.optimizer = tf.keras.optimizers.get(optimizer)\n",
    "        with tf.keras.backend.name_scope(self.__class__.__name__):\n",
    "            self.sync_period = tf.keras.backend.variable(sync_period, dtype='int64', name='sync_period')\n",
    "            self.slow_step = tf.keras.backend.variable(slow_step, name='slow_step')\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self.optimizer.lr\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, lr):\n",
    "        self.optimizer.lr = lr\n",
    "\n",
    "    @property\n",
    "    def iterations(self):\n",
    "        return self.optimizer.iterations\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        sync_cond = tf.keras.backend.equal((self.iterations + 1) % self.sync_period, 0)\n",
    "        if True:\n",
    "            slow_params = [tf.keras.backend.variable(tf.keras.backend.get_value(p), name='sp_{}'.format(i)) for i, p in enumerate(params)]\n",
    "            self.updates = self.optimizer.get_updates(loss, params)\n",
    "            slow_updates = []\n",
    "            for p, sp in zip(params, slow_params):\n",
    "                sp_t = sp + self.slow_step * (p - sp)\n",
    "                slow_updates.append(tf.keras.backend.update(sp, tf.keras.backend.switch(\n",
    "                    sync_cond,\n",
    "                    sp_t,\n",
    "                    sp,\n",
    "                )))\n",
    "                slow_updates.append(tf.keras.backend.update_add(p, tf.keras.backend.switch(\n",
    "                    sync_cond,\n",
    "                    sp_t - p,\n",
    "                    tf.keras.backend.zeros_like(p),\n",
    "                )))\n",
    "        else:\n",
    "            slow_params = {p.name: tf.keras.backend.variable(tf.keras.backend.get_value(p), name='sp_{}'.format(i)) for i, p in enumerate(params)}\n",
    "            update_names = ['update', 'update_add', 'update_sub']\n",
    "            original_updates = [getattr(K, name) for name in update_names]\n",
    "            setattr(K, 'update', lambda x, new_x: ('update', x, new_x))\n",
    "            setattr(K, 'update_add', lambda x, new_x: ('update_add', x, new_x))\n",
    "            setattr(K, 'update_sub', lambda x, new_x: ('update_sub', x, new_x))\n",
    "            self.updates = self.optimizer.get_updates(loss, params)\n",
    "            for name, original_update in zip(update_names, original_updates):\n",
    "                setattr(K, name, original_update)\n",
    "            slow_updates = []\n",
    "            for i, update in enumerate(self.updates):\n",
    "                if isinstance(update, tuple):\n",
    "                    name, x, new_x, adjusted = update + (update[-1],)\n",
    "                    update_func = getattr(K, name)\n",
    "                    if name == 'update_add':\n",
    "                        adjusted = x + new_x\n",
    "                    if name == 'update_sub':\n",
    "                        adjusted = x - new_x\n",
    "                    if x.name not in slow_params:\n",
    "                        self.updates[i] = update_func(x, new_x)\n",
    "                    else:\n",
    "                        slow_param = slow_params[x.name]\n",
    "                        slow_param_t = slow_param + self.slow_step * (adjusted - slow_param)\n",
    "                        slow_updates.append(tf.keras.backend.update(slow_param, tf.keras.backend.switch(\n",
    "                            sync_cond,\n",
    "                            slow_param_t,\n",
    "                            slow_param,\n",
    "                        )))\n",
    "                        self.updates[i] = tf.keras.backend.update(x, tf.keras.backend.switch(\n",
    "                            sync_cond,\n",
    "                            slow_param_t,\n",
    "                            adjusted,\n",
    "                        ))\n",
    "            slow_params = list(slow_params.values())\n",
    "        self.updates += slow_updates\n",
    "        self.weights = self.optimizer.weights + slow_params\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'optimizer': tf.keras.optimizers.serialize(self.optimizer),\n",
    "            'sync_period': int(tf.keras.backend.get_value(self.sync_period)),\n",
    "            'slow_step': float(tf.keras.backend.get_value(self.slow_step)),\n",
    "        }\n",
    "        base_config = super(Lookahead, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        optimizer = tf.keras.optimizers.deserialize(config.pop('optimizer'))\n",
    "        return cls(optimizer, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ported from https://github.com/LiyuanLucasLiu/RAdam/blob/master/radam.py\n",
    "class RectifiedAdam(tf.keras.optimizers.Optimizer):\n",
    "    \"\"\"RectifiedAdam optimizer.\n",
    "\n",
    "    Default parameters follow those provided in the original paper.\n",
    "\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        final_lr: float >= 0. Final learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        gamma: float >= 0. Convergence speed of the bound function.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `tf.keras.backend.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: Weight decay weight.\n",
    "        amsbound: boolean. Whether to apply the AMSBound variant of this\n",
    "            algorithm.\n",
    "\n",
    "    # References\n",
    "        - [On the Variance of the Adaptive Learning Rate and Beyond]\n",
    "          (https://arxiv.org/abs/1908.03265)\n",
    "        - [Adam - A Method for Stochastic Optimization]\n",
    "          (https://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond]\n",
    "          (https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., weight_decay=0.0, **kwargs):\n",
    "        super(RectifiedAdam, self).__init__(**kwargs)\n",
    "\n",
    "        with tf.keras.backend.name_scope(self.__class__.__name__):\n",
    "            self.iterations = tf.keras.backend.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = tf.keras.backend.variable(lr, name='lr')\n",
    "            self.beta_1 = tf.keras.backend.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = tf.keras.backend.variable(beta_2, name='beta_2')\n",
    "            self.decay = tf.keras.backend.variable(decay, name='decay')\n",
    "\n",
    "        if epsilon is None:\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "        self.weight_decay = float(weight_decay)\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [tf.keras.backend.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * tf.keras.backend.cast(self.iterations,\n",
    "                                                      tf.keras.backend.dtype(self.decay))))\n",
    "\n",
    "        t = tf.keras.backend.cast(self.iterations, tf.keras.backend.floatx()) + 1\n",
    "\n",
    "        ms = [tf.keras.backend.zeros(tf.keras.backend.int_shape(p), dtype=tf.keras.backend.dtype(p)) for p in params]\n",
    "        vs = [tf.keras.backend.zeros(tf.keras.backend.int_shape(p), dtype=tf.keras.backend.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * tf.keras.backend.square(g)\n",
    "\n",
    "            beta2_t = self.beta_2 ** t\n",
    "            N_sma_max = 2 / (1 - self.beta_2) - 1\n",
    "            N_sma = N_sma_max - 2 * t * beta2_t / (1 - beta2_t)\n",
    "\n",
    "            # apply weight decay\n",
    "            if self.weight_decay != 0.:\n",
    "                p_wd = p - self.weight_decay * lr * p\n",
    "            else:\n",
    "                p_wd = None\n",
    "\n",
    "            if p_wd is None:\n",
    "                p_ = p\n",
    "            else:\n",
    "                p_ = p_wd\n",
    "\n",
    "            def gt_path():\n",
    "                step_size = lr * tf.keras.backend.sqrt(\n",
    "                    (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max /\n",
    "                    (N_sma_max - 2)) / (1 - self.beta_1 ** t)\n",
    "\n",
    "                denom = tf.keras.backend.sqrt(v_t) + self.epsilon\n",
    "                p_t = p_ - step_size * (m_t / denom)\n",
    "\n",
    "                return p_t\n",
    "\n",
    "            def lt_path():\n",
    "                step_size = lr / (1 - self.beta_1 ** t)\n",
    "                p_t = p_ - step_size * m_t\n",
    "\n",
    "                return p_t\n",
    "\n",
    "            p_t = tf.keras.backend.switch(N_sma > 5, gt_path, lt_path)\n",
    "\n",
    "            self.updates.append(tf.keras.backend.update(m, m_t))\n",
    "            self.updates.append(tf.keras.backend.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(tf.keras.backend.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(tf.keras.backend.get_value(self.lr)),\n",
    "                  'beta_1': float(tf.keras.backend.get_value(self.beta_1)),\n",
    "                  'beta_2': float(tf.keras.backend.get_value(self.beta_2)),\n",
    "                  'decay': float(tf.keras.backend.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'weight_decay': self.weight_decay}\n",
    "        base_config = super(RectifiedAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, labels, encoder, test_size=0.2):\n",
    "    encodings = np.load(path)\n",
    "    X = np.expand_dims(encodings, 2)\n",
    "    y = encoder.transform(labels)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=16)\n",
    "    \n",
    "    del encodings\n",
    "    del X\n",
    "    gc.collect()\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_balanced_accuracy(X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "    \n",
    "    cl = classification_report(y_val, y_pred, output_dict=True)\n",
    "\n",
    "    recall_c = 0\n",
    "\n",
    "    for key in cl.keys():\n",
    "        recall_c += cl[key]['recall']\n",
    "\n",
    "    return recall_c / categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_balanced_accuracy_chunks(model, X_val, y_val, mc_dropout=False):\n",
    "    y_preds = np.array([])\n",
    "    \n",
    "    if mc_dropout is False:\n",
    "        for chunk in chunks(X_val, 1000000):\n",
    "            y_pred = model.predict(chunk)\n",
    "            y_pred = np.argmax(y_pred, axis=-1)\n",
    "            y_preds = np.append(y_preds, y_pred)\n",
    "    else:\n",
    "        num_models = 4\n",
    "        for chunk in chunks(X_val, int(1000000/num_models)):\n",
    "            y_probas = np.stack([model.predict(chunk) for sample in range(num_models)])\n",
    "            y_proba = y_probas.mean(axis=0)\n",
    "            y_proba = np.argmax(y_proba, axis=-1)\n",
    "            y_preds = np.append(y_preds, y_proba)\n",
    "            del y_probas\n",
    "            gc.collect()\n",
    "        \n",
    "    cl = classification_report(y_val, y_preds, output_dict=True)\n",
    "\n",
    "    recall_c = 0\n",
    "\n",
    "    for key in cl.keys():\n",
    "        recall_c += cl[key]['recall']\n",
    "\n",
    "    return recall_c / categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(tf.keras.callbacks.Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency.\n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored\n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or\n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "\n",
    "    # Example for CIFAR-10 w/ batch size 100:\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "\n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "\n",
    "    # References\n",
    "\n",
    "      - [Cyclical Learning Rates for Training Neural Networks](\n",
    "      https://arxiv.org/abs/1506.01186)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_lr=0.001,\n",
    "            max_lr=0.006,\n",
    "            step_size=2000.,\n",
    "            mode='triangular',\n",
    "            gamma=1.,\n",
    "            scale_fn=None,\n",
    "            scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2',\n",
    "                        'exp_range']:\n",
    "            raise KeyError(\"mode must be one of 'triangular', \"\n",
    "                           \"'triangular2', or 'exp_range'\")\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1 / (2.**(x - 1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma ** x\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr is not None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr is not None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size is not None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
    "                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
    "                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "        self.history.setdefault(\n",
    "            'lr', []).append(\n",
    "            tf.keras.backend.get_value(\n",
    "                self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = tf.keras.backend.get_value(self.model.optimizer.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname, vocab_size=None):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    i = 0\n",
    "    \n",
    "    for line in fin:\n",
    "        #tokens = line.rstrip().split(' ')\n",
    "        #data[tokens[0]] = map(float, tokens[1:])\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float16')\n",
    "        data[word] = coefs\n",
    "        \n",
    "        i+= 1\n",
    "        \n",
    "        if vocab_size is not None and i == vocab_size:\n",
    "            break\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_title(title):\n",
    "    return unicodedata.normalize('NFKD', title.lower()).encode('ASCII', 'ignore').decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, name):\n",
    "    with open(name + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(name):\n",
    "    with open(name + '.pickle', 'rb') as handle:\n",
    "        obj = pickle.load(handle)\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    model.save('model_' + str(name))\n",
    "    model.save_weights('model_weights_' + str(name))\n",
    "    \n",
    "def load_model(model, name):\n",
    "    model.load_weights('model_weights_' + str(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/axel/ssd/ml-challenge/'\n",
    "df = pd.read_csv(path + 'train_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = set(stopwords.words('spanish')).union(set(stopwords.words('portuguese')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(path + 'train.csv')\n",
    "# df['title'] = df.title.apply(normalize_title)\n",
    "# df = df[~df.title.isna() & (df.title != 'nan') & (df.title != '')]\n",
    "\n",
    "# df['title'] = df['title'].str.split(' ').apply(lambda sentence: ' '.join(word for word in sentence if word not in stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(path + 'test_processed.csv')\n",
    "# df_test = pd.read_csv(path + 'test.csv')\n",
    "# df_test['title'] = df_test.title.apply(normalize_title)\n",
    "# df_test['title'] = df_test['title'].str.split(' ').apply(lambda sentence: ' '.join(word for word in sentence if word not in stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc = BertClient()\n",
    "# titles = df['title'].iloc[19000000:].str.lower().values.tolist()\n",
    "# encodings = bc.encode(titles)\n",
    "# np.save('/media/axel/ssd/ml-challenge/large/encodings_3_layers_19', encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_1 = 'encodings_0.npy'\n",
    "# enc_2 = 'encodings_1.npy'\n",
    "# new_enc = 'encodings_3_layers_0-2.npy'\n",
    "# encodings1 = np.load('/media/axel/ssd/ml-challenge/' + enc_1)\n",
    "# encodings2 = np.load('/media/axel/ssd/ml-challenge/' + enc_2)\n",
    "# encodings = np.vstack((encodings1, encodings2))\n",
    "# np.save('/media/axel/ssd/ml-challenge/' + new_enc, encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('/media/axel/ssd/ml-challenge/encodings_1', encodings)\n",
    "#  encodings = np.load('/media/axel/ssd/ml-challenge/encodings_0.npy')\n",
    "# encodings = np.expand_dims(encodings, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc = BertClient()\n",
    "# titles = df['title'].iloc[:50000].str.lower().values.tolist()\n",
    "# encodings = bc.encode(titles, show_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings_post = []\n",
    "\n",
    "# for token_list in encodings[1]:\n",
    "#     enc_idx = 0\n",
    "#     i = 0\n",
    "#     embeddings = []\n",
    "    \n",
    "#     for token in token_list:\n",
    "#         if token not in ['[CLS]', '-', ',', '/', '[UNK]', '[SEP]']:\n",
    "#             #print(token)\n",
    "#             #print(enc[0][0][i])\n",
    "#             embeddings.append(encodings[0][enc_idx][i])\n",
    "#         i += 1\n",
    "    \n",
    "#     embeddings = np.array(embeddings)\n",
    "#     encodings_post.append(embeddings)\n",
    "#     enc_idx += 1\n",
    "    \n",
    "# encodings = encodings_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 120000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic_es = load_vectors(path+'cc.es.300.vec')\n",
    "# dic_pt = load_vectors(path+'cc.pt.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = list(dic_es.keys())\n",
    "\n",
    "# for key in keys:\n",
    "#     if key in stopwords:\n",
    "#         del dic_es[key]\n",
    "        \n",
    "# keys = list(dic_pt.keys())\n",
    "\n",
    "# for key in keys:\n",
    "#     if key in stopwords:\n",
    "#         del dic_pt[key]\n",
    "        \n",
    "# common_keys = set(dic_es.keys()).intersection(set(dic_pt.keys()))\n",
    "\n",
    "# for key in dic_pt.keys():\n",
    "#     if key in common_keys:\n",
    "#         dic_pt[key + '_pt'] = dic_pt.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = {**dic_es, **dic_pt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df.language == 'portuguese', 'title'] = df[df.language == 'portuguese']['title'].str.split(' ').apply(lambda sentence: ' '.join(word if word not in common_keys else word + '_pt' for word in sentence))\n",
    "# df_test.loc[df_test.language == 'portuguese', 'title'] = df_test[df_test.language == 'portuguese']['title'].str.split(' ').apply(lambda sentence: ' '.join(word if word not in common_keys else word + '_pt' for word in sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(path + 'train_processed.csv', index=None)\n",
    "# df_test.to_csv(path + 'test_processed.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderCat = LabelEncoder()\n",
    "encoderCat.fit(df['category'])\n",
    "y = df['category']\n",
    "y = encoderCat.transform(y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(df, y, test_size=0.2, random_state=16)\n",
    "# _, df_sample = train_test_split(df, test_size=2000000, random_state=42, stratify=df.category)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(df_sample, df_sample['category'], test_size=0.2, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles_train = df['title'].astype(str).values.tolist()\n",
    "# titles_test = df_test['title'].astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "# tokenizer.fit_on_texts(titles_train + titles_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_seq_train = tokenizer.texts_to_sequences(X_train['title'].astype(str))\n",
    "# word_seq_val = tokenizer.texts_to_sequences(X_val['title'].astype(str))\n",
    "# word_seq_test = tokenizer.texts_to_sequences(df_test['title'])\n",
    "\n",
    "# word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "# word_seq_val = sequence.pad_sequences(word_seq_val, maxlen=max_seq_len)\n",
    "# word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_object(tokenizer, name='tokenizer_sample')\n",
    "# save_object(word_seq_test, name='word_seq_test_sample')\n",
    "# save_object(word_seq_train, name='word_seq_train_sample')\n",
    "# save_object(word_seq_val, name='word_seq_val_sample')\n",
    "\n",
    "tokenizer = load_object(name='tokenizer_proc')\n",
    "word_seq_test = load_object(name='word_seq_test_proc')\n",
    "word_seq_train = load_object(name='word_seq_train_proc')\n",
    "word_seq_val = load_object(name='word_seq_val_proc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 300\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.random.normal(0.0, 0.1, (nb_words, EMB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word, i in word_index.items():\n",
    "#     if i >= nb_words:\n",
    "#         continue\n",
    "#     embedding_vector = dic.get(word)\n",
    "#     if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_object(embedding_matrix, 'embedding_matrix_proc')\n",
    "\n",
    "embedding_matrix = load_object('embedding_matrix_proc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_train, X_val, df\n",
    "# del dic, dic_es, dic_pt\n",
    "# del titles_train, titles_test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoderCat = LabelEncoder()\n",
    "\n",
    "# encoderCat.fit(df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings = np.load('/media/axel/ssd/ml-challenge/encodings_3_layers_0-2.npy')\n",
    "# encodings = np.expand_dims(encodings, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoderCat = LabelEncoder()\n",
    "# y = df['category'].iloc[:len(encodings)]\n",
    "# y = encoderCat.transform(y)\n",
    "# #y2 = df['label_quality'].iloc[:len(encodings)]\n",
    "# #y2 = LabelEncoder().fit_transform(y2)\n",
    "# #y = np.vstack((y1, y2)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[df.language == 'spanish']\n",
    "\n",
    "# for i in range(len(X)):\n",
    "#     X[i] = tf.keras.preprocessing.sequence.pad_sequences(X[i].T, maxlen=50, padding='post', dtype='float16').T\n",
    "    \n",
    "# X = np.array(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df.copy()\n",
    "# df2 = pd.concat([df2, pd.get_dummies(df['label_quality'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {}\n",
    "# prcts = []\n",
    "# for cat in df.category:\n",
    "#     cat_reliable = df2[(df2['category'] == cat)]['reliable'].sum()\n",
    "#     cat_reliable_percentage = cat_reliable / len(df2[(df2['category'] == cat)])\n",
    "#     prcts.append(cat_reliable_percentage)\n",
    "    \n",
    "# cats = encoderCat.transform(df.category)\n",
    "\n",
    "# for cat in cats:\n",
    "#     d[cat] = prcts[0]\n",
    "#     prcts.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(tf.keras.layers.Layer):\n",
    "   \n",
    "    def __init__(self):\n",
    "        super(Mish, self).__init__()\n",
    "    \n",
    "    \n",
    "    def call(self, x):\n",
    "        return x * tf.math.tanh(tf.math.softplus(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(tf.keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_encodings = 768*3\n",
    "categories = 1588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_activation(activation):\n",
    "    if activation == 'relu':\n",
    "        return tf.keras.layers.ReLU()\n",
    "    elif activation == 'prelu':\n",
    "        return tf.keras.layers.PReLU()\n",
    "    elif activation == 'mish':\n",
    "        return Mish()\n",
    "    elif activation == 'elu':\n",
    "        return tf.keras.layers.ELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_filters1, num_filters2, num_filters3, activation, dropout, spatial_dropout, num_dense1, num_dense2, pooling, num_rnn1, num_rnn2, num_rnn3, activation_rnn, dropout_rnn, recurrent_dropout_rnn):\n",
    "    inp = tf.keras.Input(shape=(max_seq_len,))\n",
    "    emb = tf.keras.layers.Embedding(nb_words, EMB_SIZE, weights=[embedding_matrix], input_length=max_seq_len,\n",
    "                                    trainable=False)(inp)\n",
    "    emb = tf.keras.layers.SpatialDropout1D(spatial_dropout)(emb)\n",
    "    x = tf.keras.layers.Conv1D(filters=num_filters1, kernel_size=3)(emb)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Mish()(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=num_filters2, kernel_size=3)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Mish()(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=num_filters3, kernel_size=3)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Mish()(x)\n",
    "    x1 = None\n",
    "\n",
    "    if pooling == 'no':\n",
    "        x1 = tf.keras.layers.Flatten()(x)\n",
    "    else:\n",
    "        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "        max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "        x1 = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
    "\n",
    "\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(num_rnn1, activation=activation_rnn, dropout=dropout_rnn, recurrent_dropout=recurrent_dropout_rnn, return_sequences=True))(emb)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(num_rnn2, activation=activation_rnn, dropout=dropout_rnn, recurrent_dropout=recurrent_dropout_rnn, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNGRU(num_rnn3, return_sequences=True))(x)\n",
    "    x2 = None\n",
    "\n",
    "    if pooling == 'no':\n",
    "        x2 = tf.keras.layers.Flatten()(x)\n",
    "    else:\n",
    "        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "        max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "        x2 = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
    "\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(num_rnn1, activation=activation_rnn, dropout=dropout_rnn, recurrent_dropout=recurrent_dropout_rnn, return_sequences=True))(emb)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(num_rnn2, activation=activation_rnn, dropout=dropout_rnn, recurrent_dropout=recurrent_dropout_rnn, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNLSTM(num_rnn3, return_sequences=True))(x)\n",
    "    x3 = None\n",
    "\n",
    "    if pooling == 'no':\n",
    "        x3 = tf.keras.layers.Flatten()(x)\n",
    "    else:\n",
    "        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "        max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "        x3 = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([x1, x2, x3])\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Dense(num_dense)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Mish()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    output1 = tf.keras.layers.Dense(categories, activation=tf.nn.softmax, name='output1')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = inp, outputs = output1)\n",
    "    optimizer = Lookahead(RectifiedAdam())\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/axel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/axel/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "inp = tf.keras.Input(shape=(max_seq_len,))\n",
    "emb = tf.keras.layers.Embedding(nb_words, EMB_SIZE, weights=[embedding_matrix], input_length=max_seq_len,\n",
    "                                trainable=False)(inp)\n",
    "# emb = tf.keras.layers.SpatialDropout1D(0.1)(emb)\n",
    "x = tf.keras.layers.Conv1D(filters=256, kernel_size=3)(emb)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x = tf.keras.layers.Conv1D(filters=512, kernel_size=3)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x = tf.keras.layers.Conv1D(filters=1024, kernel_size=3)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x1 = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "x = tf.keras.layers.SeparableConv1D(filters=256, kernel_size=3)(emb)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x = tf.keras.layers.SeparableConv1D(filters=512, kernel_size=3)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x = tf.keras.layers.SeparableConv1D(filters=1024, kernel_size=3)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x2 = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "x = tf.keras.layers.Concatenate()([x1, x2])\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "# x = MCDropout(0.2)(x1)\n",
    "x = tf.keras.layers.Dense(800)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = Mish()(x)\n",
    "# x = tf.keras.layers.PReLU()(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "# x = MCDropout(0.2)(x)\n",
    "output1 = tf.keras.layers.Dense(categories, activation=tf.nn.softmax, name='output1')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.Model(inputs = inp, outputs = [output1, output2])\n",
    "model = tf.keras.Model(inputs = inp, outputs = output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = {\n",
    "#     \"output1\": \"sparse_categorical_crossentropy\",\n",
    "#     \"output2\": \"binary_crossentropy\",\n",
    "# }\n",
    "# lossWeights = {\"output1\": 1.0, \"output2\": 1.0}\n",
    "optimizer = Lookahead(RectifiedAdam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})\n",
    "# model.compile(optimizer=RectifiedAdam(), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, batch_size = 256):\n",
    "    indices = np.arange(len(X)) \n",
    "    batch=[]\n",
    "    while True:\n",
    "            # it might be a good idea to shuffle your data before each epoch\n",
    "            np.random.shuffle(indices) \n",
    "            for i in indices:\n",
    "                batch.append(i)\n",
    "                if len(batch)==batch_size:\n",
    "                    yield X[batch], Y[batch]\n",
    "                    batch=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 800\n",
    "train_generator = batch_generator(word_seq_train, y_train, batch_size = batch_size)\n",
    "val_generator = batch_generator(word_seq_val, y_val, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study()\n",
    "# study.optimize(objective, n_trials=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decay=1\n",
    "# clr = CyclicLR(base_lr=0.001*decay, max_lr=0.002*decay, step_size=30000., mode='triangular2')\n",
    "clr = OneCycle(lr_range=(0.002/25., 0.002),\n",
    "               momentum_range=(0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15999998 samples, validate on 4000000 samples\n",
      "WARNING:tensorflow:From /home/axel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/axel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "15999998/15999998 [==============================] - 2674s 167us/sample - loss: 1.4415 - sparse_categorical_accuracy: 0.7322 - val_loss: 0.7250 - val_sparse_categorical_accuracy: 0.8451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ecfb807f0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train, validation_data=(word_seq_val, y_val),\n",
    "           batch_size=1500, epochs=1, verbose=1, callbacks=[clr])\n",
    "\n",
    "# size = 2000000\n",
    "# model.fit(word_seq_train[:size], y_train[:size], validation_data=None,\n",
    "#            batch_size=2048, epochs=1, verbose=0,)# callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_balanced_accuracy_chunks(model, word_seq_val[:size], y_val[:size]) #0.411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.815293769374877"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss: 1.0868 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.7825 - val_sparse_categorical_accuracy: 0.8334 prelu + la\n",
    "#loss: 1.1268 - sparse_categorical_accuracy: 0.7744 - val_loss: 0.8179 - val_sparse_categorical_accuracy: 0.8259 mish + la\n",
    "\n",
    "#loss: 1.0426 - sparse_categorical_accuracy: 0.7826 - val_loss: 0.7278 - val_sparse_categorical_accuracy: 0.8415 con adam\n",
    "\n",
    "calculate_balanced_accuracy_chunks(model, word_seq_val, y_val) #0.8182 clr nuevo, 0.811 adam, 0.8294 clr + ranger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 'vec3')\n",
    "# load_model(model, 'vec3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].trainable = True\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics={\"output1\": tf.keras.metrics.sparse_categorical_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_callback = LRFinder(num_samples=len(word_seq_train[:3000000]), batch_size=450, lr_scale='exp', verbose=False)\n",
    "\n",
    "# model.fit(word_seq_train[:3000000], y_train[:3000000], validation_data=None,\n",
    "#            batch_size=450, epochs=1, verbose=1, callbacks=[lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXeYG9W5/79HfaXtxb2sbWyMbTDFBUwzvYUQ7iUJTiWBkATIj5sOCS3lEhJCKuQSXy4hkARIbnIpwZRQHIOxAZtg48563dZte5NWZaTz+2PmjM40SburLdK+n+fxY+1oNHNGmvme97zve97DOOcgCIIgigvXSDeAIAiCyD8k7gRBEEUIiTtBEEQRQuJOEARRhJC4EwRBFCEk7gRBEEUIiTtBEEQRQuJOEARRhJC4EwRBFCGekTpxbW0tr6+vH6nTEwRBFCQbN25s5ZzXZdtvxMS9vr4eGzZsGKnTEwRBFCSMsX257EduGYIgiCKExJ0gCKIIIXEnCIIoQkjcCYIgihASd4IgiCKExJ0gCKIIIXEnCIIoQrKKO2PsYcZYM2Nsi8P7FYyxZxljmxhjWxljn8t/M9PsPNKD+17aibbe2FCehiAIoqDJxXJ/BMDFGd6/EcA2zvlCAMsB3McY8w2+afY0NPfi1682oC0cH6pTEARBFDxZxZ1zvgZAe6ZdAJQxxhiAUm1fJT/Ns+J2MQCAkqSFvQmCIJzIh8/9fgDHATgE4H0AN3POU3Y7MsauZ4xtYIxtaGlpGdDJhLgnUyTuBEEQTuRD3C8C8B6ASQBOBHA/Y6zcbkfO+UrO+SLO+aK6uqx1b2zxCMs9Zdt/EARBEMiPuH8OwN+4SgOAPQDm5uG4tgjLPcXJcicIgnAiH+K+H8B5AMAYGw/gWACNeTiuLR7yuRMEQWQla8lfxtjjULNgahljTQDuBOAFAM75gwB+AOARxtj7ABiAb3POW4eqwS7yuRMEQWQlq7hzzldkef8QgAvz1qIsCMs9yTl2He1BT1TBKdOrhuv0BEEQBcGILdYxUPRUyBTHhT9fAwDYe89lI9kkgiCIUUfBlR/QUyHJ504QBOFIwYq7Qj53giAIRwpO3D0utcmUCkkQBOFMwYk7We4EQRDZKVhxT9IMVYIgCEcKTtxFKuRXn9w0wi0hCIIYvRScuAvLnSAIgnCmKMT9/J/9cwRaQhAEMXopCnFvaO4dgZYQBEGMXgpO3D3kliEIgshKwYm7k89dSVL2DEEQhKBoxD2SSA5zSwiCIEYvRSPufXESd4IgCEHBibsoP2AmHBuyNbkJgiAKjoITd6d4aoQsd4IgCJ2CE3fGHNwy5HMnCILQKThxd4LcMgRBEGmKRtwpoEoQBJGmaMSdfO4EQRBpsoo7Y+xhxlgzY2xLhn2WM8beY4xtZYyNSKGXn7y4YyROSxAEMSrJxXJ/BMDFTm8yxioB/AbAhznn8wF8ND9N6x/NPbGROC1BEMSoJKu4c87XAGjPsMsnAPyNc75f2785T23rFy6HLBqCIIixSD587nMAVDHGVjPGNjLGPpOHY/abJC27RxAEoePJ0zFOAXAegBIA6xhj6znnu8w7MsauB3A9AEybNi0PpzaSSnG4qGokQRBEXiz3JgAvcM7DnPNWAGsALLTbkXO+knO+iHO+qK6uLg+nNpLkZL0TBEEA+RH3pwGcyRjzMMaCAJYC2J6H4/Ybcs0QBEGoZHXLMMYeB7AcQC1jrAnAnQC8AMA5f5Bzvp0x9gKAzQBSAB7inDumTQ4lKbLcCYIgAOQg7pzzFTnscy+Ae/PSokFAljtBEIRKQc5Q/foFc2y3k7YTBEGoFKS4X3HiZNvtKVJ3giAIAAUq7k7zlShbhiAIQqWoxJ0sd4IgCJWCFHenUgNkuRMEQagUpLg7umXIcicIggBQoOLuZLmnUsPcEIIgiFFKQYq7U/UYmsREEAShUpjiTj53giCIjBSouNtvp2wZgiAIlYIUdyef+x/f2j/MLSEIghidFKS4O/ncH3lzL1p7abk9giCIghT3TEvqUVCVIAiiQMWdZWg1c7TrCYIgxg6FKe4Z3uNkuRMEQRSmuGdyyyhaxgznnISeIIgxS0GKewZt10sQrPjv9Zhx66phahFBEMTooiDF3Wy5u13pv4W4r29sH9Y2EQRBjCYKUtzNyOKu0EQmgiCIwhR3i+XOrJY7QRDEWKYgxd3sc5cMdyhUGpIgCCK7uDPGHmaMNTPGtmTZbzFjLMkYuyp/zbPHbLm7JHUnbScIgsjNcn8EwMWZdmCMuQH8GMCLeWhTVlwmy91j8LmTuhMEQWQVd875GgDZUk++AuCvAJrz0ahsmEv+2mXLEARBjGUG7XNnjE0GcCWABwffnIEhu2koW4YgCCI/AdVfAPg25zyZbUfG2PWMsQ2MsQ0tLS15OLWKx+BzJ3EnCILw5OEYiwA8oblKagFcyhhTOOdPmXfknK8EsBIAFi1alDcVdlGeO0EQhIFBizvnfIZ4zRh7BMDf7YR9KCGfO0EQhJGs4s4YexzAcgC1jLEmAHcC8AIA53zE/OwybvK5EwRBGMgq7pzzFbkejHN+zaBaM0BcZLkTBEEYKMgZqmY8JO4EQRAGikLcjamQNImJIAiiKMT9kgUT9NdkuRMEQRSBuL9/14X49GnT9b9J3AmCIIpA3Ev9nowBVVpqjyCIsUjBiztjLGP5AdJ2giDGIgUv7kDmxTpSpO4EQYxBikLcXdJVJFMcaxta9b/JBU8QxFikKMTdbLl/8qG39L/JcicIYixSHOKeoXAYaTtBEGORohB3ZrDcjZOYyHInCGIsUhTiDgCNd18KAEiaJqiSuBMEMRYpGnF3uRgYs1ruJO0EQYxFikbcATWwavG5U6kZgiDGIMUl7i6G36zebdiW4hzd0QRiStZVAAmCIIqGohJ3ufSvIMU5TrjrJVy9cj0amnuxYW/7CLSMIAhieMnHGqqjBretuKv//2t/J87/2T8BAHvvuWw4m0UQBDHsFJXlbifukbgyAi0hCIIYWYpK3L1u6+W09sYs2w60R/Cj57dTxUiCIIqWohJ3v9dO3OOWbV/6w0b89p+N2Hm0ZziaRRAEMewUlbgHPG7LNjvLPWGe6UQQBFFkZBV3xtjDjLFmxtgWh/c/yRjbrP17kzG2MP/NzA07t0ybjeXOYPXNEwRBFBO5WO6PALg4w/t7AJzNOT8BwA8ArMxDuwaEy+Zq2sNWcec0b5UgiCInayok53wNY6w+w/tvSn+uBzBl8M0aGHYWeW/MOVuGLHiCIIqVfPvcrwXwvNObjLHrGWMbGGMbWlpa8nxqgNlotUiFLPWn+zFKkiEIotjJm7gzxs6BKu7fdtqHc76Sc76Ic76orq4uX6dOt0F6febsWgBAOJZEqd+DU2fWWPcnw50giCIlLzNUGWMnAHgIwCWc87Z8HHOA7bC8jsQVBLxuhPzWTBqCIIhiZdCWO2NsGoC/Afg053zX4Js0mLakX4sJSuFYEiU+F0KyW2a4G0YQBDHMZLXcGWOPA1gOoJYx1gTgTgBeAOCcPwjgDgA1AH6jWcsK53zRUDU4Y1tttkXiCkq8bvg9RZXSTxAEkZFcsmVWZHn/OgDX5a1Fg4DZONH3tkWwcEqFbQ48rdJEEESxUlTmrE3dMACA3+uG121TMZImqhIEUaQUlbg75a2nUhwemxlOZLkTBFGsFJW4y9p+yvQq/XVvTLG13EnbCYIoVopqsQ4h3498brHB/x6OK/BIPneRSXO0O4rjUTGcTSQIghgWispyry3zA1Bno8r+93AsabsE33WPbsD6xhFLyycIghgyikrc777yeNx1+TycMr0KLsly740quHzhJP1v2Ruz6UDnMLaQIAhieChYt8x7d1xg8ZlXlHhxzekzABgnNJ0xuxbjywO4+bzZ+OUrHxjUnWq7EwRRjBSsuFcGfRnfly33Bz5xsmGb3CckkhRVJQii+Cgqt4yMLO4lPre2Tf07mUoLukLJ7gRBFCFFLO4227SNcn47We4EQRQjRSvudqUIxCZOPneCIIqcohV3W8udWS13hSx3giCKkCIWd6u62/ncyXInCKIYGWPiLiz39LY4iTtBEEVI0Yq70Ha35J9xcsus3tmMlp7YsLaPIAhiKCl+cWeyuKv/y+Lel0jimt+9g0//z1vD2TyCIIghpWjFXVjpBstdpEJKfplwTAEA7DjSg/pbnsOB9sgwtpIgCGJoGFPizmx87kLcBat3tQx94wiCIIaYIhZ34//ya9kt02sSd6/Tck4EQRAFRNGKu7DS5TrudgHVcCxp+JybxJ0giCKgaMU9bbnbBVTT+4XjRsvdY7NiE0EQRKGRVdwZYw8zxpoZY1sc3meMsV8xxhoYY5sZYyfnv5n9J+1zT28T1jzn1oCqgGrNEARRDORiuT8C4OIM718CYLb273oA/zX4Zg0eXdyZNc9dnqGaMml5LGF00xAEQRQiWcWdc74GQHuGXa4A8ChXWQ+gkjE2MV8NHChcq9rucmV2y5iJJmjGKkEQhU8+fO6TARyQ/m7StllgjF3PGNvAGNvQ0jK0KYfCOvfYzFDNRJQsd4IgioB8iLudYtraxpzzlZzzRZzzRXV1dXk4tTMiI8ZlyHPP/rmokgTnHBFToJUgCKKQyIe4NwGYKv09BcChPBx3UIh6YHY+90xEEymsXNOIeXe8SPVmCIIoWPIh7s8A+IyWNXMqgC7O+eE8HHdQBLWl9Y6dUKZvy0Xc/+eNPXh03T4AwNHu6NA0jiAIYojJukA2Y+xxAMsB1DLGmgDcCcALAJzzBwGsAnApgAYAEQCfG6rG9oep1UH88bqlOGlapb4t1/lJBzv7hqhVBEEQw0NWceecr8jyPgdwY95alEdOP6bW8Lfd0nuZkGeyEgRBFBJFO0PVjv5WFqC0SIIgCpUxJu79U/c+SoskCKJAGVvi3s+rpZx3giAKlTEl7tl87pMqAoa/SdwJgihUxpS4Z3PLfOviuYa/C1nc97dFcNHP16C1d/Tn6t//6geov+U5Q80fgiAGxxgTd+u2l792lv66LGBMHrrtqS2WqpGFwo4j3dh5tAf72sIj3ZSs/OqVBgBAXKEANkHkizEm7lZ1P2ZcGRZOVXPhgz6PYbGORJLjsfX7hq19+SSmCaVSACWMRZE3JUXiThD5YkyJu5NXJqEJYdDnNhQaA4ASr3uomzUkCHEvBFeHaGIhdEQEUSiMKXF38rkLi9HvdVnEvTLo1V+/uPUI+uKF4YcX8YJEQYi72sYEWe4EkTdI3JG2GD0ul2HNVQDwan+v292GLz62ET9/edfQNjJPpC330S+YnCx3gsg7Y0zc7bfXlPoAACU+N7ymNVTbemN4q7ENO450AwB6CyTAGlNUy72QBDORHP0dEUEUCllryxQTTnnuD3zyZLy2oxmTK0vgMc10uv3prQCAz5w2HYA1F360EtNKJyjD5JbhnINzY/38/jIa16/94GgPdreEcfGCCSPdFILoF2S5AxhXFsDHF08DAEO2jExDcy+A/hcfGyn0bJlhEvcb/vguZn5n1aCOMRqzZS74+Rp86Q8bR7oZBNFvxpi4Zxdm4Za5aP54w/bDXWpt90KZ2CTcMsPlc39+y5FBH6OQXEgEMdohcTchAqr1tSHD9lZtVab+ivsf1u/DS1sHL3z9pZDy3AWj2efOqfwzUWCMKXHPxaMiUiHL/MZwRI8WSLUrA/zS1iP49v9utj3ebU9twfWPDf+wfrh97vlgNLd1NMYDCCITY0rcc3PLqF9Jqd8+1vxGQytOuOtFdEUSaGjuxTW/exvXP7YRT244YNivO5oYfIMHgZ4tM4oF08xos9zFd2h+TRCFwNgS9xyuVgRUSwNe2/f3tIbRHVWw9VAXvvfsVqze2WJ4/82GVryw5QhOuOslbNzXYXivMxJH/S3P4fG39w/sAvqBnuc+zII5GPfFaHMhyQuk08ItQ0NXXwJfemwj2gqgwF2hMbbEvR8BVSfLXRDyeyyzWbce6sInHnpLz65YvbPZ8P6BdnVt1j8MQ72a4c6WEQzGfdHfbJn3m7qGtLxCsyTuZLkPDVsPduGFrUewqalzpJtSdIwxcc++j8hzL/G5HdMiBW7TUOBod9Twt2z5AWnxMncK+SAcU5CShC6WSLtl3m/qwkOvN+b9nHYMRmz70zEc7urD5fe/gX9sOzrg82WjWfo9yXIfGsJaOY9wjDrPfJOTuDPGLmaM7WSMNTDGbrF5fxpj7DXG2L8YY5sZY5fmv6mDJ5ccdY9muQc81jozMkoqZXm/L24UgL1Sud09rWF8/pF3APR/oo/I0GntjaGpI2L7/vw7X8S9L+3Ut8mFwy6//w388LntBvEfKgZTH6Y/bpnuPjXA3TKEw/kWstyHnEhcMfxP5I+s4s4YcwN4AMAlAOYBWMEYm2fa7TYAf+acnwTgagC/yXdD84Fwy0ytLnHcRwh2wOvWg6t2xBUOt6lUgTlNUhaHc366Gh2RhOEcufCPbUcx9/YXsOVgF8748as448evWfbZ16YK/t83H9K3CXGXg5RdfUMf5E1mEejuaAL1tzynu6beO5AejvfHLSOuq2cIA9dhqUgcWe5DgyjE10uWe97JxXJfAqCBc97IOY8DeALAFaZ9OIBy7XUFgEMYxWTyvYs894DXrVvxArnuTCJptdwjJnGPOSw+kc3dI/Oa5rd/70CnLjDmoOWeVnX27PiydGmE9CQmjnJtEZK2sNXKPdTZl3NbAGB9Yxs+9dBbUBwCtdks98Odqqvj92/uRUxJ4iMPrE1/th+Wu4gl9EaHzuKTFw8hy31oEB1opEBqNhUSuYj7ZABynl+Ttk3mLgCfYow1AVgF4Ct5aV2eEf5gdwZxFwJe4nVb6szUhPz6ayWVsoh0n2lo6STu5uNmQpxClr22cNywz+4W1f0zvlwSdynPvUIrW9zWa/zck+/sx7J7XsWmA7kHs25+4l94o6EVNz/xHjbsbbe8n83nrv8GLmYpn+zUYQhe3nZUH76LfYeykJtB3AvYcj/+zhdx74s7RroZtohnJlwgpbQLiVxUxk4JzU/wCgCPcM6nALgUwGOMMcuxGWPXM8Y2MMY2tLS0mN8ecoTFm8n1LoQ34HVZKkSK6pGA6pYxW+7CDyxwms1q9rknkik0NPfY7su0r1+21g92GK3tPa1hsbOO7HOvKFHFvd3UKfx982EA1sBvJkTH+Nz7h3HVg+ss72fzm4va7S7GEDE90Jny3Dfu68B1j27AvS/u1PbNj+WuJFP41Ssf2M5LKJY8956Yggde241H1u7BgXZrzEYmmeLYecT+XhwKxD1QyD73VIoPqXtwoOQi7k0Apkp/T4HV7XItgD8DAOd8HYAAgFrzgTjnKznnizjni+rq6gbW4kGQ5Gmr0Qkh2H4bt4xsiauWu/HrMwuELF7HT67QX5sDmz9atQPn/2yNbbBUNDWV4vB71PMdNLlSxCLYsiUsxCiRTKGyRO2UzBa/cMn4PP0YSWRxKWVLvVQky90q7s6f3XZYLbks1rQV/vmeQVruq7Ycwc/+sQv3vbjT8p5suQ+Hzz0SV/DYur15LXUgH+uuZ7fhqX8dzLj/81sO45JfrjFkCvWH9nAcbza05rx/RPe5F664P7PpEJbd8+qoqzuVy1P9DoDZjLEZjDEf1IDpM6Z99gM4DwAYY8dBFffhN82zINzBmX3uIqDqglcS74evWYQ540v1vxPJFMzx1kfXGfPXZReFvFyf2Up5R3NvCLdJc3cUb+9Rt4kMn0SSo65MdQuZLXchQvJi3sKNkExxlJeoPnez5X5I8387uY/syBYvyOZaEW11uZjlYcgUUBUW59SqIID8BVQ7tO/ErlOKJwfnc391x1FsOdiV8/7/+dx23P70Vqz5IHdxzIa5w+zNYiEf6uxDigPtkXjG/Zx4dN1efObht7PeBwI9W6aAA6r72iLoiSrDkrDQH7KKO+dcAXATgBcBbIeaFbOVMfZ9xtiHtd2+DuALjLFNAB4HcA0fhZWWxJJ5p86scdzH43aBMcDndulCf+0ZM3Du3PH4yVUL8YdrlwIAEgpHJiM14DV+tX7p7z6TFWj2q3/m4bfxsd+uQyKZ0l1I4biiW+5mK0cIprCCUimuC5OS4noHYRb3Pk1c+2NxZJsIli0oKs616UAnfvyC0Q+c6bP7tLRSr/Yd6G6ZQVp84jsL2UxaiykphHxu/XV/+fwjG/ChX7+R8/4HtE47nymr5k4pnOX7Ehld5lFVrrT0xKCkeM7flzhPuIDdMqLtvdpck5uf+JdtPGq4yWmxDs75KqiBUnnbHdLrbQBOz2/T8s+kyhK88vWzMa06CBdjmFEXsuzjdTEEPG4wxnT/u3BblPo9uvWeSKUyWicVJV5EE2lftl9yfZgDr0J8Y4kkmjoiennhva1h3R0Qjin6SMDsmxYPkrjJZItTSab09MSOSBzPbT6MgNeF5ceO0/fpn7hnfj9bQFV+6F83WaiZ/PUi3TNhqnY5WJ+7+C3sFkKPKymUBbwIx5PDMuTW2+LLbVH2VIrjcHcUkyudU3vjJpHNNlmoUxP3ga4V3KFZ/DElBSn/wJE+3edeuJa7MDAisSSaOvrw9HuHsGFvB9becu6ItmtMzVAFgFl1pfC6Xbjj8nn49KnTLe9/aOEk3HTuMQDSLhpZmEWqZEJJZRSjclNtGreL4fefX4KqoBd9iSS2HerGf69pxDf/skkXvK8++R7O+PFreh7+B829+rC1N5bUH1SzC0G33LUHV87sUFJc3z+RTOHGP72La3+/wehPzqNbJlsqZCaRzOSWEfGB9IgkP9kyIkvD77U+CjElhTItjbS/2TJ2nVxHOI5nNjlnCfdX4F7Z0Yyzf/KaZWa0jNmCtvu+ookkHlu/D8kUR6cmzgMVWzE6zNWNJQySbCOK0YxoeziuYI82whQu1JFkTC2zlwuL66uxuL4aQDqzQw44igwaRXJ92CEyVAQuxnD2nDpcceJk/PXdJlz6q9ctnzmkWey1peqNseNIT3rYGlMQ1zoTszUm2iEeFPnBSqa4vmBHXJHKE8iZIHl0y/THcjfj5JZJJFPojhpHJWLfnkFa7qLzTCg2PnclhaBWQyjaT5+73Yji2c2HcMfTW3HytEpM0WIHxrakg+C5cKizD0qK42BnnyENVsZ8r3RFEvj6nzfhpnOPwQxtzYKHXm/ET1/aBb/bpVvuA81e6Qirn881AN1XIG6Z9nAc1z+6Ab9ccZJlpKSLe0zRR5iTKkd+Oc4xZ7n3B/GM+dyyuKuv48nMlruduAPqkDvbEF9YiY+s3YNth9QskVXvH9azYswPv+xz59zo71SSRstdP4chEySfPvdBWO4On+2QYgVChMW+MSVlEbD+IIJgdpZmXEnB73bB73H123K3S60U4q2nrlreFzn81vvq4l+swco1uw3bRDC5vdc5+GnuTLcd7sZf323CP6WiduL7O9jZp7tVmjr68Ke3+l+9tD3SP8tdT4UchQHVVIrr9+u2Q93YsK9Dfx5lenXLPYlGbUJhiXfk7WYS9wx0aw9+ZTCd3+7V3TI8o5CVm8XdlZ4clS3oKATH43bpaY+yK8bJ557UAlnyg6WkUro1Lbs9ZLGKKSk8/vZ+zLvjBTz4z922s1a5Qxqp+SFOpjjWNrQ6FirLZLk7pVHKmRvxpNW6HYxrJpOlGU+m4PO4EPC6+22522VOiO/cWdyT+nnN7DjSg7tXGQPQIg3UHCiXMXd84rsSgVMgfa929SX0dj/xzn585//eN6REtvbGsHpnM1Ipbri+Ve8fxtUr14HztFsn185wsAHVoczb+M7/vY+5t7+gXlefc6cl4hiRmILdzepv25cY+ZEIiXsGhLDW16SH0G4Xg4sBf3xrH17Z0ez0URvLXf1fngjlRFdfAjNqQ/jMadaYAGC17OJKUs+qicSTBqFKSj53OZgmRBJQrek7n96KSDyJe57fYfEL/3NXC2bcugqNLb2WPHezW0RJcnzyobccC5VlsuicOkvZMhWWu9xB9ieo+sKWI4gpSXSE4zjSFZUCgNZ2xZQkfJ78We7iHE7i3ufglnEa0YjrNs9fsDunmU6pwxTlrbv7Evr3cURzEXZL3+1dz2zFNb97B5f+6nUs/N5LurC+u68D6xvb0R6O67+LsHjX7W7D1SvXOf62YrQSTaTwtSff61c5jD+9tR8n3PUSmnuy5+S/ubsVdz69BU+/lznPX+aJd9SJ+Xtaw3pnaHcfCLdMb0zRLffRECAmcc+BaTVG/6jH7TLU+pYR/nlRz0Ug3BnTq60ZOma6+hJwuxjmjC+zfd9s2cWTKVRpo4twTDFYx4kk1y13+YGWRTmaSOkTvIB0xoPg9V3qlIVnNh2CaV6XVdwlQT/UZX1QM/lindxcsniZA6oA0BPLLb94w952fOkPG/Hj53diyd0v49QfvSIFAG0sdyUFv26591Pc+6wdjjiHk7jbuc8Aa80igfju221qBv1u7R78ZcMBR5eVbLmLn6ylN6b/PumYRno/8evs0Gawyu4IIJ3KCaSv9Y2GFqxvbNdrCpmJxJN6HOtv/zqIX7/aYLufHa/tbEZPTMHNj7+X1YL/8Qs78ft1+/DLlz/I+fhzJ6jP38Z9HeiKON8n4nto7onhaLf6W5C4Fwh1pcbIty9DtUhxk1ncMpq4T6u2BtLM9MYUeFwMs8eV2r7fF08aLLK4kkKVlsMfjiuWgKoQDfmBlofyja29hkBoZ9gollOq1ADS+01dFreMeRKRbGU2tlhFzM6SfPu752FyZYljpo1oa0WJ1xJQBVQLtjMSx+zvrsJabXbkO3vbLVPtRV7/jiPd+udFANFJ3H0eF3weV7+CzoC95R63EXc7UTK77ZwySXp1t0z6XNsPd+OtxjZ879lt+Ob/bnZ0g8kduOhM9tuUJpBdXn7TTGbhmhHtk79vYbmLtF4761q4EWul50vMRckFcc+va2wzjDDs6NV+j0yjHDMicPru/o605a6ocS35dxPXv0Mq2zDQVNJ8QuKegU8snYaZtSFLHXhzWQIZoZFmt4w4xKTKQE5VIT1uhuk1qpU/qy6kCyygpsAde9sLAFQxTXGgRntAeqJpy93ncWk+d/Vv2U8qFxFb29BmOLfZchdN3uvJAAAgAElEQVTH23a42/JdmC132Te9u6UXP/z7Npx33+r0sWws93FlAXjdDH979yDeb7LO6BTiPq7Mb8lzB1QB2nJQFez7Ncvvow+uw5k/MZZH1mfyxmX3lBaYtRHvuJKCzz1Qy93ZLdPU0ae3Zcatq3DXM1sN1rrZcnfKTe+1sdwv+eXr+PjK9dI57dvdKXX04nwi08PuHOo1KQj53Dh3rjpHwiLuUvkMcd4jurhbRxfCJSOLe39c6PJkwM4sM2qFJd3Vl8g5AC+MgY37Ogz5+zNuXYU7n9kKQA266iMXrXMbV+YfFbVySNwzcPeVx+PVbyy3bM9U511YwFbLXf3f43ahJpTd7+52qRbjn65bij9ct9TxnEKcajVf/u/f3IvdzarfL+Rzq5a7jbuj1WYoL5AffMA4k9Vsad7+1BY8+M90Fschafjd2BLGQ2/s0atWAs4+YDGyufx+64zO9nAclUEvSnxue7dMVNE7zEypmMKatrOE7UQwplnufo8L0UQSr3/QYsjcyYQs7qJNcuD7QEdEb88jb+417J8wtcVJKERA9XBXFA3ab269BvvvuyNidXXZnkMW92gCCyZX4LozZwBIi3uvbrlb3TJC3EUuvpwtJaxbOSc8m0jLyJMBOyKZXXPhmKK7f8zGixOiQ9h1tFePBYj2i1IjsstMlAWZUlVClnuh4s3B8jZb7rK1PtNmZqwZUcBs2TG1mFhRYrvAR1ckoVvC1VqH8ffNh/HD57YDUKfUyz53GXP5Xxlx83POEVdSuh82mkjpufaCxtYw7nk+ncUhFz/rtLFenXzumWqZtIfjqA764HO7dCtTFqSemKKPppIZTD/hB7erHe6YCulxo8TnRndfAtf87h08/k5u6YHdhphGUj+eYE9LGE2aGJb6PYZRlaUejKm9YtKRWFR6x5EenP+zf9papE5WqsFyN+X4y/eqXJituy+B8hKvfm+LDklkusi/vTAEDkuW+0OvN2Lu7S/o7RbiWSbFp3IVXkA1OkRTM32Oc45IPImpmks0071vOL4k0FsPqimQ5liKbCiIe3JKVdAxTjKckLgPEVa3TPqBmTuh3Ly7BbPrxs6Vs689rN9QNTZzvUM+jzaJySp4//PGHsdziwfl1682YM5tz6NLSwPrSySzDmllP7vs6kjp1qtT9oaz5dUWjqE65IPX7ZLy3Ln+nfRG0+vHKiljiqpsCQoxsutIYkoKf3xrH+5etT29TUuFDPrcaOmJIZniaO2J4+n3DlpGMNFEEj9+YYduZcuWuBD3mJLS3Wt728K6G6OuzG8Qd7Mlbc4B/96z23D7U1ssrg67maryiCQklTXojSn6byl/X36PC8dNTAfyZbdMT1RBeSAt7mm3jNEtIc7b3afoo77m7phudAjBF+IuT8DqCGe2wGUi8SQmaX7xTBZ/PJmCkuJ60Tm7RWtsj59QdENMdHIiJRIA2npjlo7X7WKYUBGggGqhIv+gVUEvXv/WOZZ9zIGhSRXpG/izy+qznsNsqdu5Zfa3R/QH1C7FMuh3G8oP5EpnJAHOOZ7S0sbekEq49mbJTHlrTzsqSryYN7HcICziIc+WUhiyqavSEU6o4u5xISbcMskUSv0euF0MvbGEfq6eaMIw0UQOXgo3iHn0oLpdUnhx61E8856aBipGLT6PCyVej94hPLx2D25+4j28uPWI4RhPvL0f/7V6Nx5cvdtwLiBd3iGmJDG+PIDKoBeNrWE0acP42lIfWiVr0uxGE5YxY6qgPv62/ejhiI24y53xhArjrEkhVLK4XzBvvJ55BRgD5t3RBMpLPDbirrbvYGdfulyDksTh7rSbpqEl7TZat7sNy+99Te/czjimFn//yhm4eP4EWwu8tTdm65rqiycxqUIV90ydgugcp/XTco/EkjjWlLEmd9rf+t/N+JFp7kFV0IuQz4O4kso6W3uoIXEfAPKQO6akbOtIyJb0l5fPwpfOnqX/PaM2hGdvOsP22CIjwWMSczvL/bnNh3VRM48UANVyV5L9v8mUFEdPTNFr0Mu+1K4svk0AWH5sHUr9HoOVLgRKDriePK0Sr5liGiG/B+t2t+GRtemRRVs4jppSzS0jrM0Uh9fNUOr3oDeatkIbW8K4Qlq6T16k3KlUQXXIh5iSRFtvDO2RuCrsmuD5Ncvd7O0xL3Ai8v9FNoZ8LjG8FwHaGbUh7G0N65ZuwOs2WN2v7DiK637/jj46EJax1+XCXlMa5VWnTMF5WoBTWMQyfdLoSZQ8CGodqBgtiWudP6kct1wyV38fSBsyqRRHb0xBWcCrd6pmcU8kuW6FRxMpvT21pX7Dal+rdzVjb1tEz2wqC3iwYHIFqkJeg+/8Z//Yhc1NnVixcj1+8oK13n5fIonxFQEwZrXcDdks2r0nxL01x0XVI/EkplSVGIrKySPMV3Y04+XtRwGkU5+rQz79+xvpoCqJ+yCJKca1VC9ZMAGAsR7NR06cbBFrc0lggcgcsFru6Rmugue3HNEDaeY0NQAI+bWAqinF8GcfW5j5omBNhxRkSzkDgKUzauD3ugxWoxA42XJfMLlCr28iiCdTeOKd/bj7+R1qJlCKoyMSR1XQB5+H6VamkkzB63ahLOBBjym3X0a20uzSEwGgKuhDLJFCW28ccSWFSDztfvJ7XAj6bSpGmqzroE99uIXQye35zP+8hc1NnYgpKfi9LkyvDmJ/e0S33GOJFJq7o3Ax9Xfe3NSFl7erAgikRcLjZpbZqAunVOAXV58IALarLAkBfvwLp+LGc9SCePVaFtaKletxoD2CRDKFmpAPz/2/MzGlKoiQL+0DF+6InpgCzlURY4yhPKDGCeRsEUAVN7/HhZiS1IOpC6eoRoIYzX5wVL1nV+9sMbSnMuhDp9a59sWT+NUrH+C5zYdxuCtquzpUXzyJUr8bFSXGTqEjHMfC772E1VqJBeEimVARgMfFMqZD/uylnXh521GkUhx9iSRKfB69kJ/8fZoRnVp1yKdX9RzpoCqJ+yBJprjBqv7l1Sdh423nG/axS530e+zLugoL3MnnHtKEZqYmigczrKZU4nWjsTWsT6xQz+vC/EkVln3NdETiWcvDOlEd8sLndhkETjxgsuUuu5rkWZLtYVVk97aF0R1NIJnius9dz5ZJcng0y70nqhhm3MrIvs/uPsWw4Eq6vZrlrvli28NxPSXT53EhaFMnxGnGpRC6mDYBClALwr26oxmxhLqtKuRDRziuByCjShJHuqOoLfUjIN0XG/d1qMfUfge3JEyiky8LeHVr2i5jpiMch8fFcNqsGl2kRKnrtnAcj7+9H3ElZfgt5JLDYgQi3BEiC6yixIvuPsUSOKwO+vQZvfvbI/C6GQLa8W7SOhcRK9jfHkFl0IsqLRmgKujVR40iL747mkBvTEFTp7Xj6osnEfC6URX0Gdw5m5o60R1V9A5BdLilfg+qQz7HWjxxJYXfrN6Nx9/er9+nQZ9b99UD6SSBWy6Zizsvn6dvH1euGmU1Ib9kuRu/m1xHDPmCxH0A3HvVCfjJv5+g/y0HS30el55zLvDaLIjtZLmL0rNOPnch8hO1qnNHtFmgPrexs/B7XIZJFfJ2sTITALz8tbOxdEa1ZT9V3Ac2rCwLeOH3Ooi7ZLnLHdLaW87FDctnIcXTFuj2wz26pWrrlnGplntvVHH05RvEPZrQA3Ay1SHV5y2yVN7d34FPPPSW2ka3y+CmEJhLHoigqW65J5IGV9n+9ohWq0YVo3A8aUivO9odw/jygL4YCSCJu1RQTKRiHqNNcBNtm1gRwK6j1t+7I5LQO5naUj/OnTsO1yyrxzXL6lFX5sdL244ikeTwetL3m8Eto412xKhHlLKuKPGiqy9huUeqQj74vW7ElCR2N/diek0IXzhzJs4/bhw+sXQaykyLosyURm7C198ZTugdQLNmmBzujBomyHHOEUkkEfS5URn0Gtwl2w/36NcOpO+BoM+NmlK/Y0B1T2sYSopjV3OP4TNTpYmHwnI/cWolzp6TXip0XFnacrcT9zW7WrDohy9j9c5mNDT3Dos/nsR9AHx00VR8bPFULJxaiXuvSov8mbMty8YCANz9sNx9JhEXmAOqIpAk/Jpmy93tYpg3yZqV4/O4UCbVmj9mXCkumj/Bst/z7x/BB5IlaBfodKI84IXf4zb43O97aSeae6KGYa0807eixKsPz4U7Yvvhbl3cq0N+eD0u3R2SUFK65d4bUxxztUUu9E9e2IHNTV2WOvvqsY3B6CZpGr3P47JdPMOccaOLu2S5y0H1A+0RxBJJ1XLXtgsXVzSRwtHuqCru0r3yr/3Cck+Xcm4Lx+F1M91/LLscxO919pw6nH6MutpYZySu3xtetwsPX7MYi+urcdeH5+PG5bPQ0NyLnUd6TJa7KsAVJV70RBW8ubsVH/+tOjFKGAblmribs0WqQ14EvKrlvrulFzNrQzhxaiUe+uxiBH0eVIaM3//MuvRISoh7RySuxzTE/a2kuCFgLFJ8S2ws9+3aervm2vQhv8cSvJYRneOB9j7dnVfidesZTh4X0911JV63Yba5sNxVt4z6HcnFw0Tw+I9v7ceVD6zF95/datuGfELiPgievvF0fHTRVADAu7dfgIc+u8h2P7u8eLvFIQAYHkQZIfbCuhQW6Etbjxo+J4gpKdx95fEW4fK4XBahtgvWPrnhgGEYKU/KunrxVLx3xwW27Vf39cDvcRlS+N7c3YZv/mWzYRKQuc3miV87jvToboiakGq5x7UOQ0ml4HGpHZWc1mdGiO1vtCyWaCKJC+aN1993Met55UqIfo/b1nJf29CKlWt269kkFstdSeoLkwOqYAhXjVxlVHy2uSeG8eV+w+8uJsXoyydydRp/VdCHa7WJRIvqqwConb34Dr5w5kx88Sw1gN8RiTsaEgunVgIAPmjuMXS04nonV5agN6bgrme26iIuW+7dfQlLmmZV0Ae/x43emIL97RHMMpXQqDZduxxzqdKEvyMS138DWdDlTlf4s0t8HhvLvVs/DpCOWQR9btSEfDjaHbUt+yCPfDY3dWqf8WDJjGpMqggYaj0FvG5DHE2UKKkptbfcxW/wj21H0RNTcM7c9EpoQwWJe55QA0nGh0g8MOZgKmAfAJU/YxbcZbNUS2yils4mhvzCYvW5XXj+5jPxyaXTAKixgIDXjQlaoEf4aN0uZikhkEs5BLmOe3XIZxEombKAFz6PyxJ8amztNaRlmmv0mDN+Xt3RjC8+thGAOtz3eVx655ZIatkyAY+h5IIZc1CrLODF/Z84Cf93wzIAakaR2UV2sNNouQd9Vp/7vrYI7l61A9c+skE9jybunVL1QLnTONIdRU9Ugc/jMqQaVod86IkqaA/HNcs93ZYebWlF2fVxpCuK6pAPJ0+rwt57LtMzYCZL5SlCfrcem+mMJGzjMUA6CJxIcsM+ovOfVBlAb1QxLE5hdssI0Re3R1XQh4DXhYaWXiSSHLPqjOIu7hvxnc+SJvRVSpa7cMvIAWSDuGvft9ly744m0KhlFAm3jIhZhPweLJtVi8NdUUsqK6CKuxDmTbq4u3HClEq8eet5+rMnzisjYkbVIZ/+nizu8mzaihIvTj/GfpSfT0jch5DpNc5FwswCKxAPmdnnfs2yerz+rXNwrFapzmz5e9wMx00sx9cvPBYfPWUKbr1kLoD0QyRcC14bF1EmcRdlDeSwgV1nJVMWUC13c359a0/c0CZzW2Q3hpyhAKiWu9ctZcukUvC4XagNqQ+2XVmAsoAHz71/GPW3PAcAuGj+eNzxoXnwe9y6OyDk91g65YNSCQUxicmJt/e2Y3NTJ/riartae2P49SsfIKokLZ1VPKnOeJWvc1JlQBeqcWV+y3fSE00Yap0f7orazmmQ77WygEdfLEK13O1/L1mg5E7lwvkT8PUL5uC4ieXojSuGTtrslhEdjyipIYwcMZnNPBtbuKQumj8BM2pDOGlalf6eyBRr7Ynb1qKRZ8DK1nhV0ItIPIktB7uw+IcvI5niCPncklsmve+/nTwZc8aX4icv7ERDcw+uuP8NPU1z19FenH5MLXweF97TtskuOfmZE/fwH69bim9cOAdBvzUVUjYsZKG/aP74jCVM8gWJ+xDy2LVL8YOPLLC4RgQfOXESLjt+ov73D66Yr4u7WXAZY5haHdRnYspZFcdPrtAj+tUhH+796EJ8UcurF8IlW+5mMi2wLNw/bjlonKFwGqAKhZ0rQIiYcFeYOwl5iL5sZtqyefL6UxHwuuFzq5OyUimuW+5LZ9YgmeKWxbYBWGr4nDWnDhWauIgHMOh3G8SvKug11BSPakE7O7541kwEfW7c8Md38ebu9Pnv+8cuJJLctsKhyJYRiNgJoI5OzA/9T1/aaSjsdrirz2D5C0SROQAo9Xt1yz3F7TOpACDgsy4fCahpfV85bzbKA15wDj0Lx8Wgx2vKA2pmiwhOioBilZYKKZhVa2+5nzqzBq99Y7lhdmp5QB1FHe2OWuYRAMB+qbCZuJcCXrfeKfzt3YOIKSn81ydPxodOmGSx3IM+DzxuF646ZQoaW8N4eXszNjV14YoH1qKhuRf72sI4bkIZZtWVYotWbkD+7eV7WmQAnX5MLW46dzYWTCrH3AllOHZ8mT4ikgVduAfPmlOHz5xWb7m2oSAncWeMXcwY28kYa2CM3eKwz8cYY9sYY1sZY3/KbzMLkwkVAdtFuAW/uPokXHXKFABqLvCnT6vXH267WjJAuua3x830etPPfuUM26Cf2A+QrWXrT37WnDr86QtLbT8vxEdepEOI8oolUx2vzclaBNIWunnJvoBkSS7TAoKL66uwdKb6WmR0qEscqul7p0yvgs/jwk6bTBFzpyqLvdftgtfNEPJ59LVy508qx7iygMFS9biY43d76swa/NenTsHR7qhtZpLdxDKfFFAFjO6UihKv5ff/w3rjbNRoImVbeE5eUKY04DG4kvpruQuE1d0dVXDD8llo/NFlunFQqk3aOdKlirCY/Vod9Om/Y1XQq3emAtEx1ZZaJ/4xxjCuLIDmnpjFcp87oQzrG9t0X3mflM2yQJts99d3m1Bb6sPFCyagKpTOmY/EFfg9Lr3twmCRJ1b9/s29SHFgzoQyTJcCpUZxlyx3k/Eys64UL/zHWagp9ev3izyJqS+uIOB14dHPL9HbO9RkFXfGmBvAAwAuATAPwArG2DzTPrMB3ArgdM75fAD/MQRtLUrEjSCyQNKWu/1PI4TIxRievul0bPv+RRmPLx5a8cAJsf/3k6cYsnuWzbL3AYb8Vn+zOOZ/fuR4PHyNfRDZKWAMpMXdzjMl3EDHjCvFbz99ClZ+On184aNPJFNIJDk8LoaA161PkjFjFvdqU/2dEq/qmxb7fePCY3XRAoAffmQBzp07ztbnDgBzJ5bh7Dl1+udnjys1pMiW2nx3fo8LJV63/jvLlnt5wKtb0OYFrz916jTH6wCMS0EGvcYgsKPlLom73RoFsghNNJUuECmNIuA5TpulXRXy6iJot3ZBtRY0dVqRbHy5X7PcjbNtL184CYe6ononrvvcfW7MnVCGoM+Nrr4E5k+qAGMMlUEvEkl1glU4rhjuY3Et7+7vQG2pH6V+1X0HAHPGlxnKa5fYdJJuF7N1b+qf0b5XuQJmJJ40TA4bDnKx3JcAaOCcN3LO4wCeAHCFaZ8vAHiAc94BAJxz5/XnCAMhPaiVDowC9r5xABCTTd0upmVyZL5hhAWYdsuox7/vYwvx2LX21vp1Z8zQXwsfq+w+Em1zuZitdSpfhx3CerO7wm9dpMYKJlWU4KL5EwwuDCFScSWFRDKljyDsiqYBQEWJWdyNf4f8HoR8Hly6YCLe+PY5OGfuOIMgX3HiJDDGLNlFnzp1Gv765WWYqAmz+A5KfG7DbNaA140bls/Cn794mr7N73WDMaZatSVeg+hUBNOWu0itA4CvnHsMzp6Tzq6oDtl/5wKXixmscqdsGTF6Ea/NjCvz6xb2xAqj6058T6JswpIZ1Zg3sRylfo8uglNtxP2EKZWYVh20uGvS5wzgUFcf2sJxwwjnwwsnAVCD7EDa5VGiZa2coHXwCyar6b/isx3hOCIxo2ttgnYtR7tjmFwZwLETytCupZjW1xjXTgjK36P2OuBxOcbMAPU+9biYKaCadBwBDhW5iPtkAAekv5u0bTJzAMxhjK1ljK1njF2crwYWO+IH18XdwecuSEqWey4IS10PqOaQGXPbh+bhmxcdq7dn050X4qvnz9Hfl4Ug4LW/Yf0O24H0aMDuEj62eCr2/OhSg6ibz5tIqsXQhDA5dTAhU9kAs7hPrQ5iWk0QLhfTs05ky11cm/mhHF8WwCnT04FAcf6AKW0y4HXhWxfPxZIZ1fq1+rVrqAr6UBPyGTJ1ygMe/RrHl6Ut5dpSv8ElUFdmtKLtkAXeyXKXr9Frsw9jTBdLc9Ex8T0d6uxDmd+Dfzt5ClbdfCYYY3qna2e5L5xaiTXfOsfirhGMK/fjQHsfOAeOn6KmavrcLkytDuK4ieV6PZqoZLkD0H+PBZNEqQNtQlREDUjLVvO4Mr/+e0yoCOhVMGfUhuDzuPR7QT4+kLbccxHpEp/bIO6RuHPsZqjIRdzt1MCcJOoBMBvAcgArADzEGKu0HIix6xljGxhjG1paWvrb1qJEF3fFaLk7+dxFQDWX9EX1OPZuGTsWTq3UXTUXarnglyyYiIoSL1yu9MQZuW3mlLAztBQvWYy+vHwW/vKl0/SMDpFizGxvLedMIq/kllGSKf3anITC/BBWmjqBRz+/BN+59DjDNuFu8LqZfj4xOhLNKjWtj6uLu89t8nWnzy8sXeGumlIVxNTqoKFzLPV7bC13q7jbj1TevOVcPHPT6frfonPLFP/Qxd3hvhBiaZ7ZK67nQHvE8v2LzKVclpQ0I7ujTp2pzpwW3/eJUyuw9VC3Xp8dSN9/F86bgGnVQSyqVz8jT4iKxJOGEZXX7dLdSBMrSvQS3LO1PPYpUqaW/N2J104jIZlSv8cwsSocVwwunuEgl7M1AZAjZ1MAHLLZZz3nPAFgD2NsJ1Sxf0feiXO+EsBKAFi0aNHI1sMcJQgxFz73TOILpC33XDOpxPGEIHscfPmAOilLMHt8Gfbec5nh/ROmVGB/e8SwmIQsoE/deDpO1CbGyA/Ap06djsmVJXjxP85COKbgP7Wa6TkOPnSEBRpTUlq2jCbumrjOHleKX1x9Ii77lbqak7kmjMvUIdqNOsSoQn7P7WLqzF6/B23huCUOIXK/Ax6XwUKUhaFMq4Ejtv30oyeAc9XvK2CM6YtP1IT8cLsYkimO2lKfYSQ0zkHcJ1WWGERY/DaZXGS6de+wz+dOr8fciWWWUY8o7RuOJzGjzijuRzV/+UDEXVxbwOvCQs1yFx3JvInlePztAzjUFTUEVIH0iECgu2W0Mhpmf/eEihIc7Y5plrsq7qK8r5w9JhsaeuZZDhb4qTNr8PK2o7o7pi+e7Ncs73yQi0S8A2A2Y2wGY8wH4GoAz5j2eQrAOQDAGKuF6qZpzGdDi5WakA/nHzceD3ziJABpd4tT6Yk7L5+P848b5xgANSPq2iRTuXUemRBDX7k8rWy5yxa97AoQs/cCXrW2hxj3ZfJb2iEe2EOdfVBSKd3aFJOFYkrKUBRNPPj1NUHc/qF5yAXdwjZZuyGfG7PGlSLgdVkm5pRn8LkLRAqhEIjKoA9VIZ+lgxG/U4nPpZeRrS3zG8TXLtPEjqsXq7Vc7MpQCLK5bmpK/fjQCZMs2+XYhDk1U9SDsfO5Z0NY7nMnlOudtuhM52m/7bZD3YZUSDtkt4ydS2Sidp6JFQEsmFyOK0+ajEuPV8twlNmUqADSoy6nulAyH188FT0xBau0QO1IuGWyWu6cc4UxdhOAFwG4ATzMOd/KGPs+gA2c82e09y5kjG0DkATwTc55m/NRCYHLxQxlC4Q+phyWi5tRG8JDn12c8/GFmIsFIzJZ7tn4xNJpaOuN43On1+vbKkq8WDilApuaugzLpcniaFf3Rv2/f+c/aVoV3C6Gt/a0qdkyJp971FShUHyH5x83HtdKQeJMiGswd67TqoM4aVolnrz+VEunJM5vLjImZwwJ14I5cGkWCpHqWuJ1o1wrZVtb6jeUgsg1MHfjOcfoZX6dEPna/Z1UI7umzDGPr14wB7f+7X1Lhk0uCMt9weRyvQMRrrK5E8rAGLD1UBeUJIeLObucqoJelHjd+KC5By09MUv6oYghTCgPwO9x4+cfPzFr23Sfe4Z4kmDpjGpMrwnimU2H8O+nTNHLBw8nOZ2Nc74KwCrTtjuk1xzA17R/xCAQroNMa4H2B/HQCuFz8uXngt/jxje0QKuAMYa/3XA69raFDRNpMvl5b7lkLgJeNy6VMnByodTvwfGTK/DajhZEE8m0z10TF2HN/fObyxFXUvoU80wBRbtzADBUIASAJ64/DR63tXQDkLbclRR3zC8XxzWXJjb7b0WH5Peqdcp9btWC73GoRT9YSjLMf8iE3+NWa/0kU5bJWiuWTMOKJdMcPpmZKVVBTK4swfI54/QORPwf8nswozaErYe6MbUqiBIt88gOj9uFZbNq8NeNB9GXSOI0ba6EQHQ85iwgwdwJZYZVvID0feQ0WpBhjGHBpApsP6JOhorEFUPmzXAwvF0JkRXhlsmTtutW8pL6ajR3R/H/zpudnwObzmF2VQgfsd2zV1Pqxw8+smBA5zr/uHH46Uu7AEAvVSzEXZT9FZ3Ms5vU0FB/hEu4AMyWeyZrWa/zo6RM2TLp18JnLcoU2O0DpN0yAa8b5QEvakt9YIzlFMQbCGmfe/87/dKAB+3huKFA2qDb43Nj7S3nAkgbJHKM45i6UuxtC6NGqr7oxNnH1uGVHc1wMVgKdV15krqAjrnMheDvXznDkjUifoNcxB3Qlk/sSS8Gbrfoy1BC4j7KENP881XvWU6FXP1N61qvQ4VPSvnLJ1ylEo4AAAkaSURBVF86exYmV5Ug5PPgQq1UsbmImkCI/GybBTqcEFaiefWqTKTXDTUueiFb7rdddhyCPjfOO84oMgFT/X7ZLXPG7Fp9lmh/Rh/9oWSAbhlAHY20h+O2ZRbygd/j0pdSFEysCGBdYxuOdkcdA8sCUW990fRqS0B4XHkgo6suU7G/XMW9rsyP7qiCaCI5On3uxPCiu2XyJO7mgOpwISz2fD/4HrcLV540xbDNKc/9306ejPraIE6WilNlQ/h3+/N9pbN4nF0uNaV+/OeVx1s+K/YRwpG23F2GdXczubkGQ6Y892wI0c1UIXQwMMbw4YWTDTOpJ1SUoCeq4IPmXj2F0YnpNSGsWDIV5xybn/K66TpNuX1XIvB9uCuKpMllNxyQuI8yhEvcrt70QBCWu9mHPNSIB99uIZB8Ux6wv40ZYzhlunWVqUykLffcv38xMclcUz4XQRaWu6itLlvudseSJ5PlgxLvICx37bsyzx/IJ/eZ1vsVvvKmjj59YfBM/OjfTsi6T66ks2Vyt9wBYJ+2SHsugdh8QuI+ysiWCtlf9Ik/w2y5T60O4rVvLDcUYRoqxBB60fTcLXQnRD50f/pWUT5YLI4uyEUEygJe/OkLS/VsDjFJzSy2jDHLvIN8MBifuxjlVGUph5BP5JmydksmDiX9yZYB0uIulo0kt8wYJ9/ZMh9fPBWv7mjGZ4epzKiMXMJ3qFl367mO7pn+UOYwCsjEhIoAtn3/IstDn6ufXJ6zoPRzBvJgGZTPPSCW4xsat4wdE0dU3DUXWs4BVWG5a+JuU0huKCFxH2Xk2y1TW+rHX7+8LC/HGs04pbT1F7sqmLlg508diEAntUDuYCab9YfAYNwyus99+Cx3uTzBaLfcReXLfcJyH2a3zNBEaYgBUy4thkAMP16twuB9H12YfechQORjm9caHSoGE1Atk5bcGy4CXrc+U3lSZf8nSQ2G/sxQBVRLv6LEqy8yQm6ZMc5lx09EW28MVw9wEggxeJ656YwRO/dtH5qHzy6rx7jy4RGubLVlMnH14qk4ZlzpsCwZJzOhogTdUUVf/Wm4GFcWwCeXTsNZWoplLtSV+bGvXQuokriPbVwuhmtOz22qPDE6uf6smVi9c2BLGnjdLj1AOxyUaEvt+Tz9dwPV14ZQP4xxFcGkigC6+xLDFpcQuF3MNp01E7WlPn2ZwoG6/AYKiTtB5JnvXHqcpZTwaGUwqZAjxVcvmIN2mwXRRyNy7f3hToUsnF+UIIi8M5iA6kixYHJFv1wjI8myWemaNuRzJwhi2FgyoxrXnzVTr8NP5JcVS6ZhQkUAWw92WUogDDUsXyl3/WXRokV8w4YNI3JugiCIQoUxtpFzbr8yvUThjMUIgiCInCFxJwiCKEJI3AmCIIoQEneCIIgihMSdIAiiCCFxJwiCKEJI3AmCIIoQEneCIIgiZMQmMTHGWgDsM22uBdA6As3JJ3QNowO6htEBXUP+mc45z1p/YcTE3Q7G2IZcZl6NZugaRgd0DaMDuoaRg9wyBEEQRQiJO0EQRBEy2sR95Ug3IA/QNYwO6BpGB3QNI8So8rkTBEEQ+WG0We4EQRBEPuCc9/sfgIsB7ATQAOAWm/f9AJ7U3n8LQL303q3a9p0ALsp2TAAztGN8oB3Tp20/C8C7ABQAV5nO/1lt/w8AfLbQrgHAiQDWAdgKYDOAjxfaNUifKwdwEMD9hXgNAKYBeAnAdgDb5PMX0DX8RLuXtgP4FbQR+yi8hq9p3/FmAK9ATfkrtGfa9hqQ4zOdz38DEXY3gN0AZgLwAdgEYJ5pnxsAPKi9vhrAk9rredr+fu3L2a0dz/GYAP4M4Grt9YMAvqy9rgdwAoBHYRTGagCN2v9V2uuqAruGOQBma68nATgMoLKQrkFqwy8B/Ak24l4I1wBgNYALtNelAIKFdA0AlgFYKx13HYDlo/QazhHfL4AvS+copGfa6RqyPtP5/jcQt8wSAA2c80bOeRzAEwCuMO1zBYDfa6//F8B5jDGmbX+Ccx7jnO+B2hsucTqm9plztWNAO+ZHAIBzvpdzvhlAynTuiwD8g3PezjnvAPAPqL1vwVwD53wX5/wD7fUhAM0AzJMWRvU1AABj7BQA46FavnaM6mtgjM0D4OGc/0Pbr5dzHimkawDAAQSgipMfgBfA0VF6Da9J3+96AFO014X0TNteQ47PdF4ZiLhPBnBA+rtJ22a7D+dcAdAFoCbDZ5221wDo1I7hdK5BtW+UXoMOY2wJ1AdzdyFdA2PMBeA+AN/MsNuovgao1lYnY+xvjLF/McbuZYyZVzke1dfAOV8H4DWoluJhAC9yzrcXwDVcC+D5gbRvlF6DToZnOq8MRNyZzTZzyo3TPvnanonBtC+XfYbjGtQGMDYRwGMAPsc5N1tko/0abgCwinN+IMM+o/0aPADOBPANAIuhDs+vybF9uewz5NfAGDsGwHFQLcjJAM5ljJ2VY/ty2Sfv18AY+xSARQDuzUP7ctlnOK5BbM/0TOeVgYh7E4Cp0t9TABxy2ocx5gFQAaA9w2edtrcCqNSO4XSuQbVvlF4DGGPlAJ4DcBvnfL3NLqP9Gk4DcBNjbC+AnwL4DGPsngK7hiYA/9KG5QqApwCcXGDXcCWA9ZpLqReqJXnqaL0Gxtj5AL4L4MOc89hA2jdKryGXZzq/9NdJD9WaaYQaeBBBhvmmfW6EMXDxZ+31fBgDF41QgxaOxwTwFxgDFzeYzvUIrAHVPVADL1Xa6+oCuwYf1Ej7fxTq72B67xrYB1RH9TVox9sEoE77+3cAbiywa/g4gJe1Y3q1++ry0XgNAE6C6qqYbTp3wTzTGa4h6zOd738D+xBwKYBd2kV8V9v2fag9FaAGcP4CNTDxNoCZ0me/q31uJ4BLMh1T2z5TO0aDdky/tn0x1J41DKANwFbpM5/X9m+AOvwpqGsA8CkACQDvSf9OLKRrMLXzGjinQo7qawBwAdTUtfehCqevkK4Bqkj9FulUzp+N4t/hZajBXnHPP1OAz7TtNSDHZzqf/2iGKkEQRBFCM1QJgiCKEBJ3giCIIoTEnSAIogghcScIgihCSNwJgiCKEBJ3giCIIoTEnSAIogghcScIgihC/j9lu4dWrlJcWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(np.power(10, lr_callback.lrs[20:400]), lr_callback.losses[20:400])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_value(model.optimizer.lr, 0.001*0.2)\n",
    "\n",
    "clr = OneCycle(lr_range=(0.001*0.05, 0.001*0.1),\n",
    "               momentum_range=(0.9, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15999998 samples, validate on 4000000 samples\n",
      "WARNING:tensorflow:From /home/axel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "15999998/15999998 [==============================] - 7012s 438us/sample - loss: 0.5951 - sparse_categorical_accuracy: 0.8671 - val_loss: 0.5702 - val_sparse_categorical_accuracy: 0.8736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0950632cc0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train, validation_data=(word_seq_val, y_val),\n",
    "           batch_size=450, epochs=1, verbose=1, callbacks=[clr])\n",
    "\n",
    "# model.fit_generator(train_generator, validation_data=val_generator, steps_per_epoch=int(len(word_seq_train)/batch_size), validation_steps=int(len(word_seq_val)/batch_size), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8501058605660863"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss: 0.6197 - sparse_categorical_accuracy: 0.8631 - val_loss: 0.5577 - val_sparse_categorical_accuracy: 0.8750 prelu + la\n",
    "#loss: 0.6337 - sparse_categorical_accuracy: 0.8605 - val_loss: 0.5601 - val_sparse_categorical_accuracy: 0.8747\n",
    "\n",
    "#loss: 0.5881 - sparse_categorical_accuracy: 0.8678 - val_loss: 0.5573 - val_sparse_categorical_accuracy: 0.8749 adam\n",
    "calculate_balanced_accuracy_chunks(model, word_seq_val, y_val) #0.8505 prelu + la, 0.8528 mish + la, 0.8528 adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model, 'vec3')\n",
    "# load_model(model, 'vec3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_value(model.optimizer.lr, 0.000014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15999998 samples, validate on 4000000 samples\n",
      "WARNING:tensorflow:From /home/axel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "15999998/15999998 [==============================] - 7280s 455us/sample - loss: 0.5186 - sparse_categorical_accuracy: 0.8818 - val_loss: 0.5612 - val_sparse_categorical_accuracy: 0.8756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f156b6b7e80>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clr = OneCycle(lr_range=(0.001*0.05*0.5, 0.001*0.1*0.5),\n",
    "#                momentum_range=(0.8, 0.7))\n",
    "\n",
    "model.fit(word_seq_train, y_train, validation_data=(word_seq_val, y_val),\n",
    "           batch_size=400, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8528503700461636"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss: 0.5019 - sparse_categorical_accuracy: 0.8844 - val_loss: 0.5296 - val_sparse_categorical_accuracy: 0.8810 prelu + la\n",
    "calculate_balanced_accuracy_chunks(model, word_seq_val, y_val) #0.8590, 0.8593 mish + la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model, 'vec3')\n",
    "# load_model(model, 'vec3') #seguir desde aca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15999998 samples, validate on 4000000 samples\n",
      "WARNING:tensorflow:From /home/axel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "15999998/15999998 [==============================] - 4793s 300us/sample - loss: 0.4337 - sparse_categorical_accuracy: 0.8971 - val_loss: 0.5312 - val_sparse_categorical_accuracy: 0.8818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f21e3aaeac8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.keras.backend.set_value(model.optimizer.lr, 0.0003*0.5*0.2)\n",
    "model.fit(word_seq_train, y_train, validation_data=(word_seq_val, y_val),\n",
    "           batch_size=1200, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8622008960485829"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss: 0.4526 - sparse_categorical_accuracy: 0.8938 - val_loss: 0.5213 - val_sparse_categorical_accuracy: 0.8833, prelu + lookahead\n",
    "calculate_balanced_accuracy_chunks(model, word_seq_val, y_val) # 0.8592 antes, 0.8622 prelu + lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model, 'vec3')\n",
    "# load_model(model, 'vec3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15999998 samples, validate on 4000000 samples\n",
      "15999998/15999998 [==============================] - 4006s 250us/sample - loss: 0.4506 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.5228 - val_sparse_categorical_accuracy: 0.8830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc05edee5c0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.set_value(model.optimizer.lr, tf.keras.backend.get_value(model.optimizer.lr)*0.3)\n",
    "model.fit(word_seq_train, y_train, validation_data=(word_seq_val, y_val),\n",
    "           batch_size=1024, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8627204686986996"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_balanced_accuracy_chunks(model, word_seq_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo word_embeddings:\n",
    "\n",
    "# 0.8472 val - 0.8852 test\n",
    "# 0.8545 val - 0.8869 test\n",
    "# 0.8592 val - 0.8915 test\n",
    "# 0.8625 val - 0.8944 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=60000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_features=int(MAX_NB_WORDS/2))\n",
    "tfidf_vect.fit(df['title'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidrolavadora': 31008,\n",
       " 'one': 41806,\n",
       " '120': 979,\n",
       " 'bar': 12773,\n",
       " '1700w': 1968,\n",
       " 'bomba': 14262,\n",
       " 'aluminio': 10047,\n",
       " 'italia': 32950,\n",
       " 'placa': 44555,\n",
       " 'sonido': 51911,\n",
       " 'behringer': 13257,\n",
       " 'umc22': 56454,\n",
       " 'maquina_pt': 37189,\n",
       " 'lavar_pt': 35005,\n",
       " 'electrolux_pt': 24038,\n",
       " '12_pt': 1161,\n",
       " 'kilos_pt': 34143,\n",
       " 'par_pt': 42837,\n",
       " 'disco_pt': 22420,\n",
       " 'freio': 27927,\n",
       " 'diant': 22128,\n",
       " 'vent_pt': 57394,\n",
       " 'gol_pt': 29497,\n",
       " '8v_pt': 7953,\n",
       " '08': 283,\n",
       " 'fremax': 27931,\n",
       " 'flashes': 27231,\n",
       " 'led': 35113,\n",
       " 'pestanas': 43925,\n",
       " 'luminoso': 36309,\n",
       " 'falso': 26349,\n",
       " 'partido': 43009,\n",
       " '4_pt': 5427,\n",
       " 'microaspersor': 38415,\n",
       " 'irrigacao': 32865,\n",
       " 'ultra_pt': 56383,\n",
       " '20_pt': 2775,\n",
       " 'metros_pt': 38309,\n",
       " 'raquete': 47182,\n",
       " 'clash_pt': 18249,\n",
       " '100_pt': 451,\n",
       " 'tour_pt': 55246,\n",
       " '_pt': 8365,\n",
       " 'nova_pt': 41146,\n",
       " 'kit_pt': 34203,\n",
       " 'tripe_pt': 55828,\n",
       " 'celular_pt': 17156,\n",
       " 'camera_pt': 15939,\n",
       " '20m_pt': 2812,\n",
       " 'brinde_pt': 14860,\n",
       " 'nf': 40761,\n",
       " 'filtro_pt': 26965,\n",
       " 'ar_pt': 11069,\n",
       " 'bonanza_pt': 14305,\n",
       " '1984': 2382,\n",
       " '1990': 2396,\n",
       " 'gatito': 28855,\n",
       " 'lunchera': 36331,\n",
       " 'neoprene': 40639,\n",
       " 'condensador_pt': 19241,\n",
       " 'bosch_pt': 14473,\n",
       " 'vw_pt': 58313,\n",
       " 'fusca_pt': 28308,\n",
       " '300_pt': 3888,\n",
       " '1968_pt': 2346,\n",
       " '1974_pt': 2361,\n",
       " 'rosario': 49046,\n",
       " 'contador': 19472,\n",
       " 'billetes': 13659,\n",
       " 'uv': 56768,\n",
       " 'mg': 38349,\n",
       " 'detecta': 21992,\n",
       " 'falsos': 26351,\n",
       " 'nuevo': 41280,\n",
       " 'magicos_pt': 36746,\n",
       " 'caixa_pt': 15684,\n",
       " 'setor_pt': 50732,\n",
       " 'hidraulica_pt': 30988,\n",
       " 'ford_pt': 27585,\n",
       " 'focus_pt': 27480,\n",
       " '0_pt': 328,\n",
       " '2006_pt': 2621,\n",
       " 'tupia_pt': 56115,\n",
       " 'coluna_pt': 18896,\n",
       " 'laminadora_pt': 34752,\n",
       " 'fresa_pt': 27949,\n",
       " '8mm': 7919,\n",
       " '1250w': 1105,\n",
       " 'mdf_pt': 37827,\n",
       " 'acm_pt': 8851,\n",
       " 'sh_pt': 50810,\n",
       " '220v_pt': 2991,\n",
       " 'touca': 55230,\n",
       " 'porco_pt': 45222,\n",
       " 'cachecol': 15573,\n",
       " 'luva': 36380,\n",
       " 'projeto_pt': 46074,\n",
       " 'unidade_pt': 56549,\n",
       " '3000_pt': 3857,\n",
       " 'psi_pt': 46282,\n",
       " 'maleta_pt': 36902,\n",
       " 'guardar_pt': 30132,\n",
       " 'transportar_pt': 55530,\n",
       " 'medicamentos_pt': 37913,\n",
       " 'promocao': 46105,\n",
       " 'battery': 13050,\n",
       " 'pack': 42462,\n",
       " 'huawei': 31524,\n",
       " 'cuerpo': 20672,\n",
       " 'metal': 38241,\n",
       " 'puertos': 46376,\n",
       " 'usb': 56714,\n",
       " 'capa_pt': 16239,\n",
       " 'cilindro_pt': 18019,\n",
       " 'tecido_pt': 53976,\n",
       " 'painel_pt': 42538,\n",
       " 'festa_pt': 26746,\n",
       " 'sereia_pt': 50641,\n",
       " '3_pt': 4650,\n",
       " 'pecas_pt': 43361,\n",
       " 'pivo_pt': 44505,\n",
       " 'inferior_pt': 32290,\n",
       " 'direito_pt': 22398,\n",
       " 'bandeja_pt': 12703,\n",
       " 'hyundai_pt': 31669,\n",
       " 'hb20': 30632,\n",
       " 's_pt': 49513,\n",
       " 'sedan_pt': 50335,\n",
       " 'mangueira': 37043,\n",
       " 'radiador_pt': 47007,\n",
       " 'siena_pt': 51055,\n",
       " '2000': 2591,\n",
       " '2003': 2614,\n",
       " '4210': 5037,\n",
       " 'galoneira': 28622,\n",
       " 'semi_pt': 50489,\n",
       " 'industrial_pt': 32254,\n",
       " 'porton': 45341,\n",
       " 'chapa': 17486,\n",
       " 'hojas': 31210,\n",
       " 'mtr': 39764,\n",
       " '50': 5597,\n",
       " 'marco': 37276,\n",
       " 'multi': 39863,\n",
       " 'rapido': 47166,\n",
       " 'cargador': 16475,\n",
       " 'cabeza': 15507,\n",
       " 'carg': 16469,\n",
       " 'tapete_pt': 53733,\n",
       " 'capacho_pt': 16244,\n",
       " '120x60_pt': 1067,\n",
       " 'churrasqueira_pt': 17902,\n",
       " 'frete_pt': 27979,\n",
       " 'gratis_pt': 29822,\n",
       " 'base': 12942,\n",
       " 'simil': 51196,\n",
       " 'cemento': 17172,\n",
       " '30': 3852,\n",
       " 'cm': 18458,\n",
       " 'mm': 38907,\n",
       " 'oxford_pt': 42326,\n",
       " '424_pt': 5045,\n",
       " 'tobacco_pt': 54899,\n",
       " 'pipe_pt': 44387,\n",
       " 'cachimbo_pt': 15579,\n",
       " 'madeira_pt': 36669,\n",
       " 'faixa_pt': 26312,\n",
       " 'auto_pt': 11954,\n",
       " 'colante': 18687,\n",
       " 'cores_pt': 19753,\n",
       " 'sortidas': 51992,\n",
       " '10cm': 680,\n",
       " 'pata_pt': 43134,\n",
       " 'resina_pt': 48164,\n",
       " 'decoracao': 21404,\n",
       " 'jardim_pt': 33200,\n",
       " 'aves_pt': 12143,\n",
       " 'enfeites': 24543,\n",
       " 'maquina': 37188,\n",
       " 'sublimar': 52819,\n",
       " 'estampar': 25420,\n",
       " 'goldex': 29506,\n",
       " '60x40': 6549,\n",
       " 'grip_pt': 29945,\n",
       " 'canon_pt': 16195,\n",
       " 't7i': 53438,\n",
       " '77d': 7340,\n",
       " 'pronta_pt': 46115,\n",
       " 'entrega_pt': 24679,\n",
       " 'notebook': 41128,\n",
       " 'hp': 31444,\n",
       " 'pavilion': 43248,\n",
       " 'special': 52115,\n",
       " 'edition': 23782,\n",
       " 'individual': 32229,\n",
       " 'servilleta': 50708,\n",
       " 'gabardina': 28516,\n",
       " 'colores': 18857,\n",
       " 'individuales': 32231,\n",
       " 'disfraz': 22450,\n",
       " 'general': 29013,\n",
       " 'adultos': 9174,\n",
       " 'talla': 53618,\n",
       " 'unica': 56528,\n",
       " 'coser': 19938,\n",
       " 'brother': 14962,\n",
       " 'industrial': 32253,\n",
       " 'maquininha': 37196,\n",
       " 'cartao': 16644,\n",
       " 'mercadopago_pt': 38152,\n",
       " 'pague_pt': 42531,\n",
       " 'seguro_pt': 50383,\n",
       " 'ate_pt': 11699,\n",
       " '12x_pt': 1273,\n",
       " 'fernet': 26692,\n",
       " 'branca': 14672,\n",
       " 'menta': 38127,\n",
       " '450': 5176,\n",
       " 'cc': 17029,\n",
       " '100': 377,\n",
       " 'caja': 15692,\n",
       " 'oportunidad': 41881,\n",
       " 'dell': 21565,\n",
       " 'i3': 31682,\n",
       " '4gb': 5463,\n",
       " 'ddr4': 21322,\n",
       " 'hd': 30650,\n",
       " '1tb': 2549,\n",
       " 'win': 58746,\n",
       " '10': 376,\n",
       " 'roda_pt': 48827,\n",
       " 'traseira': 55563,\n",
       " 'amarok_pt': 10144,\n",
       " '2010': 2687,\n",
       " 'teclado_pt': 53985,\n",
       " 'casio_pt': 16777,\n",
       " 'wk': 58825,\n",
       " '240': 3139,\n",
       " '76_pt': 7312,\n",
       " 'teclas_pt': 53989,\n",
       " 'profissional_pt': 46023,\n",
       " 'standard_pt': 52394,\n",
       " 'schwarzkopf': 50136,\n",
       " 'fibre': 26819,\n",
       " 'force': 27578,\n",
       " 'mascarilla': 37488,\n",
       " 'cuadros': 20563,\n",
       " 'decorativos': 21429,\n",
       " 'modernos': 39022,\n",
       " 'arte': 11422,\n",
       " '30x42': 4080,\n",
       " 'lamina': 34743,\n",
       " 'atornillador': 11794,\n",
       " 'taladro': 53588,\n",
       " 'inalambrico': 32113,\n",
       " 'amoladora': 10269,\n",
       " 'philco': 44035,\n",
       " 'aquecedor': 11048,\n",
       " 'conforto_pt': 19348,\n",
       " 'ceramico_pt': 17248,\n",
       " '127v_pt': 1136,\n",
       " 'cadence_pt': 15623,\n",
       " 'hermoso': 30871,\n",
       " 'arbol': 11115,\n",
       " 'navidad': 40487,\n",
       " 'madera': 36677,\n",
       " 'pino': 44305,\n",
       " 'pastilha': 43120,\n",
       " 'cb300': 16985,\n",
       " 'abs_pt': 8648,\n",
       " 'diafrag': 22092,\n",
       " 'mesa_pt': 38202,\n",
       " 'lateral_pt': 34932,\n",
       " 'rovere': 49179,\n",
       " 'italiano_pt': 32959,\n",
       " 'edn_pt': 23790,\n",
       " 'moveis_pt': 39634,\n",
       " 'bateria': 13012,\n",
       " 'honda': 31300,\n",
       " 'transalp': 55444,\n",
       " '600': 6397,\n",
       " 'quadflex': 46658,\n",
       " 'sensor_pt': 50594,\n",
       " 'posicao': 45367,\n",
       " 'comando_pt': 18903,\n",
       " 'original_pt': 42055,\n",
       " 'camara': 15891,\n",
       " 'instantanea': 32507,\n",
       " 'fujifilm': 28182,\n",
       " 'instax': 32515,\n",
       " 'mini': 38668,\n",
       " 'azul': 12262,\n",
       " 'cobalto': 18526,\n",
       " 'relogio_pt': 47921,\n",
       " 'despertador_pt': 21936,\n",
       " 'digital_pt': 22255,\n",
       " 'amarelo_pt': 10128,\n",
       " 'hiunday': 31157,\n",
       " 'santa_pt': 49869,\n",
       " 'radiador': 47006,\n",
       " 'condensador': 19240,\n",
       " 'ventoinha': 57428,\n",
       " '24': 3138,\n",
       " 'grips': 29947,\n",
       " 'yonex': 59575,\n",
       " 'hi': 30950,\n",
       " 'grap': 29792,\n",
       " 'surtidos': 53138,\n",
       " '15_pt': 1702,\n",
       " 'lanterna_pt': 34826,\n",
       " 'cabeca': 15482,\n",
       " 'triplo_pt': 55839,\n",
       " 't6_pt': 53429,\n",
       " '3led': 4710,\n",
       " 'cree_pt': 20220,\n",
       " 'tatica_pt': 53827,\n",
       " 'tv_pt': 56198,\n",
       " '55_pt': 6046,\n",
       " 'polegadas': 44989,\n",
       " 'branco_pt': 14676,\n",
       " 'canela_pt': 16137,\n",
       " 'set': 50725,\n",
       " 'latas': 34924,\n",
       " 'mate': 37601,\n",
       " 'modelo': 39001,\n",
       " 'alicia': 9811,\n",
       " 'deco': 21391,\n",
       " 'camiseta_pt': 15982,\n",
       " 'raglan_pt': 47046,\n",
       " 'crepusculo_pt': 20252,\n",
       " 'baby_pt': 12386,\n",
       " 'look_pt': 36021,\n",
       " 'castillo': 16829,\n",
       " 'tobogan': 54907,\n",
       " 'vende': 57343,\n",
       " 'galaxy': 28576,\n",
       " 'tab3': 53456,\n",
       " 'wi': 58697,\n",
       " 'fi': 26792,\n",
       " 'fly_pt': 27438,\n",
       " 'f16_pt': 26106,\n",
       " 'vermelho_pt': 57504,\n",
       " 'preto_pt': 45781,\n",
       " 'moda': 38976,\n",
       " 'bobo': 14119,\n",
       " 'peluca': 43520,\n",
       " 'mujeres': 39839,\n",
       " 'dama': 21118,\n",
       " 'corta': 19870,\n",
       " 'derecho': 21717,\n",
       " 'cabello': 15495,\n",
       " 'negro': 40598,\n",
       " 'portatil_pt': 45311,\n",
       " 'leve_pt': 35328,\n",
       " 'rodas_pt': 48842,\n",
       " 'texas_pt': 54431,\n",
       " 'camping_pt': 16035,\n",
       " 'lavarropas': 35008,\n",
       " 'repuesto': 48114,\n",
       " 'carga': 16470,\n",
       " 'superior': 53025,\n",
       " 'angosto': 10516,\n",
       " 'brigadeiro_pt': 14810,\n",
       " 'caseiros': 16752,\n",
       " 'termica_pt': 54278,\n",
       " 'cooler_pt': 19616,\n",
       " '32_pt': 4213,\n",
       " 'litros_pt': 35758,\n",
       " 'termolar': 54316,\n",
       " 'suv_pt': 53186,\n",
       " 'extra_pt': 26025,\n",
       " 'sx': 53269,\n",
       " 'wifi': 58713,\n",
       " 'nfc_pt': 40765,\n",
       " 'consulte_pt': 19454,\n",
       " 'bitarra': 13801,\n",
       " 'sombra_pt': 51856,\n",
       " 'asa_pt': 11527,\n",
       " 'borboleta_pt': 14397,\n",
       " '239_pt': 3120,\n",
       " '164_pt': 1878,\n",
       " 'oferta_pt': 41562,\n",
       " 'especial_pt': 25194,\n",
       " 'farinha_pt': 26435,\n",
       " 'mandioca_pt': 37005,\n",
       " 'belem_pt': 13282,\n",
       " 'bajo': 12515,\n",
       " 'cuerdas': 20665,\n",
       " 'lazer': 35030,\n",
       " 'modificado': 39025,\n",
       " 'silla': 51153,\n",
       " 'estilo': 25510,\n",
       " 'antigua': 10710,\n",
       " 'apc': 10853,\n",
       " 'limpiador': 35578,\n",
       " 'multiproposito': 39952,\n",
       " 'tapizado': 53746,\n",
       " 'motor': 39552,\n",
       " 'formula': 27641,\n",
       " 'nan': 40331,\n",
       " 'optipro': 41915,\n",
       " 'mayores': 37754,\n",
       " 'ano': 10585,\n",
       " '800g': 7497,\n",
       " 'proyector': 46243,\n",
       " 'mercurio': 38167,\n",
       " 'halogenado': 30442,\n",
       " 'equipo': 24817,\n",
       " '150': 1563,\n",
       " 'inc': 32120,\n",
       " 'oficial': 41597,\n",
       " 'nacional': 40281,\n",
       " 'baloncesto': 12625,\n",
       " 'asociacion': 11578,\n",
       " 'inch': 32133,\n",
       " 'compressor_pt': 19141,\n",
       " 'condicionado_pt': 19253,\n",
       " 'astra_pt': 11646,\n",
       " '95': 8195,\n",
       " '96': 8233,\n",
       " 'gm_pt': 29438,\n",
       " 'tabua': 53495,\n",
       " 'corte_pt': 19901,\n",
       " 'vidro_pt': 57764,\n",
       " '35cm_pt': 4430,\n",
       " 'estampa_pt': 25410,\n",
       " 'antiderrapante_pt': 10676,\n",
       " 'churrasco_pt': 17900,\n",
       " 'carenagem': 16457,\n",
       " 'tampa_pt': 53669,\n",
       " 'nxr': 41385,\n",
       " 'bros_pt': 14957,\n",
       " '125_pt': 1106,\n",
       " '2013_pt': 2695,\n",
       " 'break_pt': 14754,\n",
       " 'light_pt': 35484,\n",
       " 'lente_pt': 35251,\n",
       " 'cristal_pt': 20328,\n",
       " 'leds_pt': 35118,\n",
       " 'fox_pt': 27769,\n",
       " '2018': 2704,\n",
       " 'inflador': 32314,\n",
       " '12': 978,\n",
       " 'vol': 58123,\n",
       " 'adaptadores': 9037,\n",
       " 'notebook_pt': 41129,\n",
       " 'laptop_pt': 34859,\n",
       " 'dobravel': 22683,\n",
       " '2_pt': 3653,\n",
       " 'coolers_pt': 19620,\n",
       " 'macacao': 36576,\n",
       " 'bebe_pt': 13206,\n",
       " 'menina_pt': 38099,\n",
       " 'carters_pt': 16666,\n",
       " 'dvd_pt': 23356,\n",
       " 'gravador_pt': 29839,\n",
       " 'defeito': 21462,\n",
       " 'apenas_pt': 10861,\n",
       " 'carregador': 16589,\n",
       " 'bateria_pt': 13013,\n",
       " 'sony_pt': 51933,\n",
       " 'bc': 13119,\n",
       " 'trv': 55978,\n",
       " 'np_pt': 41208,\n",
       " 'cdi': 17058,\n",
       " 'pietcard': 44178,\n",
       " 'original': 42054,\n",
       " 'registradora': 47805,\n",
       " 'moretti': 39391,\n",
       " 'enciende': 24470,\n",
       " 'escova': 25007,\n",
       " 'motor_pt': 39553,\n",
       " 'arranque_pt': 11380,\n",
       " 'partida_pt': 43007,\n",
       " 'yamaha_pt': 59439,\n",
       " 'wr_pt': 58940,\n",
       " '250_pt': 3280,\n",
       " '03': 150,\n",
       " '13': 1275,\n",
       " 'grampeador': 29748,\n",
       " 'power_pt': 45470,\n",
       " 'roupa_pt': 49161,\n",
       " 'mergulho': 38174,\n",
       " 'neoprene_pt': 40640,\n",
       " '5mm_pt': 6275,\n",
       " 'cressi': 20259,\n",
       " 'lady_pt': 34684,\n",
       " 'llave': 35837,\n",
       " 'impacto': 31987,\n",
       " 'neumatica': 40696,\n",
       " 'enc': 24420,\n",
       " 'konan': 34343,\n",
       " '01': 73,\n",
       " 'bola_pt': 14195,\n",
       " 'volei_pt': 58145,\n",
       " 'quadra_pt': 46661,\n",
       " 'xalingo': 59157,\n",
       " 'sillon': 51158,\n",
       " 'barcelona': 12836,\n",
       " 'van': 56984,\n",
       " 'der': 21708,\n",
       " 'fabricantes': 26225,\n",
       " 'refil_pt': 47680,\n",
       " 'pistola_pt': 44465,\n",
       " 'cola_pt': 18677,\n",
       " 'quente_pt': 46767,\n",
       " 'transparente_pt': 55517,\n",
       " 'cis_pt': 18134,\n",
       " 'grosso_pt': 29981,\n",
       " 'mini_pt': 38669,\n",
       " 'cameras_pt': 15941,\n",
       " 'ocultas_pt': 41514,\n",
       " 'espia_pt': 25249,\n",
       " 'full_pt': 28195,\n",
       " 'hd_pt': 30672,\n",
       " '720p_pt': 7147,\n",
       " 'micro_pt': 38414,\n",
       " 'almofada': 9923,\n",
       " 'espreguicadeira': 25301,\n",
       " 'caes_pt': 15642,\n",
       " 'gatos_pt': 28862,\n",
       " 'off_pt': 41573,\n",
       " 'white_pt': 58686,\n",
       " '60cm': 6492,\n",
       " 'cruza': 20466,\n",
       " 'palabras': 42576,\n",
       " 'juego': 33580,\n",
       " 'mesa': 38201,\n",
       " 'cruzadas': 20469,\n",
       " 'fichas': 26833,\n",
       " 'griferia': 29916,\n",
       " 'bano': 12748,\n",
       " 'ducha': 23179,\n",
       " 'embutir': 24316,\n",
       " 'piazza': 44096,\n",
       " 'optima': 41904,\n",
       " 'sabonete': 49543,\n",
       " 'natura_pt': 40440,\n",
       " 'homem_pt': 31276,\n",
       " 'essence_pt': 25352,\n",
       " '6_pt': 6860,\n",
       " 'unidades_pt': 56551,\n",
       " '90g_pt': 8050,\n",
       " 'cada_pt': 15603,\n",
       " 'avent': 12125,\n",
       " 'combo': 18929,\n",
       " 'outside': 42257,\n",
       " 'mix': 38832,\n",
       " 'vas': 57123,\n",
       " 'ch': 17384,\n",
       " 'fan_pt': 26379,\n",
       " 'dell_pt': 21566,\n",
       " 'r420': 46920,\n",
       " '24h_pt': 3228,\n",
       " 'eames': 23573,\n",
       " 'acrilico': 8923,\n",
       " 'rojo': 48889,\n",
       " 'eletrico_pt': 24103,\n",
       " 'monofasico_pt': 39230,\n",
       " 'cavalos_pt': 16945,\n",
       " 'polos_pt': 45087,\n",
       " 'baixa_pt': 12497,\n",
       " 'rotacao': 49095,\n",
       " '1o_pt': 2525,\n",
       " 'grau_pt': 29827,\n",
       " 'andador': 10431,\n",
       " 'celeste': 17122,\n",
       " 'usado': 56707,\n",
       " 'buddy': 15114,\n",
       " 'estabilizador_pt': 25378,\n",
       " '300va': 3930,\n",
       " 'bivolt': 13815,\n",
       " 'tomadas_pt': 54976,\n",
       " 'sms_pt': 51610,\n",
       " 'ukelele': 56355,\n",
       " 'tipo': 54737,\n",
       " 'concierto': 19226,\n",
       " 'molokai': 39146,\n",
       " 'torta': 55181,\n",
       " 'unicornio': 56537,\n",
       " 'apliques': 10914,\n",
       " 'porcelana': 45209,\n",
       " 'cuerno': 20667,\n",
       " 'orejas': 41966,\n",
       " 'ojos': 41647,\n",
       " 'flores': 27356,\n",
       " 'ring_pt': 48600,\n",
       " 'rose': 49061,\n",
       " '08_pt': 305,\n",
       " 'leds': 35117,\n",
       " '80_pt': 7554,\n",
       " 'sup_pt': 52997,\n",
       " 'cel': 17109,\n",
       " 'g_pt': 28506,\n",
       " 'manga_pt': 37032,\n",
       " 'gola_pt': 29499,\n",
       " 'redonda_pt': 47618,\n",
       " 'coquetel': 19683,\n",
       " 'bodycon_pt': 14158,\n",
       " 'casamento_pt': 16720,\n",
       " 'noite_pt': 41037,\n",
       " '07_pt': 281,\n",
       " 'cambio_pt': 15913,\n",
       " 'civic_pt': 18174,\n",
       " 'exs_pt': 25974,\n",
       " '8_pt': 7869,\n",
       " 'flex_pt': 27275,\n",
       " '16v_pt': 1933,\n",
       " 'aut': 11937,\n",
       " 'ab': 8529,\n",
       " '41': 4985,\n",
       " 'copo_pt': 19662,\n",
       " 'malibu_pt': 36914,\n",
       " '350ml_pt': 4382,\n",
       " 'cafe_pt': 15648,\n",
       " 'jgo': 33353,\n",
       " 'cojinetes': 18670,\n",
       " 'biela': 13587,\n",
       " 'ks': 34431,\n",
       " 'supermedida': 53037,\n",
       " '25': 3269,\n",
       " 'audi': 11857,\n",
       " 'acabamento': 8720,\n",
       " 'registro_pt': 47808,\n",
       " 'gaveta_pt': 28880,\n",
       " 'chuveiro_pt': 17918,\n",
       " 'pressao': 45747,\n",
       " 'protectores': 46185,\n",
       " 'paragolpes': 42878,\n",
       " 'ford': 27584,\n",
       " 'fiesta': 26866,\n",
       " 'kinetic': 34162,\n",
       " 'home_pt': 31269,\n",
       " 'theater_pt': 54491,\n",
       " 'lg_pt': 35365,\n",
       " 'novissimo': 41184,\n",
       " 'berco': 13395,\n",
       " '10_pt': 670,\n",
       " 'cortina_pt': 19915,\n",
       " 'doce_pt': 22690,\n",
       " 'encanto_pt': 24435,\n",
       " 'rosa_pt': 49040,\n",
       " 'cadeira_pt': 15614,\n",
       " 'feita_pt': 26595,\n",
       " 'mao_pt': 37147,\n",
       " 'impecavel': 31997,\n",
       " 'cana': 16062,\n",
       " 'pesca': 43899,\n",
       " 'tramos': 55428,\n",
       " 'motocompressor': 39528,\n",
       " '24_pt': 3215,\n",
       " '110v_pt': 856,\n",
       " 'pulseras': 46442,\n",
       " 'rolo': 48947,\n",
       " 'acero': 8806,\n",
       " 'quirurgico': 46839,\n",
       " 'mayorista': 37755,\n",
       " 'hombre': 31262,\n",
       " 'minecraft': 38649,\n",
       " 'xl': 59248,\n",
       " 'gabinete': 28523,\n",
       " 'pc': 43277,\n",
       " 'thermaltake': 54515,\n",
       " 'v3': 56807,\n",
       " 'black': 13851,\n",
       " 'envio': 24717,\n",
       " 'estampitas': 25427,\n",
       " 'comunion': 19185,\n",
       " 'bautismo': 13077,\n",
       " 'nacimientos': 40278,\n",
       " 'gel_pt': 28956,\n",
       " 'comestivel': 18958,\n",
       " 'lubrificante_pt': 36191,\n",
       " 'massageador': 37527,\n",
       " 'frente': 27939,\n",
       " 'stereo': 52512,\n",
       " 'pioneer': 44372,\n",
       " 'deh': 21502,\n",
       " 'inmaculado': 32400,\n",
       " 'tira': 54751,\n",
       " 'rgb': 48463,\n",
       " 'exterior': 26008,\n",
       " '5mt': 6280,\n",
       " '300': 3853,\n",
       " 'nx': 41375,\n",
       " '2014': 2696,\n",
       " 'krros': 34423,\n",
       " 'mestre_pt': 38229,\n",
       " 'c_pt': 15462,\n",
       " '2012_pt': 2692,\n",
       " 'canos': 16200,\n",
       " '20mts': 2821,\n",
       " 'cuotas': 20738,\n",
       " 'intereses': 32632,\n",
       " 'retalho': 48263,\n",
       " 'tecidos': 53977,\n",
       " 'guarda_pt': 30119,\n",
       " 'chuva_pt': 17914,\n",
       " 'sombrinha': 51865,\n",
       " 'longa_pt': 36002,\n",
       " 'gotas_pt': 29621,\n",
       " '50x50': 5795,\n",
       " 'oro': 42092,\n",
       " 'plata': 44680,\n",
       " 'mould': 39608,\n",
       " 'fusion': 28318,\n",
       " 'grafito': 29729,\n",
       " '35': 4366,\n",
       " 'tong': 55029,\n",
       " 'cartuchera': 16685,\n",
       " 'canopla': 16198,\n",
       " 'cierre': 17986,\n",
       " 'cars': 16636,\n",
       " 'disney': 22462,\n",
       " 'wabro': 58375,\n",
       " 'ems_pt': 24407,\n",
       " 'dezenas': 22049,\n",
       " 'eletrica_pt': 24098,\n",
       " '5_pt': 6192,\n",
       " 'modos_pt': 39034,\n",
       " 'muscle_pt': 40040,\n",
       " 'transmissao': 55502,\n",
       " 'completo_pt': 19090,\n",
       " 'xre': 59321,\n",
       " 'rally_pt': 47087,\n",
       " 'pantalon': 42763,\n",
       " 'revit': 48419,\n",
       " 'enterprise': 24663,\n",
       " 'lady': 34683,\n",
       " 'motorman': 39573,\n",
       " 'nitro': 40915,\n",
       " 'design_pt': 21863,\n",
       " 'cueca_pt': 20647,\n",
       " 'tapa_pt': 53720,\n",
       " 'sexo_pt': 50753,\n",
       " 'listrada': 35731,\n",
       " 'vermelha_pt': 57501,\n",
       " 'pulseira': 46438,\n",
       " 'mormaii': 39402,\n",
       " 'qualidade_pt': 46693,\n",
       " 'garantida_pt': 28729,\n",
       " 'labial': 34597,\n",
       " 'liquido': 35693,\n",
       " 'pink': 44299,\n",
       " '21': 2872,\n",
       " 'gato': 28857,\n",
       " 'x6': 59119,\n",
       " 'unidades': 56550,\n",
       " 'bomba_pt': 14263,\n",
       " 'agua': 9397,\n",
       " '2009_pt': 2628,\n",
       " '127_pt': 1132,\n",
       " 'v_pt': 56846,\n",
       " 'descartavel': 21791,\n",
       " 'estilete_pt': 25505,\n",
       " 'grande_pt': 29762,\n",
       " 'preto': 45780,\n",
       " 'amarelo': 10127,\n",
       " 'tools': 55056,\n",
       " 'amortecedor': 10280,\n",
       " 'traseiro': 55565,\n",
       " 'cofap': 18635,\n",
       " '2005_pt': 2619,\n",
       " 'bioderma_pt': 13700,\n",
       " 'sensibio': 50579,\n",
       " 'mascara_pt': 37481,\n",
       " '75ml': 7275,\n",
       " 'sensiveis': 50588,\n",
       " 'platillos': 44704,\n",
       " 'sabian': 49535,\n",
       " 'sbr': 50060,\n",
       " 'hat': 30582,\n",
       " 'crash': 20165,\n",
       " '16': 1797,\n",
       " 'practicuna': 45542,\n",
       " 'piedi': 44160,\n",
       " 'transistor_pt': 55483,\n",
       " 'lote_pt': 36066,\n",
       " 'sache_pt': 49580,\n",
       " 'perfumado_pt': 43734,\n",
       " 'via_pt': 57645,\n",
       " 'aroma_pt': 11325,\n",
       " '10g_pt': 696,\n",
       " 'maca_pt': 36575,\n",
       " 'flauta_pt': 27243,\n",
       " 'bicos_pt': 13567,\n",
       " 'injetores': 32389,\n",
       " 'linha_pt': 35641,\n",
       " 'renault_pt': 48011,\n",
       " 'fluence_pt': 27400,\n",
       " 'compare': 19039,\n",
       " 'grelha': 29887,\n",
       " 'aluminio_pt': 10048,\n",
       " '87_pt': 7821,\n",
       " 'cm_pt': 18464,\n",
       " 'comprimento': 19145,\n",
       " '40_pt': 4903,\n",
       " 'largura_pt': 34896,\n",
       " 'tenys': 54244,\n",
       " 'pe_pt': 43335,\n",
       " 'baruel': 12931,\n",
       " 'po_pt': 44881,\n",
       " 'canforado': 16149,\n",
       " '1_pt': 2435,\n",
       " 'orignal_pt': 42064,\n",
       " '60g_pt': 6499,\n",
       " 'pollera': 45070,\n",
       " 'negra': 40594,\n",
       " 'cintas': 18058,\n",
       " 'rotura': 49150,\n",
       " '36': 4463,\n",
       " '44': 5136,\n",
       " 'p1': 42357,\n",
       " 'reloj': 47923,\n",
       " 'decoraciones': 21410,\n",
       " 'florales': 27338,\n",
       " 'funciona': 28229,\n",
       " 'latex_pt': 34935,\n",
       " 'tam_pt': 53638,\n",
       " 'descarpack': 21787,\n",
       " 'cx_pt': 20872,\n",
       " 'excelente': 25820,\n",
       " 'jarra': 33211,\n",
       " 'jofaina': 33448,\n",
       " 'art': 11419,\n",
       " 'nouveau': 41140,\n",
       " 'wmf': 58835,\n",
       " 'campainha': 16005,\n",
       " 'fio_pt': 27032,\n",
       " 'wireless_pt': 58801,\n",
       " 'mts_pt': 39772,\n",
       " 'dong_pt': 22820,\n",
       " 'envio_pt': 24719,\n",
       " 'imediato_pt': 31965,\n",
       " 'sensor': 50593,\n",
       " 'rpm': 49209,\n",
       " 'fae': 26295,\n",
       " 'multiuso_pt': 39971,\n",
       " 'inflar_pt': 32318,\n",
       " 'boia_pt': 14179,\n",
       " 'liquidos_pt': 35696,\n",
       " 'western_pt': 58636,\n",
       " 'album_pt': 9638,\n",
       " 'fotografico_pt': 27740,\n",
       " 'infantil_pt': 32283,\n",
       " 'fotos_pt': 27753,\n",
       " '10x15_pt': 786,\n",
       " 'pcs_pt': 43307,\n",
       " 'perchas': 43659,\n",
       " 'plastico': 44659,\n",
       " 'blanca': 13878,\n",
       " 'adulto': 9172,\n",
       " 'nino': 40874,\n",
       " 'bebe': 13205,\n",
       " 'redoblante': 47615,\n",
       " 'tama': 53639,\n",
       " 'maple': 37163,\n",
       " 'arce': 11130,\n",
       " '14': 1413,\n",
       " 'evans': 25721,\n",
       " 'dolores': 22761,\n",
       " 'pradera': 45545,\n",
       " 'amigos': 10246,\n",
       " 'cd': 17050,\n",
       " 'listo': 35726,\n",
       " 'termo': 54296,\n",
       " 'autocebante': 11975,\n",
       " 'inoxidable': 32434,\n",
       " 'natura': 40439,\n",
       " 'ekos': 23948,\n",
       " 'cumaru': 20711,\n",
       " 'jabon': 33079,\n",
       " 'manos': 37096,\n",
       " 'oferta': 41561,\n",
       " 'pentax_pt': 43598,\n",
       " 'carrinho': 16613,\n",
       " 'dodge_pt': 22722,\n",
       " 'viper_pt': 57897,\n",
       " 'controle_pt': 19562,\n",
       " 'remoto_pt': 47979,\n",
       " 'extremo': 26052,\n",
       " 'toyota': 55279,\n",
       " 'land': 34802,\n",
       " 'cruiser': 20457,\n",
       " 'prado': 45546,\n",
       " '20': 2589,\n",
       " 'macaco_pt': 36577,\n",
       " 'hidraulico_pt': 30991,\n",
       " 'sanfona': 49829,\n",
       " 'subaru_pt': 52798,\n",
       " 'legacy_pt': 35135,\n",
       " 'tonelada_pt': 55022,\n",
       " '42_pt': 5060,\n",
       " 'pip': 44381,\n",
       " 'plasma': 44644,\n",
       " 'mod_pt': 38975,\n",
       " 'mc': 37791,\n",
       " 'heladera': 30760,\n",
       " 'gafa': 28543,\n",
       " '380': 4572,\n",
       " 'impecable': 31995,\n",
       " 'urgente': 56670,\n",
       " 'billetera': 13657,\n",
       " 'sakura': 49658,\n",
       " 'dije': 22268,\n",
       " 'swarovski': 53215,\n",
       " 'cruz': 20464,\n",
       " 'equilibrio': 24801,\n",
       " 'cristal': 20327,\n",
       " 'cadena': 15620,\n",
       " 'dorada': 22851,\n",
       " 'hidrometro': 31019,\n",
       " 'lcd_pt': 35049,\n",
       " 'k24': 33741,\n",
       " 'gasolina': 28816,\n",
       " 'diesel_pt': 22208,\n",
       " 'lacrado_pt': 34646,\n",
       " 'zapatilla': 59769,\n",
       " 'protector': 46181,\n",
       " 'tension': 54234,\n",
       " 'consumo': 19457,\n",
       " '20a': 2776,\n",
       " 'armario_pt': 11279,\n",
       " 'rustico_pt': 49357,\n",
       " 'portas_pt': 45304,\n",
       " 'demolicao': 21605,\n",
       " 'tapa': 53719,\n",
       " 'cilindros': 18020,\n",
       " 'fiat': 26802,\n",
       " 'uno': 56624,\n",
       " 'fiorino': 27040,\n",
       " 'palio': 42620,\n",
       " 'fire': 27050,\n",
       " 'valv': 56964,\n",
       " 'ampolla': 10334,\n",
       " 'alfaparf': 9752,\n",
       " 'shine': 50922,\n",
       " 'lotion': 36069,\n",
       " 'semi': 50488,\n",
       " 'di': 22078,\n",
       " 'lino': 35653,\n",
       " 'diamante': 22112,\n",
       " 'x1': 58990,\n",
       " 'flotador': 27378,\n",
       " 'intex': 32682,\n",
       " 'cocodrilo': 18597,\n",
       " 'samsung_pt': 49785,\n",
       " 'galaxy_pt': 28577,\n",
       " 'mega_pt': 37968,\n",
       " 'azul_pt': 12263,\n",
       " 'monitor': 39196,\n",
       " 'curvo': 20813,\n",
       " 'gamer': 28660,\n",
       " '34': 4323,\n",
       " 'lg': 35364,\n",
       " 'ultrawide': 56433,\n",
       " '144hz': 1484,\n",
       " 'ips': 32818,\n",
       " 'banda': 12682,\n",
       " 'abrasiva': 8616,\n",
       " '80': 7474,\n",
       " 'zirconio': 59887,\n",
       " 'klingspor': 34243,\n",
       " 'x3': 59062,\n",
       " 'giratoria_pt': 29270,\n",
       " 'turim': 56149,\n",
       " 'secretaria_pt': 50305,\n",
       " 'plastica_pt': 44656,\n",
       " 'parafuso': 42873,\n",
       " 'porta_pt': 45247,\n",
       " 'ix35_pt': 33018,\n",
       " '03_pt': 174,\n",
       " 'rolamento': 48904,\n",
       " 'dianteira': 22133,\n",
       " 'bmw_pt': 14079,\n",
       " '530_pt': 5928,\n",
       " 'new_pt': 40727,\n",
       " 'beauty_pt': 13198,\n",
       " 'lagrimas_pt': 34703,\n",
       " '30ml_pt': 4036,\n",
       " 'box_pt': 14585,\n",
       " '10pcs': 744,\n",
       " 'micro': 38413,\n",
       " 'smt': 51611,\n",
       " '2pin': 3767,\n",
       " 'conector': 19292,\n",
       " 'femenino': 26639,\n",
       " 'socket': 51701,\n",
       " 'android': 10470,\n",
       " 'co': 18497,\n",
       " 'molde': 39101,\n",
       " 'cortante': 19890,\n",
       " 'galletitas': 28607,\n",
       " 'dinosaurios': 22339,\n",
       " 'cookie': 19606,\n",
       " 'cutter': 20845,\n",
       " 'ventilador': 57418,\n",
       " 'pie': 44151,\n",
       " '72': 7136,\n",
       " 'velocidades': 57297,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(tfidf_vect, 'tfidf_vect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split('(\\d+)',text) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodings_3_layers_0.npy\n",
      "lr before: 0.0002\n",
      "Train on 900000 samples, validate on 100000 samples\n",
      "900000/900000 [==============================] - 1074s 1ms/sample - loss: 0.9977 - sparse_categorical_accuracy: 0.8001 - val_loss: 0.8735 - val_sparse_categorical_accuracy: 0.8239\n",
      "lr after: 0.0002\n",
      "balanced_accuracy: 0.7878131573558538\n",
      "encodings_3_layers_1.npy\n",
      "lr before: 0.0002\n",
      "Train on 900000 samples, validate on 100000 samples\n",
      "900000/900000 [==============================] - 1083s 1ms/sample - loss: 0.9617 - sparse_categorical_accuracy: 0.8064 - val_loss: 0.8439 - val_sparse_categorical_accuracy: 0.8290\n",
      "lr after: 0.0002\n",
      "balanced_accuracy: 0.7945393969624746\n",
      "encodings_3_layers_2.npy\n",
      "lr before: 0.0002\n",
      "Train on 900000 samples, validate on 100000 samples\n",
      "900000/900000 [==============================] - 1089s 1ms/sample - loss: 0.9340 - sparse_categorical_accuracy: 0.8116 - val_loss: 0.8435 - val_sparse_categorical_accuracy: 0.8289\n",
      "lr after: 0.0002\n",
      "balanced_accuracy: 0.7977145227319922\n",
      "encodings_3_layers_3.npy\n",
      "lr before: 0.0002\n",
      "Train on 900000 samples, validate on 100000 samples\n",
      "900000/900000 [==============================] - 1068s 1ms/sample - loss: 0.9131 - sparse_categorical_accuracy: 0.8145 - val_loss: 0.8507 - val_sparse_categorical_accuracy: 0.8272\n",
      "lr after: 0.0002\n",
      "balanced_accuracy: 0.7931988681934606\n",
      "encodings_3_layers_4.npy\n",
      "lr before: 1e-04\n",
      "Train on 900000 samples, validate on 100000 samples\n",
      "900000/900000 [==============================] - 1083s 1ms/sample - loss: 0.8871 - sparse_categorical_accuracy: 0.8192 - val_loss: 0.8374 - val_sparse_categorical_accuracy: 0.8305\n",
      "lr after: 1e-04\n",
      "balanced_accuracy: 0.7969944545986862\n",
      "encodings_3_layers_5.npy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-fe786c6e1f95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter_num\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen_data\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/large/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoderCat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr before: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4a051cdf89ec>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(path, labels, encoder, test_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2212\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2212\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filenames = [f for f in listdir(path+'large')]\n",
    "filenames.sort(key=natural_keys)\n",
    "epochs = 1\n",
    "len_data = 1000000\n",
    "decay = 1\n",
    "best_balanced_accuracy = 0\n",
    "\n",
    "for _ in range(epochs):\n",
    "    iter_num = 0\n",
    "    \n",
    "#     clr = CyclicLR(base_lr=0.001*decay, max_lr=0.002*decay, step_size=20000., mode='triangular2')\n",
    "    tf.keras.backend.set_value(model.optimizer.lr, 0.001*0.2)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        labels = df['category'].iloc[iter_num*len_data : (iter_num+1)*len_data]\n",
    "        X_train, X_val, y_train, y_val = get_data(path + '/large/' + filename, labels, encoderCat, test_size=0.1)\n",
    "        \n",
    "        print('lr before: ' + str(tf.keras.backend.get_value(model.optimizer.lr)))\n",
    "        \n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "               batch_size=256, epochs=1, verbose=1,)# callbacks=[clr])\n",
    "        \n",
    "        print('lr after: ' + str(tf.keras.backend.get_value(model.optimizer.lr)))\n",
    "        balanced_accuracy = calculate_balanced_accuracy(X_val, y_val)\n",
    "        print('balanced_accuracy: ' + str(balanced_accuracy))\n",
    "        \n",
    "        if balanced_accuracy > best_balanced_accuracy:\n",
    "            best_balanced_accuracy = balanced_accuracy\n",
    "        else:\n",
    "            tf.keras.backend.set_value(model.optimizer.lr, tf.keras.backend.get_value(model.optimizer.lr)*0.5)\n",
    "        \n",
    "        del X_train, X_val, y_train, y_val\n",
    "        gc.collect()\n",
    "\n",
    "        iter_num += 1\n",
    "        \n",
    "    decay = decay*0.2\n",
    "    tf.keras.backend.set_value(model.optimizer.lr, tf.keras.backend.get_value(model.optimizer.lr)*decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.set_value(model.optimizer.lr, 0.001*0.2)\n",
    "# tf.keras.backend.get_value(model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7fde1a3ebcf8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.save('model2')\n",
    "# model.save_weights('model_weights2')\n",
    "# model.load_weights('model_weights2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triangular: (max_lr=0.003, Adam)\n",
    "# it 8: 0.718 60k step_size, 0.707 30k step_size, 0.722 20k step_size, 0.7154 10k step_size \n",
    "# it 12:                     0.737 30k step_size, 0.754 20k step_size, 0.7515 10k step_size\n",
    "# it 19:                                          0.738 20k step_size\n",
    "\n",
    "# triangular2:\n",
    "\n",
    "# it 8:  0.723 20k step\n",
    "# it 12: 0.752 20k step\n",
    "# it 19: 0.752 20k step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.6752 val - 0.709 test\n",
    "#0.7335 val - 0.770 test\n",
    "#0.7540 val - 0.794 test\n",
    "#0.7793 val - 0.819 test\n",
    "#0.7850 val - 0.823 test\n",
    "#0.8136 val - 0.838 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv(path + 'test.csv')\n",
    "# bc = BertClient()\n",
    "# titles = df_test['title'].str.lower().values.tolist()\n",
    "# encodings = bc.encode(titles)\n",
    "# np.save('/media/axel/ssd/ml-challenge/encodings_test', encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodingsTest = np.load('/media/axel/ssd/ml-challenge/encodings_test.npy')\n",
    "# encodingsTest = np.expand_dims(encodingsTest, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_chunks = np.array([])\n",
    "\n",
    "num_models = 50\n",
    "for chunk in chunks(word_seq_test, int(50000/num_models)):\n",
    "    y_probas = np.stack([model.predict(chunk) for sample in range(num_models)])\n",
    "    y_proba = y_probas.mean(axis=0)\n",
    "    y_proba = np.argmax(y_proba, axis=-1)\n",
    "    y_preds_chunks = np.append(y_preds_chunks, y_proba)\n",
    "    del y_probas\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_preds.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del y_preds\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(encodingsTest)\n",
    "y_pred = model.predict(word_seq_test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = encoderCat.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['category'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('../../sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
