{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As discussed in the discussion section, we have leak in the competition if we use external data.\n",
    "# D[](http://)iscussed here: https://www.kaggle.com/c/ga-customer-revenue-prediction/discussion/68235#401950\n",
    " \n",
    "#  I have downloaded the data and used it to climb to the 2nd position. Before using it I was at the 600th position. \n",
    "#  Most of the code used it from olivier's (https://www.kaggle.com/ogrellier) notebooks in this competition and I haven't done any feature engineering with the external data yet.\n",
    " \n",
    "#  You can use the data that I downloaded as it would be in the data of this kernel. \n",
    "#  There are 4 files 2 for train and 2 for test.\n",
    "#Can't upload the notebook as kaggle kernels keep on crashing but this script will get you same score as mine.\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 1000, \"display.max_columns\", 1000)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#using https://www.kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields/notebook\n",
    "def load_df(csv_path='./train.csv', nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting VisitId to Join with our train, test data\n",
    "def get_visitid(id):\n",
    "    bef_, af_ = str(x).split('.')\n",
    "    return int(bef_), (int(af_)*10 if len(af_)==1 else int(af_))\n",
    "\n",
    "def get_folds(df=None, n_splits=5):\n",
    "    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n",
    "    # Get sorted unique visitors\n",
    "    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n",
    "\n",
    "    # Get folds\n",
    "    folds = GroupKFold(n_splits=n_splits)\n",
    "    fold_ids = []\n",
    "    ids = np.arange(df.shape[0])\n",
    "    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n",
    "        fold_ids.append(\n",
    "            [\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return fold_ids\n",
    "\n",
    "def process_device(data_df):\n",
    "    data_df['source.country'] = data_df['source'] + '_' + data_df['country']\n",
    "    #data_df['campaign.medium'] = data_df['campaign'] + '_' + data_df['medium']\n",
    "    data_df['browser.category'] = data_df['browser'] + '_' + data_df['deviceCategory']\n",
    "    data_df['browser.os'] = data_df['browser'] + '_' + data_df['operatingSystem']\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "\n",
    "def custom(data):\n",
    "    data['device_deviceCategory_channelGrouping'] = data['deviceCategory'] + \"_\" + data['channelGrouping']\n",
    "    data['channelGrouping_browser'] = data['browser'] + \"_\" + data['channelGrouping']\n",
    "    data['channelGrouping_OS'] = data['operatingSystem'] + \"_\" + data['channelGrouping']\n",
    "    \n",
    "    for i in ['city', 'continent', 'country','metro', 'networkDomain', 'region','subContinent']:\n",
    "        for j in ['browser','deviceCategory', 'operatingSystem', 'source']:\n",
    "            data[i + \"_\" + j] = data[i] + \"_\" + data[j]\n",
    "    \n",
    "    data['content.source'] = data['adContent'] + \"_\" + data['country']\n",
    "    data['medium.source'] = data['medium'] + \"_\" + data['country']\n",
    "    return data\n",
    "\n",
    "def browser_mapping(x):\n",
    "    browsers = ['chrome','safari','firefox','internet explorer','edge','opera','coc coc','maxthon','iron']\n",
    "    if x in browsers:\n",
    "        return x.lower()\n",
    "    elif  ('android' in x) or ('samsung' in x) or ('mini' in x) or ('iphone' in x) or ('in-app' in x) or ('playstation' in x):\n",
    "        return 'mobile browser'\n",
    "    elif  ('mozilla' in x) or ('chrome' in x) or ('blackberry' in x) or ('nokia' in x) or ('browser' in x) or ('amazon' in x):\n",
    "        return 'mobile browser'\n",
    "    elif  ('lunascape' in x) or ('netscape' in x) or ('blackberry' in x) or ('konqueror' in x) or ('puffin' in x) or ('amazon' in x):\n",
    "        return 'mobile browser'\n",
    "    elif '(not set)' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return 'others'\n",
    "    \n",
    "def adcontents_mapping(x):\n",
    "    if  ('google' in x):\n",
    "        return 'google'\n",
    "    elif  ('placement' in x) | ('placememnt' in x):\n",
    "        return 'placement'\n",
    "    elif '(not set)' in x or 'nan' in x:\n",
    "        return x\n",
    "    elif 'ad' in x:\n",
    "        return 'ad'\n",
    "    else:\n",
    "        return 'others'\n",
    "    \n",
    "def source_mapping(x):\n",
    "    if  ('google' in x):\n",
    "        return 'google'\n",
    "    elif  ('youtube' in x):\n",
    "        return 'youtube'\n",
    "    elif '(not set)' in x or 'nan' in x:\n",
    "        return x\n",
    "    elif 'yahoo' in x:\n",
    "        return 'yahoo'\n",
    "    elif 'facebook' in x:\n",
    "        return 'facebook'\n",
    "    elif 'reddit' in x:\n",
    "        return 'reddit'\n",
    "    elif 'bing' in x:\n",
    "        return 'bing'\n",
    "    elif 'quora' in x:\n",
    "        return 'quora'\n",
    "    elif 'outlook' in x:\n",
    "        return 'outlook'\n",
    "    elif 'linkedin' in x:\n",
    "        return 'linkedin'\n",
    "    elif 'pinterest' in x:\n",
    "        return 'pinterest'\n",
    "    elif 'ask' in x:\n",
    "        return 'ask'\n",
    "    elif 'siliconvalley' in x:\n",
    "        return 'siliconvalley'\n",
    "    elif 'lunametrics' in x:\n",
    "        return 'lunametrics'\n",
    "    elif 'amazon' in x:\n",
    "        return 'amazon'\n",
    "    elif 'mysearch' in x:\n",
    "        return 'mysearch'\n",
    "    elif 'qiita' in x:\n",
    "        return 'qiita'\n",
    "    elif 'messenger' in x:\n",
    "        return 'messenger'\n",
    "    elif 'twitter' in x:\n",
    "        return 'twitter'\n",
    "    elif 't.co' in x:\n",
    "        return 't.co'\n",
    "    elif 'vk.com' in x:\n",
    "        return 'vk.com'\n",
    "    elif 'search' in x:\n",
    "        return 'search'\n",
    "    elif 'edu' in x:\n",
    "        return 'edu'\n",
    "    elif 'mail' in x:\n",
    "        return 'mail'\n",
    "    elif 'ad' in x:\n",
    "        return 'ad'\n",
    "    elif 'golang' in x:\n",
    "        return 'golang'\n",
    "    elif 'direct' in x:\n",
    "        return 'direct'\n",
    "    elif 'dealspotr' in x:\n",
    "        return 'dealspotr'\n",
    "    elif 'sashihara' in x:\n",
    "        return 'sashihara'\n",
    "    elif 'phandroid' in x:\n",
    "        return 'phandroid'\n",
    "    elif 'baidu' in x:\n",
    "        return 'baidu'\n",
    "    elif 'mdn' in x:\n",
    "        return 'mdn'\n",
    "    elif 'duckduckgo' in x:\n",
    "        return 'duckduckgo'\n",
    "    elif 'seroundtable' in x:\n",
    "        return 'seroundtable'\n",
    "    elif 'metrics' in x:\n",
    "        return 'metrics'\n",
    "    elif 'sogou' in x:\n",
    "        return 'sogou'\n",
    "    elif 'businessinsider' in x:\n",
    "        return 'businessinsider'\n",
    "    elif 'github' in x:\n",
    "        return 'github'\n",
    "    elif 'gophergala' in x:\n",
    "        return 'gophergala'\n",
    "    elif 'yandex' in x:\n",
    "        return 'yandex'\n",
    "    elif 'msn' in x:\n",
    "        return 'msn'\n",
    "    elif 'dfa' in x:\n",
    "        return 'dfa'\n",
    "    elif '(not set)' in x:\n",
    "        return '(not set)'\n",
    "    elif 'feedly' in x:\n",
    "        return 'feedly'\n",
    "    elif 'arstechnica' in x:\n",
    "        return 'arstechnica'\n",
    "    elif 'squishable' in x:\n",
    "        return 'squishable'\n",
    "    elif 'flipboard' in x:\n",
    "        return 'flipboard'\n",
    "    elif 't-online.de' in x:\n",
    "        return 't-online.de'\n",
    "    elif 'sm.cn' in x:\n",
    "        return 'sm.cn'\n",
    "    elif 'wow' in x:\n",
    "        return 'wow'\n",
    "    elif 'baidu' in x:\n",
    "        return 'baidu'\n",
    "    elif 'partners' in x:\n",
    "        return 'partners'\n",
    "    else:\n",
    "        return 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:120: RuntimeWarning: divide by zero encountered in log1p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['channelGrouping', 'visitNumber', 'browser', 'deviceCategory', 'isMobile', 'operatingSystem', 'city', 'continent', 'country', 'metro', 'networkDomain', 'region', 'subContinent', 'hits', 'pageviews', 'adContent', 'isTrueDirect', 'keyword', 'medium', 'referralPath', 'source', 'Sessions', 'Avg. Session Duration', 'Bounce Rate', 'Revenue', 'Transactions', 'Goal Conversion Rate', 'sess_date_dow', 'sess_date_hours', 'sess_month', 'hits/pageviews', 'is_high_hits', 'fullVisitorId_hits_mean']\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.5793\n",
      "[200]\tvalid_0's rmse: 1.55979\n",
      "[300]\tvalid_0's rmse: 1.55439\n",
      "[400]\tvalid_0's rmse: 1.55081\n",
      "[500]\tvalid_0's rmse: 1.54908\n",
      "[600]\tvalid_0's rmse: 1.54802\n",
      "[700]\tvalid_0's rmse: 1.54782\n",
      "[800]\tvalid_0's rmse: 1.54852\n",
      "Early stopping, best iteration is:\n",
      "[733]\tvalid_0's rmse: 1.54768\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.59148\n",
      "[200]\tvalid_0's rmse: 1.57931\n",
      "[300]\tvalid_0's rmse: 1.57604\n",
      "[400]\tvalid_0's rmse: 1.5737\n",
      "[500]\tvalid_0's rmse: 1.57206\n",
      "[600]\tvalid_0's rmse: 1.57181\n",
      "Early stopping, best iteration is:\n",
      "[556]\tvalid_0's rmse: 1.57148\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.58577\n",
      "[200]\tvalid_0's rmse: 1.57149\n",
      "[300]\tvalid_0's rmse: 1.5672\n",
      "[400]\tvalid_0's rmse: 1.56367\n",
      "[500]\tvalid_0's rmse: 1.56146\n",
      "[600]\tvalid_0's rmse: 1.5603\n",
      "[700]\tvalid_0's rmse: 1.55962\n",
      "[800]\tvalid_0's rmse: 1.55925\n",
      "[900]\tvalid_0's rmse: 1.55868\n",
      "[1000]\tvalid_0's rmse: 1.55803\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 1.55803\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.54095\n",
      "[200]\tvalid_0's rmse: 1.52749\n",
      "[300]\tvalid_0's rmse: 1.5234\n",
      "[400]\tvalid_0's rmse: 1.52062\n",
      "[500]\tvalid_0's rmse: 1.52044\n",
      "Early stopping, best iteration is:\n",
      "[459]\tvalid_0's rmse: 1.51992\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.6363\n",
      "[200]\tvalid_0's rmse: 1.62194\n",
      "[300]\tvalid_0's rmse: 1.61693\n",
      "[400]\tvalid_0's rmse: 1.614\n",
      "[500]\tvalid_0's rmse: 1.61373\n",
      "[600]\tvalid_0's rmse: 1.61324\n",
      "[700]\tvalid_0's rmse: 1.61332\n",
      "Early stopping, best iteration is:\n",
      "[614]\tvalid_0's rmse: 1.6129\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.59414\n",
      "[200]\tvalid_0's rmse: 1.57794\n",
      "[300]\tvalid_0's rmse: 1.57349\n",
      "[400]\tvalid_0's rmse: 1.57104\n",
      "[500]\tvalid_0's rmse: 1.56956\n",
      "[600]\tvalid_0's rmse: 1.56765\n",
      "[700]\tvalid_0's rmse: 1.56641\n",
      "[800]\tvalid_0's rmse: 1.56605\n",
      "[900]\tvalid_0's rmse: 1.56624\n",
      "Early stopping, best iteration is:\n",
      "[862]\tvalid_0's rmse: 1.5657\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.56181\n",
      "[200]\tvalid_0's rmse: 1.54985\n",
      "[300]\tvalid_0's rmse: 1.54755\n",
      "[400]\tvalid_0's rmse: 1.54721\n",
      "[500]\tvalid_0's rmse: 1.54577\n",
      "[600]\tvalid_0's rmse: 1.54497\n",
      "[700]\tvalid_0's rmse: 1.54459\n",
      "[800]\tvalid_0's rmse: 1.54423\n",
      "Early stopping, best iteration is:\n",
      "[770]\tvalid_0's rmse: 1.54388\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.58682\n",
      "[200]\tvalid_0's rmse: 1.57436\n",
      "[300]\tvalid_0's rmse: 1.57105\n",
      "[400]\tvalid_0's rmse: 1.56771\n",
      "[500]\tvalid_0's rmse: 1.56662\n",
      "[600]\tvalid_0's rmse: 1.56575\n",
      "[700]\tvalid_0's rmse: 1.56578\n",
      "Early stopping, best iteration is:\n",
      "[633]\tvalid_0's rmse: 1.56532\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.59559\n",
      "[200]\tvalid_0's rmse: 1.57991\n",
      "[300]\tvalid_0's rmse: 1.57605\n",
      "[400]\tvalid_0's rmse: 1.57351\n",
      "[500]\tvalid_0's rmse: 1.57282\n",
      "[600]\tvalid_0's rmse: 1.57233\n",
      "Early stopping, best iteration is:\n",
      "[538]\tvalid_0's rmse: 1.57223\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.5793\n",
      "[200]\tvalid_0's rmse: 1.56855\n",
      "[300]\tvalid_0's rmse: 1.56395\n",
      "[400]\tvalid_0's rmse: 1.5613\n",
      "[500]\tvalid_0's rmse: 1.56031\n",
      "[600]\tvalid_0's rmse: 1.56009\n",
      "Early stopping, best iteration is:\n",
      "[548]\tvalid_0's rmse: 1.55968\n",
      "1.5614995604800894\n"
     ]
    }
   ],
   "source": [
    "PATH = '/media/axel/ssd/google-analytics/'\n",
    "train = pd.read_feather(PATH + 'train')\n",
    "test = pd.read_feather(PATH + 'test')\n",
    "\n",
    "# train = load_df()\n",
    "# test = load_df(PATH + \"test.csv\")\n",
    "\n",
    "#Loading external data\n",
    "train_store_1 = pd.read_csv('./Train_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "train_store_2 = pd.read_csv('./Train_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "test_store_1 = pd.read_csv('./Test_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "test_store_2 = pd.read_csv('./Test_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "\n",
    "\n",
    "for df in [train_store_1, train_store_2, test_store_1, test_store_2]:\n",
    "    df[\"visitId\"] = df[\"Client Id\"].apply(lambda x: x.split('.', 1)[1]).astype(str)\n",
    "\n",
    "train_exdata = pd.concat([train_store_1, train_store_2], sort=False)\n",
    "test_exdata = pd.concat([test_store_1, test_store_2], sort=False)\n",
    "\n",
    "for df in [train, test]:\n",
    "    df[\"visitId\"] = df[\"visitId\"].astype(str)\n",
    "\n",
    "# Merge with train/test data\n",
    "train_new = train.merge(train_exdata, how=\"left\", on=\"visitId\")\n",
    "test_new = test.merge(test_exdata, how=\"left\", on=\"visitId\")\n",
    "\n",
    "# Drop Client Id\n",
    "for df in [train_new, test_new]:\n",
    "    df.drop(\"Client Id\", 1, inplace=True)\n",
    "\n",
    "#Cleaning Revenue\n",
    "for df in [train_new, test_new]:\n",
    "    df[\"Revenue\"].fillna('$', inplace=True)\n",
    "    df[\"Revenue\"] = df[\"Revenue\"].apply(lambda x: x.replace('$', '').replace(',', ''))\n",
    "    df[\"Revenue\"] = pd.to_numeric(df[\"Revenue\"], errors=\"coerce\")\n",
    "    df[\"Revenue\"].fillna(0.0, inplace=True)\n",
    "\n",
    "#Imputing NaN\n",
    "for df in [train_new, test_new]:\n",
    "    df[\"Sessions\"] = df[\"Sessions\"].fillna(0)\n",
    "    df[\"Avg. Session Duration\"] = df[\"Avg. Session Duration\"].fillna(0)\n",
    "    df[\"Bounce Rate\"] = df[\"Bounce Rate\"].fillna(0)\n",
    "    df[\"Revenue\"] = df[\"Revenue\"].fillna(0)\n",
    "    df[\"Transactions\"] = df[\"Transactions\"].fillna(0)\n",
    "    df[\"Goal Conversion Rate\"] = df[\"Goal Conversion Rate\"].fillna(0)\n",
    "    df['adContent'].fillna('N/A', inplace=True)\n",
    "    #df['slot'].fillna('N/A', inplace=True)\n",
    "    #df['page'].fillna(0.0, inplace=True)\n",
    "    #df['isVideoAd'].fillna('N/A', inplace=True)\n",
    "    #df['adNetworkType'].fillna('N/A', inplace=True)\n",
    "    #df['gclId'].fillna('N/A', inplace=True)\n",
    "    df['isTrueDirect'].fillna('N/A', inplace=True)\n",
    "    df['referralPath'].fillna('N/A', inplace=True)\n",
    "    df['keyword'].fillna('N/A', inplace=True)\n",
    "    #df['bounces'].fillna(0.0, inplace=True)\n",
    "    #df['newVisits'].fillna(0.0, inplace=True)\n",
    "    df['pageviews'].fillna(0.0, inplace=True)\n",
    "\n",
    "del train\n",
    "del test\n",
    "train = train_new\n",
    "test = test_new\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['date_new'] = pd.to_datetime(df['visitStartTime'], unit='s')\n",
    "    df['date'] = df['date_new'].dt.date\n",
    "    df['sess_date_dow'] = df['date_new'].dt.dayofweek\n",
    "    df['sess_date_hours'] = df['date_new'].dt.hour\n",
    "    df['sess_month'] = df['date_new'].dt.month\n",
    "\n",
    "# Dropping Constant Columns\n",
    "\n",
    "const_cols = [col for col in train.columns if len(train[col].unique())==1]\n",
    "\n",
    "for df in [train, test]:\n",
    "    df.drop(const_cols, 1, inplace=True)\n",
    "\n",
    "train.drop('campaignCode', 1, inplace=True)\n",
    "\n",
    "\n",
    "y_reg = train['transactionRevenue'].fillna(0)\n",
    "del train['transactionRevenue']\n",
    "\n",
    "if 'transactionRevenue' in test.columns:\n",
    "    del test['transactionRevenue']\n",
    "\n",
    "excluded_features = [\n",
    "    'date', 'date_new', 'fullVisitorId', 'sessionId', 'transactionRevenue', \n",
    "    'visitId', 'visitStartTime'\n",
    "]\n",
    "\n",
    "\n",
    "# train['browser'] = train['browser'].map(lambda x:browser_mapping(str(x).lower())).astype('str')\n",
    "# train['adContent'] = train['adContent'].map(lambda x:adcontents_mapping(str(x).lower())).astype('str')\n",
    "# train['source'] = train['source'].map(lambda x:source_mapping(str(x).lower())).astype('str')\n",
    "# test['browser'] = test['browser'].map(lambda x:browser_mapping(str(x).lower())).astype('str')\n",
    "# test['adContent'] = test['adContent'].map(lambda x:adcontents_mapping(str(x).lower())).astype('str')\n",
    "# test['source'] = test['source'].map(lambda x:source_mapping(str(x).lower())).astype('str')\n",
    "\n",
    "# train = process_device(train)\n",
    "# test = process_device(test)\n",
    "# train = custom(train)\n",
    "# test = custom(test)\n",
    "\n",
    "\n",
    "categorical_features = [\n",
    "    _f for _f in train.columns\n",
    "    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n",
    "]\n",
    "\n",
    "for f in categorical_features:\n",
    "    train[f], indexer = pd.factorize(train[f])\n",
    "    test[f] = indexer.get_indexer(test[f])\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['hits/pageviews'] = (df[\"pageviews\"]/(df[\"hits\"])).apply(lambda x: 0 if np.isinf(x) else x)\n",
    "    df['is_high_hits'] = np.logical_or(df[\"hits\"]>4,df[\"pageviews\"]>4).astype(np.int32)\n",
    "    df[\"Revenue\"] = np.log1p(df[\"Revenue\"])\n",
    "    df['hits'] = np.log1p(df['hits'])\n",
    "#     df['next_session_2'] = (\n",
    "#         df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(-1)\n",
    "#     ).astype(np.int64) // 1e9 // 60 // 60\n",
    "    \n",
    "\n",
    "# train['fullVisitorId_hits_mean'] = train.groupby('fullVisitorId')['Revenue'].transform('mean')\n",
    "# test['fullVisitorId_hits_mean'] = test.groupby('fullVisitorId')['Revenue'].transform('mean')\n",
    "\n",
    "\n",
    "num_folds = 10\n",
    "folds = get_folds(df=train, n_splits=num_folds)\n",
    "y_reg = y_reg.astype(float)\n",
    "train_features = [_f for _f in train.columns if _f not in excluded_features]\n",
    "print(train_features)\n",
    "\n",
    "importances = pd.DataFrame()\n",
    "oof_reg_preds = np.zeros(train.shape[0])\n",
    "sub_reg_preds = np.zeros(test.shape[0])\n",
    "for fold_, (trn_, val_) in enumerate(folds):\n",
    "    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_]\n",
    "    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_]\n",
    "        \n",
    "    params = {}\n",
    "    params[\"device\"] = \"gpu\"\n",
    "    \n",
    "    reg = lgb.LGBMRegressor(**params,\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=1000,\n",
    "        subsample=.9,\n",
    "        colsample_bytree=.9,\n",
    "        random_state=1\n",
    "    )\n",
    "    reg.fit(\n",
    "        trn_x, np.log1p(trn_y),\n",
    "        eval_set=[(val_x, np.log1p(val_y))],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=100,\n",
    "        eval_metric='rmse'\n",
    "    )\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = train_features\n",
    "    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n",
    "    \n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "    \n",
    "    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n",
    "    oof_reg_preds[oof_reg_preds < 0] = 0\n",
    "    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n",
    "    _preds[_preds < 0] = 0\n",
    "    sub_reg_preds += np.expm1(_preds) / len(folds)\n",
    "    \n",
    "print(mean_squared_error(np.log1p(y_reg), oof_reg_preds) ** .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in log1p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714167, 316)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in log1p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617242, 316)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.45006\tVALID's rmse: 1.46301\n",
      "[200]\tTRAIN's rmse: 1.41547\tVALID's rmse: 1.45826\n",
      "Early stopping, best iteration is:\n",
      "[174]\tTRAIN's rmse: 1.42154\tVALID's rmse: 1.4579\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.444\tVALID's rmse: 1.507\n",
      "[200]\tTRAIN's rmse: 1.40897\tVALID's rmse: 1.50285\n",
      "[300]\tTRAIN's rmse: 1.38828\tVALID's rmse: 1.50297\n",
      "Early stopping, best iteration is:\n",
      "[220]\tTRAIN's rmse: 1.40423\tVALID's rmse: 1.50253\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.44511\tVALID's rmse: 1.495\n",
      "[200]\tTRAIN's rmse: 1.40964\tVALID's rmse: 1.49223\n",
      "Early stopping, best iteration is:\n",
      "[164]\tTRAIN's rmse: 1.41922\tVALID's rmse: 1.49199\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.44916\tVALID's rmse: 1.46352\n",
      "[200]\tTRAIN's rmse: 1.41409\tVALID's rmse: 1.45963\n",
      "[300]\tTRAIN's rmse: 1.39375\tVALID's rmse: 1.46018\n",
      "Early stopping, best iteration is:\n",
      "[248]\tTRAIN's rmse: 1.40415\tVALID's rmse: 1.45934\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.44582\tVALID's rmse: 1.49888\n",
      "[200]\tTRAIN's rmse: 1.4099\tVALID's rmse: 1.49186\n",
      "[300]\tTRAIN's rmse: 1.38871\tVALID's rmse: 1.49115\n",
      "Early stopping, best iteration is:\n",
      "[298]\tTRAIN's rmse: 1.38922\tVALID's rmse: 1.49097\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.44726\tVALID's rmse: 1.4782\n",
      "[200]\tTRAIN's rmse: 1.4124\tVALID's rmse: 1.4724\n",
      "Early stopping, best iteration is:\n",
      "[188]\tTRAIN's rmse: 1.41529\tVALID's rmse: 1.47211\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.4472\tVALID's rmse: 1.48293\n",
      "[200]\tTRAIN's rmse: 1.41364\tVALID's rmse: 1.48183\n",
      "Early stopping, best iteration is:\n",
      "[153]\tTRAIN's rmse: 1.42562\tVALID's rmse: 1.48149\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.44527\tVALID's rmse: 1.50212\n",
      "[200]\tTRAIN's rmse: 1.41049\tVALID's rmse: 1.50041\n",
      "Early stopping, best iteration is:\n",
      "[155]\tTRAIN's rmse: 1.4224\tVALID's rmse: 1.49928\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.4468\tVALID's rmse: 1.48216\n",
      "[200]\tTRAIN's rmse: 1.41178\tVALID's rmse: 1.47806\n",
      "[300]\tTRAIN's rmse: 1.38936\tVALID's rmse: 1.4788\n",
      "Early stopping, best iteration is:\n",
      "[263]\tTRAIN's rmse: 1.39731\tVALID's rmse: 1.4779\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.44558\tVALID's rmse: 1.49999\n",
      "[200]\tTRAIN's rmse: 1.41041\tVALID's rmse: 1.49906\n",
      "Early stopping, best iteration is:\n",
      "[169]\tTRAIN's rmse: 1.41834\tVALID's rmse: 1.49868\n",
      "1.4832816838421563\n"
     ]
    }
   ],
   "source": [
    "train['predictions'] = np.expm1(oof_reg_preds)\n",
    "test['predictions'] = sub_reg_preds\n",
    "\n",
    "# Aggregate data at User level\n",
    "trn_data = train[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n",
    "\n",
    "# Create a list of predictions for each Visitor\n",
    "trn_pred_list = train[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n",
    "    .apply(lambda df: list(df.predictions))\\\n",
    "    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\n",
    "\n",
    "# Create a DataFrame with VisitorId as index\n",
    "# trn_pred_list contains dict \n",
    "# so creating a dataframe from it will expand dict values into columns\n",
    "trn_all_predictions = pd.DataFrame(list(trn_pred_list.values), index=trn_data.index)\n",
    "trn_feats = trn_all_predictions.columns\n",
    "trn_all_predictions['t_mean'] = np.log1p(trn_all_predictions[trn_feats].mean(axis=1))\n",
    "trn_all_predictions['t_median'] = np.log1p(trn_all_predictions[trn_feats].median(axis=1))\n",
    "trn_all_predictions['t_sum_log'] = np.log1p(trn_all_predictions[trn_feats]).sum(axis=1)\n",
    "trn_all_predictions['t_sum_act'] = np.log1p(trn_all_predictions[trn_feats].fillna(0).sum(axis=1))\n",
    "trn_all_predictions['t_nb_sess'] = trn_all_predictions[trn_feats].isnull().sum(axis=1)\n",
    "full_data = pd.concat([trn_data, trn_all_predictions], axis=1)\n",
    "del trn_data, trn_all_predictions\n",
    "gc.collect()\n",
    "print(full_data.shape)\n",
    "\n",
    "\n",
    "sub_pred_list = test[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n",
    "    .apply(lambda df: list(df.predictions))\\\n",
    "    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\n",
    "\n",
    "sub_data = test[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n",
    "sub_all_predictions = pd.DataFrame(list(sub_pred_list.values), index=sub_data.index)\n",
    "for f in trn_feats:\n",
    "    if f not in sub_all_predictions.columns:\n",
    "        sub_all_predictions[f] = np.nan\n",
    "sub_all_predictions['t_mean'] = np.log1p(sub_all_predictions[trn_feats].mean(axis=1))\n",
    "sub_all_predictions['t_median'] = np.log1p(sub_all_predictions[trn_feats].median(axis=1))\n",
    "sub_all_predictions['t_sum_log'] = np.log1p(sub_all_predictions[trn_feats]).sum(axis=1)\n",
    "sub_all_predictions['t_sum_act'] = np.log1p(sub_all_predictions[trn_feats].fillna(0).sum(axis=1))\n",
    "sub_all_predictions['t_nb_sess'] = sub_all_predictions[trn_feats].isnull().sum(axis=1)\n",
    "sub_full_data = pd.concat([sub_data, sub_all_predictions], axis=1)\n",
    "del sub_data, sub_all_predictions\n",
    "gc.collect()\n",
    "print(sub_full_data.shape)\n",
    "\n",
    "# Create target at Visitor level\n",
    "train['target'] = y_reg\n",
    "trn_user_target = train[['fullVisitorId', 'target']].groupby('fullVisitorId').sum()\n",
    "\n",
    "# Train a model at Visitor level\n",
    "folds = get_folds(df=full_data[['pageviews']].reset_index(), n_splits=num_folds)\n",
    "\n",
    "oof_preds = np.zeros(full_data.shape[0])\n",
    "sub_preds = np.zeros(sub_full_data.shape[0])\n",
    "vis_importances = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_, val_) in enumerate(folds):\n",
    "    trn_x, trn_y = full_data.iloc[trn_], trn_user_target['target'].iloc[trn_]\n",
    "    val_x, val_y = full_data.iloc[val_], trn_user_target['target'].iloc[val_]\n",
    "        \n",
    "    params = {}\n",
    "    params[\"device\"] = \"gpu\"\n",
    "    \n",
    "    reg = lgb.LGBMRegressor(**params,\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=1000,\n",
    "        subsample=.9,\n",
    "        colsample_bytree=.9,\n",
    "        random_state=1\n",
    "    )\n",
    "    reg.fit(\n",
    "        trn_x, np.log1p(trn_y),\n",
    "        eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n",
    "        eval_names=['TRAIN', 'VALID'],\n",
    "        early_stopping_rounds=100,\n",
    "        eval_metric='rmse',\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = trn_x.columns\n",
    "    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n",
    "    \n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    vis_importances = pd.concat([vis_importances, imp_df], axis=0, sort=False)\n",
    "    \n",
    "    oof_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n",
    "    oof_preds[oof_preds < 0] = 0\n",
    "    \n",
    "    # Make sure features are in the same order\n",
    "    _preds = reg.predict(sub_full_data[full_data.columns], num_iteration=reg.best_iteration_)\n",
    "    _preds[_preds < 0] = 0\n",
    "    sub_preds += _preds / len(folds)\n",
    "    \n",
    "print(mean_squared_error(np.log1p(trn_user_target['target']), oof_preds) ** .5)\n",
    "\n",
    "# # Display feature importances\n",
    "# vis_importances['gain_log'] = np.log1p(vis_importances['gain'])\n",
    "# mean_gain = vis_importances[['gain', 'feature']].groupby('feature').mean()\n",
    "# vis_importances['mean_gain'] = vis_importances['feature'].map(mean_gain['gain'])\n",
    "\n",
    "# plt.figure(figsize=(8, 25))\n",
    "# sns.barplot(x='gain_log', y='feature', data=vis_importances.sort_values('mean_gain', ascending=False).iloc[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_full_data['PredictedLogRevenue'] = sub_preds\n",
    "sub_full_data[['PredictedLogRevenue']].to_csv('../../submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
