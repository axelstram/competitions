{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "\n",
    "from pandas_summary import DataFrameSummary\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from collections import OrderedDict, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from skopt import gp_minimize\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_mean_deal_prob(dic):\n",
    "    probabilities = list(dic.values())\n",
    "    probs = []\n",
    "    for p in probabilities:\n",
    "        probs.append(p['deal_probability'])\n",
    "        \n",
    "    return sum(probs) / float(len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_feature_importance(m, cols):\n",
    "    headers = [\"name\", \"score\"]\n",
    "    values = sorted(zip(cols, m.feature_importances_), key=lambda x: x[1] * -1)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predictions(df, dic, mean):\n",
    "    y_pred = []\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        category_name = row.category_name\n",
    "\n",
    "        if category_name in dic:\n",
    "            y_pred.append(dic[category_name]['deal_probability'])\n",
    "        else:\n",
    "            y_pred.append(mean)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_target_by_fields(df, target, cols = None):\n",
    "    if cols is None:\n",
    "        cols = df.columns\n",
    "    \n",
    "    for c in cols:\n",
    "        gp = df.groupby(c)[target]\n",
    "        mean = gp.mean()\n",
    "        std  = gp.std()\n",
    "        \n",
    "        if True not in np.isnan(mean):\n",
    "            df[c + '_' + target + '_avg'] = df[c].map(mean)\n",
    "        \n",
    "        if True not in np.isnan(std):\n",
    "            df[c + '_' + target + '_std'] = df[c].map(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_to_df(df, new_field, dic):\n",
    "    mean = calculate_mean_deal_prob(dic)\n",
    "    new_col = np.zeros((len(df),1))\n",
    "    field = new_field\n",
    "    \n",
    "    if field.endswith('_mean'):\n",
    "        field = field[:-5]\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "\n",
    "        if row[field] in dic:\n",
    "            new_col[i] = dic[row[field]]['deal_probability']\n",
    "        else:\n",
    "            new_col[i] = mean\n",
    "        \n",
    "        i = i + 1    \n",
    "        \n",
    "    df[new_field] = new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_columns_to_categorical(df):\n",
    "    train_cats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_na_columns(df):\n",
    "    for c in df.columns:\n",
    "        if c.endswith('_na'):\n",
    "            df.drop(labels=[c], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_columns_with_nans(df):\n",
    "    for c in df.columns:\n",
    "        if df[c].isnull().values.any():\n",
    "            df.drop(labels=[c], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    n_estimators = int(space['n_estimators'])\n",
    "    max_depth = int(space['max_depth'])\n",
    "    min_child_weight = space['min_child_weight']\n",
    "    subsample = space['subsample']\n",
    "    reg_alpha = space['reg_alpha']\n",
    "    reg_lambda = space['reg_lambda']\n",
    "    colsample_bylevel = space['colsample_bylevel']\n",
    "    colsample_bytree = space['colsample_bytree']\n",
    "    learning_rate = space['learning_rate']\n",
    "    gamma = space['gamma']\n",
    "    #scale_pos_weight = space['scale_pos_weight']\n",
    "    \n",
    "    print(\"params: \")\n",
    "    print(\"n_estimators: \" + str(n_estimators))\n",
    "    print(\"max_depth: \" + str(max_depth))\n",
    "    print(\"min_child_weight: \" + str(min_child_weight))\n",
    "    print(\"subsample: \" + str(subsample))\n",
    "    print(\"reg_alpha: \" + str(reg_alpha))\n",
    "    print(\"reg_lambda: \" + str(reg_lambda))\n",
    "    print(\"colsample_bylevel: \" + str(colsample_bylevel))\n",
    "    print(\"colsample_bytree: \" + str(colsample_bytree))\n",
    "    print(\"learning rate: \" + str(learning_rate))\n",
    "    print(\"gamma: \" + str(gamma))\n",
    "    #print(\"scale_pos_weight: \" + str(scale_pos_weight))\n",
    "        \n",
    "    clf = xgb.XGBRegressor(n_jobs=4,\n",
    "                        #scale_pos_weight = scale_pos_weight,\n",
    "                        n_estimators = n_estimators, \n",
    "                        learning_rate = learning_rate,\n",
    "                        max_depth = max_depth,\n",
    "                        min_child_weight = min_child_weight,\n",
    "                        subsample = subsample,\n",
    "                        reg_alpha = reg_alpha,\n",
    "                        reg_lambda = reg_lambda,\n",
    "                        colsample_bylevel = colsample_bylevel,\n",
    "                        colsample_bytree = colsample_bytree,\n",
    "                        gamma=gamma)\n",
    "    \n",
    "    eval_set  = [(X_train, y_train), (X_valid, y_valid)]\n",
    "\n",
    "    clf.fit(X_train, y_train,\n",
    "            eval_set=eval_set, eval_metric=\"rmse\", early_stopping_rounds=10)\n",
    "  \n",
    "    y_pred = clf.predict(X_valid)\n",
    "        \n",
    "    print(\"rmse:\" + str(rmse(y_valid, y_pred)))\n",
    "\n",
    "    return{'loss': rmse(y_valid, y_pred), 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective_lgb(space):\n",
    "   \n",
    "    num_leaves = int(space['num_leaves'])\n",
    "    max_depth = int(space['max_depth'])\n",
    "    subsample = space['subsample']\n",
    "    subsample_freq = int(space['subsample_freq'])\n",
    "    lambda_l1 = space['lambda_l1']\n",
    "    lambda_l2 = space['lambda_l2']\n",
    "    learning_rate = space['learning_rate']\n",
    "    feature_fraction = space['feature_fraction']\n",
    "    min_data_per_leaf = int(space['min_data_per_leaf'])\n",
    "    min_child_weight = space['min_child_weight']\n",
    "    \n",
    "    print(\"params: \")\n",
    "    print(\"num_leaves: \" + str(num_leaves))\n",
    "    print(\"max_depth: \" + str(max_depth))\n",
    "    print(\"min_child_weight: \" + str(min_child_weight))\n",
    "    print(\"subsample: \" + str(subsample))\n",
    "    print(\"subsample_freq: \" + str(subsample_freq))\n",
    "    print(\"lambda_l1: \" + str(lambda_l1))\n",
    "    print(\"lambda_l2: \" + str(lambda_l2))\n",
    "    print(\"feature_fraction: \" + str(feature_fraction))\n",
    "    print(\"learning rate: \" + str(learning_rate))\n",
    "    print(\"min_data_per_leaf: \" + str(min_data_per_leaf))        \n",
    "    \n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"n_estimators\": 5000,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"num_leaves\" : num_leaves, \n",
    "        \"learning_rate\" : learning_rate,\n",
    "        \"subsample\" : subsample,\n",
    "        \"subsample_freq\" : subsample_freq,\n",
    "        \"lambda_l1\": lambda_l1,\n",
    "        \"lambda_l2\": lambda_l2,\n",
    "        \"feature_fraction\" : feature_fraction,\n",
    "        \"min_data_per_leaf\": min_data_per_leaf,\n",
    "        \"min_child_weight\": min_child_weight,\n",
    "        \"bagging_seed\" : 16,\n",
    "        \"device\" : \"gpu\",\n",
    "        \"max_bin\": 64\n",
    "    }\n",
    "\n",
    "    model = lgb.train(params, train, valid_sets=[train, val], early_stopping_rounds=100, verbose_eval=10)\n",
    "    \n",
    "    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    y_pred = np.clip(y_pred, 0, 1)\n",
    "      \n",
    "    print(\"rmse:\" + str(rmse(y_valid, y_pred)))\n",
    "\n",
    "    return{'loss': rmse(y_valid, y_pred), 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = \"/home/axel/Desktop/kaggle/avito/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{PATH}train/train.csv', parse_dates=['activation_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(f'{PATH}test/test.csv', parse_dates=['activation_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df, df_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['params'] = df_all.apply(lambda row: ' '.join([\n",
    "                        str(row['param_1']), \n",
    "                        str(row['param_2']), \n",
    "                        str(row['param_3'])]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['description'] = df_all['description'].fillna(' ')\n",
    "df_all['price'] = df_all['price'].fillna(df_all['price'].mean())\n",
    "df_all['image_top_1'] = df_all['image_top_1'].fillna(-999)\n",
    "df_all['params'] = df_all['params'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_features = [\"description\", \"params\", \"title\"]\n",
    "\n",
    "for col in text_features:\n",
    "    df_all[col] = df_all[col].str.lower()\n",
    "    df_all[col] = df_all[col].str.replace(\"[^[:alpha:]]\", \" \")\n",
    "    df_all[col] = df_all[col].str.replace(\"\\\\s+\", \" \")\n",
    "    df_all[col + '_num_chars'] = df_all[col].apply(len)\n",
    "    df_all[col + '_num_words'] = df_all[col].apply(lambda comment: len(comment.split()))\n",
    "    df_all[col + '_num_unique_words'] = df_all[col].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df_all[col + '_words_vs_unique'] = df_all[col+'_num_unique_words'] / df_all[col+'_num_words'] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['weekday'] = df_all['activation_date'].dt.weekday\n",
    "df_all[\"day_of_month\"] = df_all['activation_date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_group = ['region', 'city', 'parent_category_name', 'category_name',\n",
    "                 'image_top_1', 'user_type','item_seq_number','weekday', 'day_of_month', 'params']\n",
    "group_target_by_fields(df_all, target='deal_probability', cols=cols_to_group)\n",
    "group_target_by_fields(df_all, target='price', cols=cols_to_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cats(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=1000, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', '...гда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'],\n",
       "        strip_accents=None, sublinear_tf=True,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_text_features = 1000\n",
    "\n",
    "tfidf_title = TfidfVectorizer(max_features=num_text_features, stop_words = stopwords.words('russian'),\n",
    "                              ngram_range=(1,1), min_df=3, max_df=0.5, norm='l2', sublinear_tf=True)\n",
    "tfidf_title.fit(df_all['title'])\n",
    "\n",
    "tfidf_descr = TfidfVectorizer(max_features=num_text_features, stop_words = stopwords.words('russian'),\n",
    "                              ngram_range=(1,1), min_df=3, max_df=0.5, norm='l2', sublinear_tf=True)\n",
    "tfidf_descr.fit(df_all['description'])\n",
    "\n",
    "tfidf_params = TfidfVectorizer(max_features=num_text_features, stop_words = stopwords.words('russian'),\n",
    "                              ngram_range=(1,1), min_df=3, max_df=0.5, norm='l2', sublinear_tf=True)\n",
    "tfidf_params.fit(df_all['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='arpack', n_components=250, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_features = 200\n",
    "\n",
    "svd_title = TruncatedSVD(n_components=svd_features, algorithm='arpack')\n",
    "svd_title.fit(tfidf_title.transform(df_all['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='arpack', n_components=250, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_descr = TruncatedSVD(n_components=svd_features, algorithm='arpack')\n",
    "svd_descr.fit(tfidf_descr.transform(df_all['description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='arpack', n_components=250, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_params = TruncatedSVD(n_components=svd_features, algorithm='arpack')\n",
    "svd_params.fit(tfidf_params.transform(df_all['params']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_features = np.array(svd_title.transform(tfidf_title.transform(df_all['title'])), dtype=np.float16)\n",
    "\n",
    "for i in range(svd_features):\n",
    "    df_all['title_feature_' + str(i)] = title_features[:, i]\n",
    "\n",
    "del title_features\n",
    "gc.collect()\n",
    "\n",
    "description_features = np.array(svd_descr.transform(tfidf_descr.transform(df_all['description'])), dtype=np.float16)\n",
    "\n",
    "for i in range(svd_features):\n",
    "    df_all['description_feature_' + str(i)] = description_features[:, i]\n",
    "\n",
    "del description_features\n",
    "gc.collect()\n",
    "\n",
    "params_features = np.array(svd_params.transform(tfidf_params.transform(df_all['params'])), dtype=np.float16)\n",
    "\n",
    "for i in range(svd_features):\n",
    "    df_all['params_feature_' + str(i)] = params_features[:, i]\n",
    "\n",
    "del params_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64374811908647944"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_title.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60560088131368084"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_descr.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93922217868122249"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_params.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# msk = np.array([i for i in range(len(df_all))])\n",
    "# msk_train = (msk < 1390439)\n",
    "# msk_valid = (msk >= 1390439) & (msk < 1503424)\n",
    "# msk_test = (msk >= 1503424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train = df_all_sparse[msk_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_val = df_all_sparse[msk_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_test = df_all_sparse[msk_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.drop(labels=['param_1', 'param_2', 'param_3', 'title', 'description', 'params'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all[df_all.activation_date < '2017-03-28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_all[(df_all.activation_date >= '2017-03-28') & (df_all.activation_date <= '2017-04-07')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = df_all[df_all.activation_date >= '2017-04-12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1390439"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112985"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(df_test['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/axel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/axel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_train.drop(labels=['activation_date', 'item_id'], axis=1, inplace=True)\n",
    "df_val.drop(labels=['activation_date', 'item_id'], axis=1, inplace=True)\n",
    "df_test.drop(labels=['activation_date', 'item_id', 'deal_probability'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, _ = proc_df(df_train, 'deal_probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid, y_valid, _ = proc_df(df_val, 'deal_probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_test['dummy'] = np.zeros((len(df_test))); X_test, _, _ = proc_df(df_test, 'dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-9512a1bf5449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#del title_features; del params_features; del descr_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_all' is not defined"
     ]
    }
   ],
   "source": [
    "del df_all; del df_train; del df_val; del df_test; del df;\n",
    "#del title_features; del params_features; del descr_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'sp_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-17b4250c2a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mremove_na_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mremove_columns_with_nans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mremove_columns_with_nans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mremove_columns_with_nans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-777a80677806>\u001b[0m in \u001b[0;36mremove_columns_with_nans\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_columns_with_nans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/sparse/series.py\u001b[0m in \u001b[0;36misnull\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'isnull'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         arr = SparseArray(isnull(self.values.sp_values),\n\u001b[0m\u001b[1;32m    648\u001b[0m                           \u001b[0msparse_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                           fill_value=isnull(self.fill_value))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'sp_values'"
     ]
    }
   ],
   "source": [
    "#remove_na_columns(X_train)\n",
    "#remove_na_columns(X_valid)\n",
    "#remove_na_columns(X_test)\n",
    "\n",
    "remove_columns_with_nans(X_train)\n",
    "remove_columns_with_nans(X_valid)\n",
    "remove_columns_with_nans(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# space = {\n",
    "#         'n_estimators': hp.uniform('x_n_estimators', 150, 250),\n",
    "#         'max_depth': hp.uniform(\"x_max_depth\", 10, 70),\n",
    "#         'min_child_weight': hp.uniform('x_min_child', 1, 2),\n",
    "#         'subsample': hp.uniform('x_subsample', 0.85, 0.95),\n",
    "#         'reg_alpha': hp.uniform('x_reg_alpha', 0, 1.5),\n",
    "#         'reg_lambda': hp.uniform('x_reg_lambda', 0, 3.5),\n",
    "#         'colsample_bylevel': hp.uniform('x_colsample_bylevel', 0.2, 0.7),\n",
    "#         'colsample_bytree': hp.uniform('x_colsample_bytree', 0.2, 0.7),\n",
    "#         'learning_rate': hp.uniform('x_learning_rate', 0.05, 0.5),\n",
    "#         'gamma': hp.uniform('x_gamma', 0, 1),\n",
    "#         #'scale_pos_weight': hp.uniform('x_scale_pos_weight', 0, 1)\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trials = Trials()\n",
    "# best = fmin(fn=objective,\n",
    "#             space=space,\n",
    "#             algo=tpe.suggest,\n",
    "#             max_evals=10,\n",
    "#             trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reg_lambda de 0.29 a 1.29 (si)\n",
    "#reg_alpha de 1.18 a 2.18 (si)\n",
    "#colsample_bytree de 0.35 a 0.50 (no)\n",
    "#colsample_bytree de 0.35 a 0.25 (si)\n",
    "#learning_rate de 0.13 a 0.23 (no)\n",
    "\n",
    "#clf = xgb.XGBRegressor(n_jobs=4,\n",
    "#                        n_estimators = 1000, \n",
    "#                        learning_rate = 0.30774999932786233,\n",
    "#                        max_depth = 62,\n",
    "#                        min_child_weight = 1.0233847718190983,\n",
    "#                        subsample = 0.85298705648176,\n",
    "#                        reg_alpha = 3.1867660576819443,\n",
    "#                        reg_lambda = 3.29044542975829774,\n",
    "#                        colsample_bylevel = 0.2148790179469499,\n",
    "#                        colsample_bytree = 0.2086361239400142,\n",
    "#                        gamma=2.3194278336025607)\n",
    "    \n",
    "#eval_set  = [(X_train, y_train), (X_valid, y_valid)]\n",
    "\n",
    "#clf.fit(X_train, y_train, eval_set=eval_set, eval_metric=\"rmse\", early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_pred = clf.predict(X_valid, ntree_limit=clf.best_ntree_limit)\n",
    "#y_pred = np.clip(y_pred, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.DataFrame(y_pred, columns=['deal_probability']).to_feather(f'{PATH}y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = lgb.Dataset(X_train, label=y_train)\n",
    "val = lgb.Dataset(X_valid, label=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer\n",
    "\n",
    "space  = [Integer(10, 60, name='max_depth'),\n",
    "          Integer(40, 80, name='num_leaves'),\n",
    "          Integer(40, 80, name='min_data_per_leaf'),\n",
    "          Real(0.05, 2, name='min_child_weight'),\n",
    "          Real(0.6, 0.9, name='subsample'),\n",
    "          Integer(1, 6, name='subsample_freq'),\n",
    "          Real(0, 15, name='lambda_l1'),\n",
    "          Real(0, 15, name='lambda_l2'),\n",
    "          Real(0.4, 0.9, name='feature_fraction'),\n",
    "          Real(0.07, 0.13, name='learning_rate')\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective2(values):\n",
    "    \n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"n_estimators\": 5000,\n",
    "        \"max_depth\": values[0],\n",
    "        \"num_leaves\" : values[1],\n",
    "        \"min_data_per_leaf\": values[2],\n",
    "        \"min_child_weight\": values[3],\n",
    "        \"subsample\" : values[4],\n",
    "        \"subsample_freq\" : values[5],\n",
    "        \"lambda_l1\": values[6],\n",
    "        \"lambda_l2\": values[7],\n",
    "        \"feature_fraction\" : values[8],\n",
    "        \"learning_rate\" : values[9],\n",
    "        \"bagging_seed\" : 16,\n",
    "        \"device\" : \"gpu\",\n",
    "        \"max_bin\": 64\n",
    "    }\n",
    "\n",
    "    print(\"params: \")\n",
    "    print(\"max_depth: \" + str(values[0]))\n",
    "    print(\"num_leaves: \" + str(values[1]))\n",
    "    print(\"min_data_per_leaf: \" + str(values[2]))\n",
    "    print(\"min_child_weight: \" + str(values[3]))\n",
    "    print(\"subsample: \" + str(values[4]))\n",
    "    print(\"subsample_freq: \" + str(values[5]))\n",
    "    print(\"lambda_l1: \" + str(values[6]))\n",
    "    print(\"lambda_l2: \" + str(values[7]))\n",
    "    print(\"feature_fraction: \" + str(values[8]))\n",
    "    print(\"learning rate: \" + str(values[9]))\n",
    "    \n",
    "    \n",
    "    model = lgb.train(params, train, valid_sets=[train, val], early_stopping_rounds=100, verbose_eval=10)\n",
    "    \n",
    "    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    y_pred = np.clip(y_pred, 0, 1)\n",
    "    \n",
    "    \n",
    "    print(\"rmse:\" + str(rmse(y_valid, y_pred)))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return rmse(y_valid, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: \n",
      "max_depth: 40\n",
      "num_leaves: 74\n",
      "min_data_per_leaf: 74\n",
      "min_child_weight: 1.70214089063\n",
      "subsample: 0.787069109036\n",
      "subsample_freq: 3\n",
      "lambda_l1: 4.46301909817\n",
      "lambda_l2: 0.850694659762\n",
      "feature_fraction: 0.53632814729\n",
      "learning rate: 0.0986599070393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.235709\tvalid_1's rmse: 0.233941\n",
      "[20]\ttraining's rmse: 0.230241\tvalid_1's rmse: 0.228266\n",
      "[30]\ttraining's rmse: 0.228199\tvalid_1's rmse: 0.226292\n",
      "[40]\ttraining's rmse: 0.227033\tvalid_1's rmse: 0.225263\n",
      "[50]\ttraining's rmse: 0.226279\tvalid_1's rmse: 0.224682\n",
      "[60]\ttraining's rmse: 0.225605\tvalid_1's rmse: 0.224225\n",
      "[70]\ttraining's rmse: 0.225031\tvalid_1's rmse: 0.223807\n",
      "[80]\ttraining's rmse: 0.224495\tvalid_1's rmse: 0.223423\n",
      "[90]\ttraining's rmse: 0.224042\tvalid_1's rmse: 0.223124\n",
      "[100]\ttraining's rmse: 0.223639\tvalid_1's rmse: 0.222878\n",
      "[110]\ttraining's rmse: 0.223277\tvalid_1's rmse: 0.222665\n",
      "[120]\ttraining's rmse: 0.222935\tvalid_1's rmse: 0.222482\n",
      "[130]\ttraining's rmse: 0.222638\tvalid_1's rmse: 0.222353\n",
      "[140]\ttraining's rmse: 0.222333\tvalid_1's rmse: 0.222208\n",
      "[150]\ttraining's rmse: 0.222035\tvalid_1's rmse: 0.222071\n",
      "[160]\ttraining's rmse: 0.221772\tvalid_1's rmse: 0.221956\n",
      "[170]\ttraining's rmse: 0.221483\tvalid_1's rmse: 0.221841\n",
      "[180]\ttraining's rmse: 0.221212\tvalid_1's rmse: 0.221733\n",
      "[190]\ttraining's rmse: 0.220973\tvalid_1's rmse: 0.221638\n",
      "[200]\ttraining's rmse: 0.220753\tvalid_1's rmse: 0.221561\n",
      "[210]\ttraining's rmse: 0.220503\tvalid_1's rmse: 0.221481\n",
      "[220]\ttraining's rmse: 0.220282\tvalid_1's rmse: 0.221424\n",
      "[230]\ttraining's rmse: 0.220062\tvalid_1's rmse: 0.221376\n",
      "[240]\ttraining's rmse: 0.219849\tvalid_1's rmse: 0.221323\n",
      "[250]\ttraining's rmse: 0.219649\tvalid_1's rmse: 0.221288\n",
      "[260]\ttraining's rmse: 0.21945\tvalid_1's rmse: 0.221239\n",
      "[270]\ttraining's rmse: 0.219265\tvalid_1's rmse: 0.221196\n",
      "[280]\ttraining's rmse: 0.219072\tvalid_1's rmse: 0.22115\n",
      "[290]\ttraining's rmse: 0.218891\tvalid_1's rmse: 0.221111\n",
      "[300]\ttraining's rmse: 0.218703\tvalid_1's rmse: 0.221048\n",
      "[310]\ttraining's rmse: 0.218511\tvalid_1's rmse: 0.221005\n",
      "[320]\ttraining's rmse: 0.218327\tvalid_1's rmse: 0.220983\n",
      "[330]\ttraining's rmse: 0.21813\tvalid_1's rmse: 0.220941\n",
      "[340]\ttraining's rmse: 0.217932\tvalid_1's rmse: 0.220878\n",
      "[350]\ttraining's rmse: 0.217743\tvalid_1's rmse: 0.22082\n",
      "[360]\ttraining's rmse: 0.217573\tvalid_1's rmse: 0.220805\n",
      "[370]\ttraining's rmse: 0.217405\tvalid_1's rmse: 0.220798\n",
      "[380]\ttraining's rmse: 0.217237\tvalid_1's rmse: 0.220777\n",
      "[390]\ttraining's rmse: 0.217053\tvalid_1's rmse: 0.220759\n",
      "[400]\ttraining's rmse: 0.216885\tvalid_1's rmse: 0.220741\n",
      "[410]\ttraining's rmse: 0.21674\tvalid_1's rmse: 0.220733\n",
      "[420]\ttraining's rmse: 0.216566\tvalid_1's rmse: 0.220717\n",
      "[430]\ttraining's rmse: 0.216419\tvalid_1's rmse: 0.220697\n",
      "[440]\ttraining's rmse: 0.21625\tvalid_1's rmse: 0.22068\n",
      "[450]\ttraining's rmse: 0.216091\tvalid_1's rmse: 0.220664\n",
      "[460]\ttraining's rmse: 0.21594\tvalid_1's rmse: 0.220647\n",
      "[470]\ttraining's rmse: 0.215799\tvalid_1's rmse: 0.220631\n",
      "[480]\ttraining's rmse: 0.215656\tvalid_1's rmse: 0.220621\n",
      "[490]\ttraining's rmse: 0.21551\tvalid_1's rmse: 0.220604\n",
      "[500]\ttraining's rmse: 0.215344\tvalid_1's rmse: 0.220582\n",
      "[510]\ttraining's rmse: 0.215198\tvalid_1's rmse: 0.22056\n",
      "[520]\ttraining's rmse: 0.215038\tvalid_1's rmse: 0.220551\n",
      "[530]\ttraining's rmse: 0.214886\tvalid_1's rmse: 0.220565\n",
      "[540]\ttraining's rmse: 0.214726\tvalid_1's rmse: 0.220553\n",
      "[550]\ttraining's rmse: 0.214581\tvalid_1's rmse: 0.220552\n",
      "[560]\ttraining's rmse: 0.214439\tvalid_1's rmse: 0.220548\n",
      "[570]\ttraining's rmse: 0.214289\tvalid_1's rmse: 0.220533\n",
      "[580]\ttraining's rmse: 0.214148\tvalid_1's rmse: 0.220525\n",
      "[590]\ttraining's rmse: 0.213992\tvalid_1's rmse: 0.220511\n",
      "[600]\ttraining's rmse: 0.213837\tvalid_1's rmse: 0.220504\n",
      "[610]\ttraining's rmse: 0.213692\tvalid_1's rmse: 0.220505\n",
      "[620]\ttraining's rmse: 0.213548\tvalid_1's rmse: 0.220485\n",
      "[630]\ttraining's rmse: 0.213404\tvalid_1's rmse: 0.220475\n",
      "[640]\ttraining's rmse: 0.213272\tvalid_1's rmse: 0.220461\n",
      "[650]\ttraining's rmse: 0.213116\tvalid_1's rmse: 0.220435\n",
      "[660]\ttraining's rmse: 0.212962\tvalid_1's rmse: 0.220427\n",
      "[670]\ttraining's rmse: 0.212827\tvalid_1's rmse: 0.220419\n",
      "[680]\ttraining's rmse: 0.212682\tvalid_1's rmse: 0.220411\n",
      "[690]\ttraining's rmse: 0.212552\tvalid_1's rmse: 0.220399\n",
      "[700]\ttraining's rmse: 0.212426\tvalid_1's rmse: 0.220405\n",
      "[710]\ttraining's rmse: 0.212295\tvalid_1's rmse: 0.220398\n",
      "[720]\ttraining's rmse: 0.212157\tvalid_1's rmse: 0.220388\n",
      "[730]\ttraining's rmse: 0.212029\tvalid_1's rmse: 0.220374\n",
      "[740]\ttraining's rmse: 0.21189\tvalid_1's rmse: 0.220366\n",
      "[750]\ttraining's rmse: 0.211763\tvalid_1's rmse: 0.220363\n",
      "[760]\ttraining's rmse: 0.211627\tvalid_1's rmse: 0.220382\n",
      "[770]\ttraining's rmse: 0.211498\tvalid_1's rmse: 0.220376\n",
      "[780]\ttraining's rmse: 0.211365\tvalid_1's rmse: 0.220375\n",
      "[790]\ttraining's rmse: 0.211229\tvalid_1's rmse: 0.220368\n",
      "[800]\ttraining's rmse: 0.211096\tvalid_1's rmse: 0.220379\n",
      "[810]\ttraining's rmse: 0.210962\tvalid_1's rmse: 0.220367\n",
      "[820]\ttraining's rmse: 0.210837\tvalid_1's rmse: 0.220358\n",
      "[830]\ttraining's rmse: 0.2107\tvalid_1's rmse: 0.220353\n",
      "[840]\ttraining's rmse: 0.210578\tvalid_1's rmse: 0.220356\n",
      "[850]\ttraining's rmse: 0.210438\tvalid_1's rmse: 0.220365\n",
      "[860]\ttraining's rmse: 0.210301\tvalid_1's rmse: 0.220368\n",
      "[870]\ttraining's rmse: 0.210171\tvalid_1's rmse: 0.220384\n",
      "[880]\ttraining's rmse: 0.210038\tvalid_1's rmse: 0.220377\n",
      "[890]\ttraining's rmse: 0.209905\tvalid_1's rmse: 0.220372\n",
      "[900]\ttraining's rmse: 0.209774\tvalid_1's rmse: 0.220373\n",
      "[910]\ttraining's rmse: 0.209645\tvalid_1's rmse: 0.22037\n",
      "[920]\ttraining's rmse: 0.209514\tvalid_1's rmse: 0.220377\n",
      "[930]\ttraining's rmse: 0.209389\tvalid_1's rmse: 0.22038\n",
      "Early stopping, best iteration is:\n",
      "[837]\ttraining's rmse: 0.210617\tvalid_1's rmse: 0.220347\n",
      "rmse:0.22030132710672293\n",
      "params: \n",
      "max_depth: 51\n",
      "num_leaves: 59\n",
      "min_data_per_leaf: 56\n",
      "min_child_weight: 1.6803535889\n",
      "subsample: 0.701218848125\n",
      "subsample_freq: 4\n",
      "lambda_l1: 5.52362309761\n",
      "lambda_l2: 14.3573273843\n",
      "feature_fraction: 0.470175390206\n",
      "learning rate: 0.122205235502\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.23423\tvalid_1's rmse: 0.232233\n",
      "[20]\ttraining's rmse: 0.229834\tvalid_1's rmse: 0.227812\n",
      "[30]\ttraining's rmse: 0.228022\tvalid_1's rmse: 0.226045\n",
      "[40]\ttraining's rmse: 0.227163\tvalid_1's rmse: 0.225338\n",
      "[50]\ttraining's rmse: 0.226353\tvalid_1's rmse: 0.224697\n",
      "[60]\ttraining's rmse: 0.225692\tvalid_1's rmse: 0.224186\n",
      "[70]\ttraining's rmse: 0.225192\tvalid_1's rmse: 0.223829\n",
      "[80]\ttraining's rmse: 0.224688\tvalid_1's rmse: 0.223462\n",
      "[90]\ttraining's rmse: 0.224304\tvalid_1's rmse: 0.223234\n",
      "[100]\ttraining's rmse: 0.22394\tvalid_1's rmse: 0.223022\n",
      "[110]\ttraining's rmse: 0.223613\tvalid_1's rmse: 0.222834\n",
      "[120]\ttraining's rmse: 0.223267\tvalid_1's rmse: 0.222618\n",
      "[130]\ttraining's rmse: 0.223005\tvalid_1's rmse: 0.222504\n",
      "[140]\ttraining's rmse: 0.222706\tvalid_1's rmse: 0.22237\n",
      "[150]\ttraining's rmse: 0.222448\tvalid_1's rmse: 0.222279\n",
      "[160]\ttraining's rmse: 0.222186\tvalid_1's rmse: 0.222169\n",
      "[170]\ttraining's rmse: 0.221961\tvalid_1's rmse: 0.222092\n",
      "[180]\ttraining's rmse: 0.221736\tvalid_1's rmse: 0.222006\n",
      "[190]\ttraining's rmse: 0.221513\tvalid_1's rmse: 0.221921\n",
      "[200]\ttraining's rmse: 0.221296\tvalid_1's rmse: 0.221855\n",
      "[210]\ttraining's rmse: 0.221062\tvalid_1's rmse: 0.221751\n",
      "[220]\ttraining's rmse: 0.220873\tvalid_1's rmse: 0.221691\n",
      "[230]\ttraining's rmse: 0.220669\tvalid_1's rmse: 0.221615\n",
      "[240]\ttraining's rmse: 0.220482\tvalid_1's rmse: 0.221573\n",
      "[250]\ttraining's rmse: 0.220298\tvalid_1's rmse: 0.221539\n",
      "[260]\ttraining's rmse: 0.220128\tvalid_1's rmse: 0.221528\n",
      "[270]\ttraining's rmse: 0.219947\tvalid_1's rmse: 0.221499\n",
      "[280]\ttraining's rmse: 0.219762\tvalid_1's rmse: 0.221447\n",
      "[290]\ttraining's rmse: 0.219599\tvalid_1's rmse: 0.221411\n",
      "[300]\ttraining's rmse: 0.219425\tvalid_1's rmse: 0.221389\n",
      "[310]\ttraining's rmse: 0.21925\tvalid_1's rmse: 0.221354\n",
      "[320]\ttraining's rmse: 0.219067\tvalid_1's rmse: 0.221296\n",
      "[330]\ttraining's rmse: 0.218909\tvalid_1's rmse: 0.22126\n",
      "[340]\ttraining's rmse: 0.218738\tvalid_1's rmse: 0.22122\n",
      "[350]\ttraining's rmse: 0.218592\tvalid_1's rmse: 0.221216\n",
      "[360]\ttraining's rmse: 0.218428\tvalid_1's rmse: 0.221192\n",
      "[370]\ttraining's rmse: 0.218266\tvalid_1's rmse: 0.221168\n",
      "[380]\ttraining's rmse: 0.218118\tvalid_1's rmse: 0.22115\n",
      "[390]\ttraining's rmse: 0.217955\tvalid_1's rmse: 0.221126\n",
      "[400]\ttraining's rmse: 0.21781\tvalid_1's rmse: 0.221106\n",
      "[410]\ttraining's rmse: 0.217658\tvalid_1's rmse: 0.221075\n",
      "[420]\ttraining's rmse: 0.217499\tvalid_1's rmse: 0.221055\n",
      "[430]\ttraining's rmse: 0.217359\tvalid_1's rmse: 0.221039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[440]\ttraining's rmse: 0.217203\tvalid_1's rmse: 0.220996\n",
      "[450]\ttraining's rmse: 0.21707\tvalid_1's rmse: 0.220983\n",
      "[460]\ttraining's rmse: 0.216922\tvalid_1's rmse: 0.220991\n",
      "[470]\ttraining's rmse: 0.21679\tvalid_1's rmse: 0.22098\n",
      "[480]\ttraining's rmse: 0.21664\tvalid_1's rmse: 0.220969\n",
      "[490]\ttraining's rmse: 0.2165\tvalid_1's rmse: 0.220949\n",
      "[500]\ttraining's rmse: 0.21636\tvalid_1's rmse: 0.220931\n",
      "[510]\ttraining's rmse: 0.21622\tvalid_1's rmse: 0.220923\n",
      "[520]\ttraining's rmse: 0.216074\tvalid_1's rmse: 0.220897\n",
      "[530]\ttraining's rmse: 0.215941\tvalid_1's rmse: 0.220879\n",
      "[540]\ttraining's rmse: 0.215806\tvalid_1's rmse: 0.220856\n",
      "[550]\ttraining's rmse: 0.215667\tvalid_1's rmse: 0.220842\n",
      "[560]\ttraining's rmse: 0.215517\tvalid_1's rmse: 0.2208\n",
      "[570]\ttraining's rmse: 0.215378\tvalid_1's rmse: 0.220811\n",
      "[580]\ttraining's rmse: 0.215247\tvalid_1's rmse: 0.220794\n",
      "[590]\ttraining's rmse: 0.215107\tvalid_1's rmse: 0.22078\n",
      "[600]\ttraining's rmse: 0.214979\tvalid_1's rmse: 0.220768\n",
      "[610]\ttraining's rmse: 0.214835\tvalid_1's rmse: 0.220752\n",
      "[620]\ttraining's rmse: 0.214696\tvalid_1's rmse: 0.220748\n",
      "[630]\ttraining's rmse: 0.214557\tvalid_1's rmse: 0.220727\n",
      "[640]\ttraining's rmse: 0.214434\tvalid_1's rmse: 0.220725\n",
      "[650]\ttraining's rmse: 0.214301\tvalid_1's rmse: 0.220718\n",
      "[660]\ttraining's rmse: 0.214174\tvalid_1's rmse: 0.220712\n",
      "[670]\ttraining's rmse: 0.214059\tvalid_1's rmse: 0.220728\n",
      "[680]\ttraining's rmse: 0.213934\tvalid_1's rmse: 0.220733\n",
      "[690]\ttraining's rmse: 0.213811\tvalid_1's rmse: 0.220726\n",
      "[700]\ttraining's rmse: 0.213681\tvalid_1's rmse: 0.220706\n",
      "[710]\ttraining's rmse: 0.213557\tvalid_1's rmse: 0.220697\n",
      "[720]\ttraining's rmse: 0.213434\tvalid_1's rmse: 0.220692\n",
      "[730]\ttraining's rmse: 0.213304\tvalid_1's rmse: 0.22069\n",
      "[740]\ttraining's rmse: 0.213182\tvalid_1's rmse: 0.220702\n",
      "[750]\ttraining's rmse: 0.213063\tvalid_1's rmse: 0.220701\n",
      "[760]\ttraining's rmse: 0.212933\tvalid_1's rmse: 0.220679\n",
      "[770]\ttraining's rmse: 0.212812\tvalid_1's rmse: 0.220677\n",
      "[780]\ttraining's rmse: 0.21269\tvalid_1's rmse: 0.220661\n",
      "[790]\ttraining's rmse: 0.212559\tvalid_1's rmse: 0.220651\n",
      "[800]\ttraining's rmse: 0.212435\tvalid_1's rmse: 0.220632\n",
      "[810]\ttraining's rmse: 0.212312\tvalid_1's rmse: 0.220637\n",
      "[820]\ttraining's rmse: 0.212194\tvalid_1's rmse: 0.220622\n",
      "[830]\ttraining's rmse: 0.212077\tvalid_1's rmse: 0.220635\n",
      "[840]\ttraining's rmse: 0.211958\tvalid_1's rmse: 0.22062\n",
      "[850]\ttraining's rmse: 0.21184\tvalid_1's rmse: 0.220608\n",
      "[860]\ttraining's rmse: 0.211726\tvalid_1's rmse: 0.220618\n",
      "[870]\ttraining's rmse: 0.211606\tvalid_1's rmse: 0.220618\n",
      "[880]\ttraining's rmse: 0.211488\tvalid_1's rmse: 0.220609\n",
      "[890]\ttraining's rmse: 0.21137\tvalid_1's rmse: 0.220612\n",
      "[900]\ttraining's rmse: 0.21124\tvalid_1's rmse: 0.220603\n",
      "[910]\ttraining's rmse: 0.211123\tvalid_1's rmse: 0.220604\n",
      "[920]\ttraining's rmse: 0.211014\tvalid_1's rmse: 0.220606\n",
      "[930]\ttraining's rmse: 0.210892\tvalid_1's rmse: 0.220605\n",
      "[940]\ttraining's rmse: 0.210785\tvalid_1's rmse: 0.220609\n",
      "[950]\ttraining's rmse: 0.210657\tvalid_1's rmse: 0.22061\n",
      "[960]\ttraining's rmse: 0.210537\tvalid_1's rmse: 0.220612\n",
      "[970]\ttraining's rmse: 0.210413\tvalid_1's rmse: 0.220603\n",
      "[980]\ttraining's rmse: 0.210301\tvalid_1's rmse: 0.220605\n",
      "[990]\ttraining's rmse: 0.210193\tvalid_1's rmse: 0.220621\n",
      "[1000]\ttraining's rmse: 0.210076\tvalid_1's rmse: 0.220614\n",
      "[1010]\ttraining's rmse: 0.209958\tvalid_1's rmse: 0.220607\n",
      "[1020]\ttraining's rmse: 0.209851\tvalid_1's rmse: 0.22061\n",
      "[1030]\ttraining's rmse: 0.209737\tvalid_1's rmse: 0.220602\n",
      "[1040]\ttraining's rmse: 0.209618\tvalid_1's rmse: 0.220609\n",
      "[1050]\ttraining's rmse: 0.209499\tvalid_1's rmse: 0.220601\n",
      "[1060]\ttraining's rmse: 0.209392\tvalid_1's rmse: 0.220616\n",
      "[1070]\ttraining's rmse: 0.209273\tvalid_1's rmse: 0.220622\n",
      "Early stopping, best iteration is:\n",
      "[973]\ttraining's rmse: 0.210379\tvalid_1's rmse: 0.220592\n",
      "rmse:0.22052531315116924\n",
      "params: \n",
      "max_depth: 34\n",
      "num_leaves: 72\n",
      "min_data_per_leaf: 61\n",
      "min_child_weight: 1.37381508373\n",
      "subsample: 0.816189796418\n",
      "subsample_freq: 4\n",
      "lambda_l1: 8.06059844174\n",
      "lambda_l2: 11.3792343648\n",
      "feature_fraction: 0.452953803594\n",
      "learning rate: 0.0984160251608\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.235931\tvalid_1's rmse: 0.234171\n",
      "[20]\ttraining's rmse: 0.230578\tvalid_1's rmse: 0.228553\n",
      "[30]\ttraining's rmse: 0.2285\tvalid_1's rmse: 0.226489\n",
      "[40]\ttraining's rmse: 0.227339\tvalid_1's rmse: 0.225426\n",
      "[50]\ttraining's rmse: 0.226559\tvalid_1's rmse: 0.224786\n",
      "[60]\ttraining's rmse: 0.22593\tvalid_1's rmse: 0.224297\n",
      "[70]\ttraining's rmse: 0.225374\tvalid_1's rmse: 0.223899\n",
      "[80]\ttraining's rmse: 0.224846\tvalid_1's rmse: 0.223515\n",
      "[90]\ttraining's rmse: 0.224405\tvalid_1's rmse: 0.223196\n",
      "[100]\ttraining's rmse: 0.224041\tvalid_1's rmse: 0.222971\n",
      "[110]\ttraining's rmse: 0.223699\tvalid_1's rmse: 0.222766\n",
      "[120]\ttraining's rmse: 0.223365\tvalid_1's rmse: 0.222584\n",
      "[130]\ttraining's rmse: 0.223092\tvalid_1's rmse: 0.222446\n",
      "[140]\ttraining's rmse: 0.22278\tvalid_1's rmse: 0.222266\n",
      "[150]\ttraining's rmse: 0.222505\tvalid_1's rmse: 0.222113\n",
      "[160]\ttraining's rmse: 0.222257\tvalid_1's rmse: 0.222001\n",
      "[170]\ttraining's rmse: 0.222015\tvalid_1's rmse: 0.221897\n",
      "[180]\ttraining's rmse: 0.221759\tvalid_1's rmse: 0.221765\n",
      "[190]\ttraining's rmse: 0.221529\tvalid_1's rmse: 0.221662\n",
      "[200]\ttraining's rmse: 0.221295\tvalid_1's rmse: 0.22155\n",
      "[210]\ttraining's rmse: 0.221077\tvalid_1's rmse: 0.221478\n",
      "[220]\ttraining's rmse: 0.220877\tvalid_1's rmse: 0.221395\n",
      "[230]\ttraining's rmse: 0.220662\tvalid_1's rmse: 0.221294\n",
      "[240]\ttraining's rmse: 0.220466\tvalid_1's rmse: 0.221219\n",
      "[250]\ttraining's rmse: 0.220281\tvalid_1's rmse: 0.221174\n",
      "[260]\ttraining's rmse: 0.220121\tvalid_1's rmse: 0.221148\n",
      "[270]\ttraining's rmse: 0.219933\tvalid_1's rmse: 0.221085\n",
      "[280]\ttraining's rmse: 0.219767\tvalid_1's rmse: 0.221053\n",
      "[290]\ttraining's rmse: 0.219604\tvalid_1's rmse: 0.221018\n",
      "[300]\ttraining's rmse: 0.21945\tvalid_1's rmse: 0.220993\n",
      "[310]\ttraining's rmse: 0.219283\tvalid_1's rmse: 0.220952\n",
      "[320]\ttraining's rmse: 0.219121\tvalid_1's rmse: 0.220889\n",
      "[330]\ttraining's rmse: 0.218957\tvalid_1's rmse: 0.220858\n",
      "[340]\ttraining's rmse: 0.218795\tvalid_1's rmse: 0.22081\n",
      "[350]\ttraining's rmse: 0.218636\tvalid_1's rmse: 0.220778\n",
      "[360]\ttraining's rmse: 0.218477\tvalid_1's rmse: 0.220738\n",
      "[370]\ttraining's rmse: 0.218338\tvalid_1's rmse: 0.220722\n",
      "[380]\ttraining's rmse: 0.218195\tvalid_1's rmse: 0.220695\n",
      "[390]\ttraining's rmse: 0.218045\tvalid_1's rmse: 0.220682\n",
      "[400]\ttraining's rmse: 0.21791\tvalid_1's rmse: 0.220672\n",
      "[410]\ttraining's rmse: 0.217762\tvalid_1's rmse: 0.220626\n",
      "[420]\ttraining's rmse: 0.217628\tvalid_1's rmse: 0.2206\n",
      "[430]\ttraining's rmse: 0.217486\tvalid_1's rmse: 0.220577\n",
      "[440]\ttraining's rmse: 0.217358\tvalid_1's rmse: 0.220557\n",
      "[450]\ttraining's rmse: 0.217225\tvalid_1's rmse: 0.220537\n",
      "[460]\ttraining's rmse: 0.217098\tvalid_1's rmse: 0.220523\n",
      "[470]\ttraining's rmse: 0.216963\tvalid_1's rmse: 0.220508\n",
      "[480]\ttraining's rmse: 0.216834\tvalid_1's rmse: 0.220499\n",
      "[490]\ttraining's rmse: 0.216703\tvalid_1's rmse: 0.220496\n",
      "[500]\ttraining's rmse: 0.216568\tvalid_1's rmse: 0.22047\n",
      "[510]\ttraining's rmse: 0.216414\tvalid_1's rmse: 0.220452\n",
      "[520]\ttraining's rmse: 0.216278\tvalid_1's rmse: 0.220439\n",
      "[530]\ttraining's rmse: 0.216149\tvalid_1's rmse: 0.220423\n",
      "[540]\ttraining's rmse: 0.216026\tvalid_1's rmse: 0.220422\n",
      "[550]\ttraining's rmse: 0.215893\tvalid_1's rmse: 0.220404\n",
      "[560]\ttraining's rmse: 0.215771\tvalid_1's rmse: 0.220379\n",
      "[570]\ttraining's rmse: 0.215639\tvalid_1's rmse: 0.220359\n",
      "[580]\ttraining's rmse: 0.215516\tvalid_1's rmse: 0.220341\n",
      "[590]\ttraining's rmse: 0.215404\tvalid_1's rmse: 0.22033\n",
      "[600]\ttraining's rmse: 0.215286\tvalid_1's rmse: 0.220313\n",
      "[610]\ttraining's rmse: 0.215162\tvalid_1's rmse: 0.220292\n",
      "[620]\ttraining's rmse: 0.215042\tvalid_1's rmse: 0.220284\n",
      "[630]\ttraining's rmse: 0.214915\tvalid_1's rmse: 0.220269\n",
      "[640]\ttraining's rmse: 0.21479\tvalid_1's rmse: 0.220264\n",
      "[650]\ttraining's rmse: 0.214667\tvalid_1's rmse: 0.220257\n",
      "[660]\ttraining's rmse: 0.214549\tvalid_1's rmse: 0.220256\n",
      "[670]\ttraining's rmse: 0.214439\tvalid_1's rmse: 0.22026\n",
      "[680]\ttraining's rmse: 0.21432\tvalid_1's rmse: 0.220246\n",
      "[690]\ttraining's rmse: 0.214203\tvalid_1's rmse: 0.220233\n",
      "[700]\ttraining's rmse: 0.214081\tvalid_1's rmse: 0.220219\n",
      "[710]\ttraining's rmse: 0.213964\tvalid_1's rmse: 0.22021\n",
      "[720]\ttraining's rmse: 0.213854\tvalid_1's rmse: 0.220197\n",
      "[730]\ttraining's rmse: 0.213737\tvalid_1's rmse: 0.220186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[740]\ttraining's rmse: 0.213623\tvalid_1's rmse: 0.220181\n",
      "[750]\ttraining's rmse: 0.213503\tvalid_1's rmse: 0.22017\n",
      "[760]\ttraining's rmse: 0.213376\tvalid_1's rmse: 0.220135\n",
      "[770]\ttraining's rmse: 0.213261\tvalid_1's rmse: 0.220122\n",
      "[780]\ttraining's rmse: 0.213142\tvalid_1's rmse: 0.220117\n",
      "[790]\ttraining's rmse: 0.213032\tvalid_1's rmse: 0.220107\n",
      "[800]\ttraining's rmse: 0.212923\tvalid_1's rmse: 0.220113\n",
      "[810]\ttraining's rmse: 0.212812\tvalid_1's rmse: 0.220125\n",
      "[820]\ttraining's rmse: 0.21271\tvalid_1's rmse: 0.220117\n",
      "[830]\ttraining's rmse: 0.212601\tvalid_1's rmse: 0.220105\n",
      "[840]\ttraining's rmse: 0.212497\tvalid_1's rmse: 0.220088\n",
      "[850]\ttraining's rmse: 0.212386\tvalid_1's rmse: 0.220078\n",
      "[860]\ttraining's rmse: 0.212279\tvalid_1's rmse: 0.220068\n",
      "[870]\ttraining's rmse: 0.212156\tvalid_1's rmse: 0.220068\n",
      "[880]\ttraining's rmse: 0.212041\tvalid_1's rmse: 0.220058\n",
      "[890]\ttraining's rmse: 0.211931\tvalid_1's rmse: 0.220063\n",
      "[900]\ttraining's rmse: 0.211815\tvalid_1's rmse: 0.220038\n",
      "[910]\ttraining's rmse: 0.211713\tvalid_1's rmse: 0.220043\n",
      "[920]\ttraining's rmse: 0.211594\tvalid_1's rmse: 0.220041\n",
      "[930]\ttraining's rmse: 0.21149\tvalid_1's rmse: 0.220037\n",
      "[940]\ttraining's rmse: 0.211391\tvalid_1's rmse: 0.22003\n",
      "[950]\ttraining's rmse: 0.211293\tvalid_1's rmse: 0.220029\n",
      "[960]\ttraining's rmse: 0.211187\tvalid_1's rmse: 0.220029\n",
      "[970]\ttraining's rmse: 0.211064\tvalid_1's rmse: 0.220017\n",
      "[980]\ttraining's rmse: 0.210953\tvalid_1's rmse: 0.220018\n",
      "[990]\ttraining's rmse: 0.210854\tvalid_1's rmse: 0.22001\n",
      "[1000]\ttraining's rmse: 0.210743\tvalid_1's rmse: 0.220005\n",
      "[1010]\ttraining's rmse: 0.21064\tvalid_1's rmse: 0.219993\n",
      "[1020]\ttraining's rmse: 0.210536\tvalid_1's rmse: 0.21999\n",
      "[1030]\ttraining's rmse: 0.210429\tvalid_1's rmse: 0.219988\n",
      "[1040]\ttraining's rmse: 0.210327\tvalid_1's rmse: 0.219983\n",
      "[1050]\ttraining's rmse: 0.210234\tvalid_1's rmse: 0.219984\n",
      "[1060]\ttraining's rmse: 0.210137\tvalid_1's rmse: 0.219994\n",
      "[1070]\ttraining's rmse: 0.210042\tvalid_1's rmse: 0.219986\n",
      "[1080]\ttraining's rmse: 0.209929\tvalid_1's rmse: 0.21999\n",
      "[1090]\ttraining's rmse: 0.209821\tvalid_1's rmse: 0.219987\n",
      "[1100]\ttraining's rmse: 0.209721\tvalid_1's rmse: 0.219983\n",
      "[1110]\ttraining's rmse: 0.209623\tvalid_1's rmse: 0.219984\n",
      "[1120]\ttraining's rmse: 0.209522\tvalid_1's rmse: 0.219972\n",
      "[1130]\ttraining's rmse: 0.209434\tvalid_1's rmse: 0.219966\n",
      "[1140]\ttraining's rmse: 0.209335\tvalid_1's rmse: 0.219955\n",
      "[1150]\ttraining's rmse: 0.20924\tvalid_1's rmse: 0.219957\n",
      "[1160]\ttraining's rmse: 0.209139\tvalid_1's rmse: 0.219947\n",
      "[1170]\ttraining's rmse: 0.209034\tvalid_1's rmse: 0.219946\n",
      "[1180]\ttraining's rmse: 0.208936\tvalid_1's rmse: 0.219948\n",
      "[1190]\ttraining's rmse: 0.208831\tvalid_1's rmse: 0.219952\n",
      "[1200]\ttraining's rmse: 0.208734\tvalid_1's rmse: 0.219951\n",
      "[1210]\ttraining's rmse: 0.208628\tvalid_1's rmse: 0.219945\n",
      "[1220]\ttraining's rmse: 0.208538\tvalid_1's rmse: 0.219935\n",
      "[1230]\ttraining's rmse: 0.208434\tvalid_1's rmse: 0.219924\n",
      "[1240]\ttraining's rmse: 0.208334\tvalid_1's rmse: 0.219914\n",
      "[1250]\ttraining's rmse: 0.208234\tvalid_1's rmse: 0.219908\n",
      "[1260]\ttraining's rmse: 0.208124\tvalid_1's rmse: 0.219905\n",
      "[1270]\ttraining's rmse: 0.208025\tvalid_1's rmse: 0.219903\n",
      "[1280]\ttraining's rmse: 0.207931\tvalid_1's rmse: 0.219896\n",
      "[1290]\ttraining's rmse: 0.207837\tvalid_1's rmse: 0.219894\n",
      "[1300]\ttraining's rmse: 0.207749\tvalid_1's rmse: 0.219904\n",
      "[1310]\ttraining's rmse: 0.207656\tvalid_1's rmse: 0.219904\n",
      "[1320]\ttraining's rmse: 0.207556\tvalid_1's rmse: 0.219894\n",
      "[1330]\ttraining's rmse: 0.207449\tvalid_1's rmse: 0.219894\n",
      "[1340]\ttraining's rmse: 0.207357\tvalid_1's rmse: 0.2199\n",
      "[1350]\ttraining's rmse: 0.207264\tvalid_1's rmse: 0.219892\n",
      "[1360]\ttraining's rmse: 0.207169\tvalid_1's rmse: 0.219896\n",
      "[1370]\ttraining's rmse: 0.207078\tvalid_1's rmse: 0.219898\n",
      "[1380]\ttraining's rmse: 0.206987\tvalid_1's rmse: 0.219899\n",
      "[1390]\ttraining's rmse: 0.206882\tvalid_1's rmse: 0.219906\n",
      "[1400]\ttraining's rmse: 0.206783\tvalid_1's rmse: 0.219897\n",
      "[1410]\ttraining's rmse: 0.206691\tvalid_1's rmse: 0.219898\n",
      "Early stopping, best iteration is:\n",
      "[1318]\ttraining's rmse: 0.207574\tvalid_1's rmse: 0.21989\n",
      "rmse:0.21983500685990562\n",
      "params: \n",
      "max_depth: 19\n",
      "num_leaves: 69\n",
      "min_data_per_leaf: 49\n",
      "min_child_weight: 0.313675438141\n",
      "subsample: 0.697242302338\n",
      "subsample_freq: 2\n",
      "lambda_l1: 3.33482082377\n",
      "lambda_l2: 5.79733471689\n",
      "feature_fraction: 0.851299237765\n",
      "learning rate: 0.0969969993947\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.235504\tvalid_1's rmse: 0.233708\n",
      "[20]\ttraining's rmse: 0.230129\tvalid_1's rmse: 0.228117\n",
      "[30]\ttraining's rmse: 0.228251\tvalid_1's rmse: 0.226287\n",
      "[40]\ttraining's rmse: 0.227116\tvalid_1's rmse: 0.225313\n",
      "[50]\ttraining's rmse: 0.226373\tvalid_1's rmse: 0.22472\n",
      "[60]\ttraining's rmse: 0.22572\tvalid_1's rmse: 0.224246\n",
      "[70]\ttraining's rmse: 0.225097\tvalid_1's rmse: 0.223756\n",
      "[80]\ttraining's rmse: 0.224604\tvalid_1's rmse: 0.223438\n",
      "[90]\ttraining's rmse: 0.224165\tvalid_1's rmse: 0.223142\n",
      "[100]\ttraining's rmse: 0.223761\tvalid_1's rmse: 0.222909\n",
      "[110]\ttraining's rmse: 0.223421\tvalid_1's rmse: 0.222727\n",
      "[120]\ttraining's rmse: 0.223061\tvalid_1's rmse: 0.22253\n",
      "[130]\ttraining's rmse: 0.222761\tvalid_1's rmse: 0.222406\n",
      "[140]\ttraining's rmse: 0.222432\tvalid_1's rmse: 0.222232\n",
      "[150]\ttraining's rmse: 0.222125\tvalid_1's rmse: 0.222101\n",
      "[160]\ttraining's rmse: 0.221833\tvalid_1's rmse: 0.221987\n",
      "[170]\ttraining's rmse: 0.221604\tvalid_1's rmse: 0.221933\n",
      "[180]\ttraining's rmse: 0.221335\tvalid_1's rmse: 0.221819\n",
      "[190]\ttraining's rmse: 0.221112\tvalid_1's rmse: 0.22173\n",
      "[200]\ttraining's rmse: 0.22088\tvalid_1's rmse: 0.221647\n",
      "[210]\ttraining's rmse: 0.220643\tvalid_1's rmse: 0.221545\n",
      "[220]\ttraining's rmse: 0.220421\tvalid_1's rmse: 0.221507\n",
      "[230]\ttraining's rmse: 0.220209\tvalid_1's rmse: 0.221447\n",
      "[240]\ttraining's rmse: 0.21999\tvalid_1's rmse: 0.22142\n",
      "[250]\ttraining's rmse: 0.219784\tvalid_1's rmse: 0.221391\n",
      "[260]\ttraining's rmse: 0.219582\tvalid_1's rmse: 0.221359\n",
      "[270]\ttraining's rmse: 0.219381\tvalid_1's rmse: 0.221309\n",
      "[280]\ttraining's rmse: 0.219158\tvalid_1's rmse: 0.221257\n",
      "[290]\ttraining's rmse: 0.218984\tvalid_1's rmse: 0.221255\n",
      "[300]\ttraining's rmse: 0.218786\tvalid_1's rmse: 0.22121\n",
      "[310]\ttraining's rmse: 0.218616\tvalid_1's rmse: 0.221205\n",
      "[320]\ttraining's rmse: 0.218437\tvalid_1's rmse: 0.221187\n",
      "[330]\ttraining's rmse: 0.218268\tvalid_1's rmse: 0.221171\n",
      "[340]\ttraining's rmse: 0.218083\tvalid_1's rmse: 0.221149\n",
      "[350]\ttraining's rmse: 0.217889\tvalid_1's rmse: 0.221086\n",
      "[360]\ttraining's rmse: 0.217713\tvalid_1's rmse: 0.221051\n",
      "[370]\ttraining's rmse: 0.217545\tvalid_1's rmse: 0.221045\n",
      "[380]\ttraining's rmse: 0.217384\tvalid_1's rmse: 0.221024\n",
      "[390]\ttraining's rmse: 0.217212\tvalid_1's rmse: 0.221\n",
      "[400]\ttraining's rmse: 0.217056\tvalid_1's rmse: 0.220996\n",
      "[410]\ttraining's rmse: 0.216883\tvalid_1's rmse: 0.220978\n",
      "[420]\ttraining's rmse: 0.216702\tvalid_1's rmse: 0.220964\n",
      "[430]\ttraining's rmse: 0.216528\tvalid_1's rmse: 0.22093\n",
      "[440]\ttraining's rmse: 0.216354\tvalid_1's rmse: 0.220918\n",
      "[450]\ttraining's rmse: 0.216192\tvalid_1's rmse: 0.220894\n",
      "[460]\ttraining's rmse: 0.216032\tvalid_1's rmse: 0.220881\n",
      "[470]\ttraining's rmse: 0.215872\tvalid_1's rmse: 0.220851\n",
      "[480]\ttraining's rmse: 0.215714\tvalid_1's rmse: 0.220839\n",
      "[490]\ttraining's rmse: 0.215554\tvalid_1's rmse: 0.220824\n",
      "[500]\ttraining's rmse: 0.215382\tvalid_1's rmse: 0.220818\n",
      "[510]\ttraining's rmse: 0.215216\tvalid_1's rmse: 0.220812\n",
      "[520]\ttraining's rmse: 0.215062\tvalid_1's rmse: 0.220797\n",
      "[530]\ttraining's rmse: 0.214924\tvalid_1's rmse: 0.220784\n",
      "[540]\ttraining's rmse: 0.214763\tvalid_1's rmse: 0.220763\n",
      "[550]\ttraining's rmse: 0.214617\tvalid_1's rmse: 0.220751\n",
      "[560]\ttraining's rmse: 0.214453\tvalid_1's rmse: 0.220717\n",
      "[570]\ttraining's rmse: 0.214309\tvalid_1's rmse: 0.220706\n",
      "[580]\ttraining's rmse: 0.214146\tvalid_1's rmse: 0.220703\n",
      "[590]\ttraining's rmse: 0.214001\tvalid_1's rmse: 0.220707\n",
      "[600]\ttraining's rmse: 0.213859\tvalid_1's rmse: 0.220702\n",
      "[610]\ttraining's rmse: 0.213705\tvalid_1's rmse: 0.22069\n",
      "[620]\ttraining's rmse: 0.213557\tvalid_1's rmse: 0.220686\n",
      "[630]\ttraining's rmse: 0.213406\tvalid_1's rmse: 0.220677\n",
      "[640]\ttraining's rmse: 0.213258\tvalid_1's rmse: 0.220673\n",
      "[650]\ttraining's rmse: 0.213118\tvalid_1's rmse: 0.220667\n",
      "[660]\ttraining's rmse: 0.212967\tvalid_1's rmse: 0.220671\n",
      "[670]\ttraining's rmse: 0.212831\tvalid_1's rmse: 0.220662\n",
      "[680]\ttraining's rmse: 0.212681\tvalid_1's rmse: 0.220653\n",
      "[690]\ttraining's rmse: 0.212533\tvalid_1's rmse: 0.220652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\ttraining's rmse: 0.212388\tvalid_1's rmse: 0.22064\n",
      "[710]\ttraining's rmse: 0.212251\tvalid_1's rmse: 0.22064\n",
      "[720]\ttraining's rmse: 0.21211\tvalid_1's rmse: 0.220639\n",
      "[730]\ttraining's rmse: 0.211976\tvalid_1's rmse: 0.220636\n",
      "[740]\ttraining's rmse: 0.211839\tvalid_1's rmse: 0.220637\n",
      "[750]\ttraining's rmse: 0.21169\tvalid_1's rmse: 0.220621\n",
      "[760]\ttraining's rmse: 0.211531\tvalid_1's rmse: 0.2206\n",
      "[770]\ttraining's rmse: 0.2114\tvalid_1's rmse: 0.220594\n",
      "[780]\ttraining's rmse: 0.211262\tvalid_1's rmse: 0.220598\n",
      "[790]\ttraining's rmse: 0.211119\tvalid_1's rmse: 0.220591\n",
      "[800]\ttraining's rmse: 0.210979\tvalid_1's rmse: 0.220576\n",
      "[810]\ttraining's rmse: 0.210841\tvalid_1's rmse: 0.220587\n",
      "[820]\ttraining's rmse: 0.210698\tvalid_1's rmse: 0.220578\n",
      "[830]\ttraining's rmse: 0.21056\tvalid_1's rmse: 0.220577\n",
      "[840]\ttraining's rmse: 0.210407\tvalid_1's rmse: 0.220576\n",
      "[850]\ttraining's rmse: 0.21028\tvalid_1's rmse: 0.220572\n",
      "[860]\ttraining's rmse: 0.210147\tvalid_1's rmse: 0.220566\n",
      "[870]\ttraining's rmse: 0.210002\tvalid_1's rmse: 0.220551\n",
      "[880]\ttraining's rmse: 0.209864\tvalid_1's rmse: 0.220534\n",
      "[890]\ttraining's rmse: 0.20973\tvalid_1's rmse: 0.220525\n",
      "[900]\ttraining's rmse: 0.209599\tvalid_1's rmse: 0.220531\n",
      "[910]\ttraining's rmse: 0.209457\tvalid_1's rmse: 0.220528\n",
      "[920]\ttraining's rmse: 0.209326\tvalid_1's rmse: 0.220534\n",
      "[930]\ttraining's rmse: 0.209193\tvalid_1's rmse: 0.220543\n",
      "[940]\ttraining's rmse: 0.209052\tvalid_1's rmse: 0.220531\n",
      "[950]\ttraining's rmse: 0.208913\tvalid_1's rmse: 0.220519\n",
      "[960]\ttraining's rmse: 0.208783\tvalid_1's rmse: 0.220524\n",
      "[970]\ttraining's rmse: 0.208647\tvalid_1's rmse: 0.220535\n",
      "[980]\ttraining's rmse: 0.20853\tvalid_1's rmse: 0.220537\n",
      "[990]\ttraining's rmse: 0.208407\tvalid_1's rmse: 0.220545\n",
      "[1000]\ttraining's rmse: 0.208279\tvalid_1's rmse: 0.22054\n",
      "[1010]\ttraining's rmse: 0.208152\tvalid_1's rmse: 0.220536\n",
      "[1020]\ttraining's rmse: 0.208023\tvalid_1's rmse: 0.220536\n",
      "[1030]\ttraining's rmse: 0.207897\tvalid_1's rmse: 0.220526\n",
      "[1040]\ttraining's rmse: 0.207766\tvalid_1's rmse: 0.220532\n",
      "[1050]\ttraining's rmse: 0.207645\tvalid_1's rmse: 0.220527\n",
      "Early stopping, best iteration is:\n",
      "[951]\ttraining's rmse: 0.2089\tvalid_1's rmse: 0.220517\n",
      "rmse:0.220473525282653\n",
      "params: \n",
      "max_depth: 41\n",
      "num_leaves: 76\n",
      "min_data_per_leaf: 44\n",
      "min_child_weight: 1.94112768211\n",
      "subsample: 0.795942010739\n",
      "subsample_freq: 2\n",
      "lambda_l1: 5.37228250454\n",
      "lambda_l2: 11.2602921183\n",
      "feature_fraction: 0.703915334358\n",
      "learning rate: 0.0895028337405\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.236425\tvalid_1's rmse: 0.23473\n",
      "[20]\ttraining's rmse: 0.230561\tvalid_1's rmse: 0.228623\n",
      "[30]\ttraining's rmse: 0.228403\tvalid_1's rmse: 0.226533\n",
      "[40]\ttraining's rmse: 0.227256\tvalid_1's rmse: 0.225534\n",
      "[50]\ttraining's rmse: 0.226526\tvalid_1's rmse: 0.224953\n",
      "[60]\ttraining's rmse: 0.225813\tvalid_1's rmse: 0.224381\n",
      "[70]\ttraining's rmse: 0.225291\tvalid_1's rmse: 0.223983\n",
      "[80]\ttraining's rmse: 0.224729\tvalid_1's rmse: 0.223553\n",
      "[90]\ttraining's rmse: 0.224238\tvalid_1's rmse: 0.223197\n",
      "[100]\ttraining's rmse: 0.223852\tvalid_1's rmse: 0.222975\n",
      "[110]\ttraining's rmse: 0.223467\tvalid_1's rmse: 0.222765\n",
      "[120]\ttraining's rmse: 0.223111\tvalid_1's rmse: 0.222544\n",
      "[130]\ttraining's rmse: 0.222803\tvalid_1's rmse: 0.222405\n",
      "[140]\ttraining's rmse: 0.222454\tvalid_1's rmse: 0.222218\n",
      "[150]\ttraining's rmse: 0.222172\tvalid_1's rmse: 0.222085\n",
      "[160]\ttraining's rmse: 0.221925\tvalid_1's rmse: 0.221985\n",
      "[170]\ttraining's rmse: 0.221637\tvalid_1's rmse: 0.221851\n",
      "[180]\ttraining's rmse: 0.221392\tvalid_1's rmse: 0.221763\n",
      "[190]\ttraining's rmse: 0.221167\tvalid_1's rmse: 0.22167\n",
      "[200]\ttraining's rmse: 0.22094\tvalid_1's rmse: 0.221577\n",
      "[210]\ttraining's rmse: 0.220712\tvalid_1's rmse: 0.221514\n",
      "[220]\ttraining's rmse: 0.22047\tvalid_1's rmse: 0.221445\n",
      "[230]\ttraining's rmse: 0.220253\tvalid_1's rmse: 0.221373\n",
      "[240]\ttraining's rmse: 0.22004\tvalid_1's rmse: 0.221305\n",
      "[250]\ttraining's rmse: 0.219843\tvalid_1's rmse: 0.221248\n",
      "[260]\ttraining's rmse: 0.21966\tvalid_1's rmse: 0.221206\n",
      "[270]\ttraining's rmse: 0.219469\tvalid_1's rmse: 0.221165\n",
      "[280]\ttraining's rmse: 0.219286\tvalid_1's rmse: 0.221109\n",
      "[290]\ttraining's rmse: 0.219129\tvalid_1's rmse: 0.221089\n",
      "[300]\ttraining's rmse: 0.218954\tvalid_1's rmse: 0.221073\n",
      "[310]\ttraining's rmse: 0.218767\tvalid_1's rmse: 0.221014\n",
      "[320]\ttraining's rmse: 0.218608\tvalid_1's rmse: 0.220992\n",
      "[330]\ttraining's rmse: 0.218435\tvalid_1's rmse: 0.220956\n",
      "[340]\ttraining's rmse: 0.218269\tvalid_1's rmse: 0.22092\n",
      "[350]\ttraining's rmse: 0.218103\tvalid_1's rmse: 0.220907\n",
      "[360]\ttraining's rmse: 0.217956\tvalid_1's rmse: 0.220898\n",
      "[370]\ttraining's rmse: 0.217798\tvalid_1's rmse: 0.220876\n",
      "[380]\ttraining's rmse: 0.21763\tvalid_1's rmse: 0.220848\n",
      "[390]\ttraining's rmse: 0.217493\tvalid_1's rmse: 0.220839\n",
      "[400]\ttraining's rmse: 0.217348\tvalid_1's rmse: 0.220814\n",
      "[410]\ttraining's rmse: 0.217196\tvalid_1's rmse: 0.220788\n",
      "[420]\ttraining's rmse: 0.217035\tvalid_1's rmse: 0.22077\n",
      "[430]\ttraining's rmse: 0.216873\tvalid_1's rmse: 0.220732\n",
      "[440]\ttraining's rmse: 0.21673\tvalid_1's rmse: 0.22072\n",
      "[450]\ttraining's rmse: 0.216596\tvalid_1's rmse: 0.220707\n",
      "[460]\ttraining's rmse: 0.216439\tvalid_1's rmse: 0.22069\n",
      "[470]\ttraining's rmse: 0.216302\tvalid_1's rmse: 0.220686\n",
      "[480]\ttraining's rmse: 0.216151\tvalid_1's rmse: 0.220652\n",
      "[490]\ttraining's rmse: 0.216021\tvalid_1's rmse: 0.220645\n",
      "[500]\ttraining's rmse: 0.215865\tvalid_1's rmse: 0.220613\n",
      "[510]\ttraining's rmse: 0.215724\tvalid_1's rmse: 0.22061\n",
      "[520]\ttraining's rmse: 0.21559\tvalid_1's rmse: 0.220599\n",
      "[530]\ttraining's rmse: 0.215439\tvalid_1's rmse: 0.220581\n",
      "[540]\ttraining's rmse: 0.215309\tvalid_1's rmse: 0.220571\n",
      "[550]\ttraining's rmse: 0.215165\tvalid_1's rmse: 0.220556\n",
      "[560]\ttraining's rmse: 0.21501\tvalid_1's rmse: 0.220524\n",
      "[570]\ttraining's rmse: 0.214865\tvalid_1's rmse: 0.220502\n",
      "[580]\ttraining's rmse: 0.214725\tvalid_1's rmse: 0.220499\n",
      "[590]\ttraining's rmse: 0.214589\tvalid_1's rmse: 0.220478\n",
      "[600]\ttraining's rmse: 0.214458\tvalid_1's rmse: 0.220461\n",
      "[610]\ttraining's rmse: 0.214321\tvalid_1's rmse: 0.220459\n",
      "[620]\ttraining's rmse: 0.214191\tvalid_1's rmse: 0.22045\n",
      "[630]\ttraining's rmse: 0.214058\tvalid_1's rmse: 0.220454\n",
      "[640]\ttraining's rmse: 0.213932\tvalid_1's rmse: 0.220443\n",
      "[650]\ttraining's rmse: 0.213799\tvalid_1's rmse: 0.220431\n",
      "[660]\ttraining's rmse: 0.213671\tvalid_1's rmse: 0.220428\n",
      "[670]\ttraining's rmse: 0.213533\tvalid_1's rmse: 0.220419\n",
      "[680]\ttraining's rmse: 0.213405\tvalid_1's rmse: 0.220413\n",
      "[690]\ttraining's rmse: 0.213272\tvalid_1's rmse: 0.220396\n",
      "[700]\ttraining's rmse: 0.213126\tvalid_1's rmse: 0.22038\n",
      "[710]\ttraining's rmse: 0.212993\tvalid_1's rmse: 0.220374\n",
      "[720]\ttraining's rmse: 0.212866\tvalid_1's rmse: 0.220381\n",
      "[730]\ttraining's rmse: 0.212727\tvalid_1's rmse: 0.220368\n",
      "[740]\ttraining's rmse: 0.212593\tvalid_1's rmse: 0.220353\n",
      "[750]\ttraining's rmse: 0.212461\tvalid_1's rmse: 0.22033\n",
      "[760]\ttraining's rmse: 0.212325\tvalid_1's rmse: 0.22031\n",
      "[770]\ttraining's rmse: 0.212209\tvalid_1's rmse: 0.22032\n",
      "[780]\ttraining's rmse: 0.212074\tvalid_1's rmse: 0.220321\n",
      "[790]\ttraining's rmse: 0.211957\tvalid_1's rmse: 0.22031\n",
      "[800]\ttraining's rmse: 0.211837\tvalid_1's rmse: 0.220317\n",
      "[810]\ttraining's rmse: 0.2117\tvalid_1's rmse: 0.220319\n",
      "[820]\ttraining's rmse: 0.211577\tvalid_1's rmse: 0.220299\n",
      "[830]\ttraining's rmse: 0.211453\tvalid_1's rmse: 0.220287\n",
      "[840]\ttraining's rmse: 0.211329\tvalid_1's rmse: 0.220268\n",
      "[850]\ttraining's rmse: 0.211199\tvalid_1's rmse: 0.220266\n",
      "[860]\ttraining's rmse: 0.211086\tvalid_1's rmse: 0.220268\n",
      "[870]\ttraining's rmse: 0.21096\tvalid_1's rmse: 0.220268\n",
      "[880]\ttraining's rmse: 0.210849\tvalid_1's rmse: 0.220258\n",
      "[890]\ttraining's rmse: 0.210736\tvalid_1's rmse: 0.220261\n",
      "[900]\ttraining's rmse: 0.21063\tvalid_1's rmse: 0.220256\n",
      "[910]\ttraining's rmse: 0.2105\tvalid_1's rmse: 0.220245\n",
      "[920]\ttraining's rmse: 0.210382\tvalid_1's rmse: 0.220238\n",
      "[930]\ttraining's rmse: 0.210249\tvalid_1's rmse: 0.220238\n",
      "[940]\ttraining's rmse: 0.210135\tvalid_1's rmse: 0.220235\n",
      "[950]\ttraining's rmse: 0.210005\tvalid_1's rmse: 0.220247\n",
      "[960]\ttraining's rmse: 0.209877\tvalid_1's rmse: 0.220241\n",
      "[970]\ttraining's rmse: 0.209755\tvalid_1's rmse: 0.220247\n",
      "[980]\ttraining's rmse: 0.209633\tvalid_1's rmse: 0.220239\n",
      "[990]\ttraining's rmse: 0.209519\tvalid_1's rmse: 0.220231\n",
      "[1000]\ttraining's rmse: 0.209402\tvalid_1's rmse: 0.22023\n",
      "[1010]\ttraining's rmse: 0.209284\tvalid_1's rmse: 0.220238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1020]\ttraining's rmse: 0.209169\tvalid_1's rmse: 0.220237\n",
      "[1030]\ttraining's rmse: 0.209062\tvalid_1's rmse: 0.220239\n",
      "[1040]\ttraining's rmse: 0.208923\tvalid_1's rmse: 0.220225\n",
      "[1050]\ttraining's rmse: 0.208807\tvalid_1's rmse: 0.220229\n",
      "[1060]\ttraining's rmse: 0.20869\tvalid_1's rmse: 0.220219\n",
      "[1070]\ttraining's rmse: 0.208572\tvalid_1's rmse: 0.220212\n",
      "[1080]\ttraining's rmse: 0.208453\tvalid_1's rmse: 0.220207\n",
      "[1090]\ttraining's rmse: 0.208343\tvalid_1's rmse: 0.220208\n",
      "[1100]\ttraining's rmse: 0.208223\tvalid_1's rmse: 0.220203\n",
      "[1110]\ttraining's rmse: 0.208096\tvalid_1's rmse: 0.220191\n",
      "[1120]\ttraining's rmse: 0.207985\tvalid_1's rmse: 0.220196\n",
      "[1130]\ttraining's rmse: 0.207871\tvalid_1's rmse: 0.220191\n",
      "[1140]\ttraining's rmse: 0.20776\tvalid_1's rmse: 0.220188\n",
      "[1150]\ttraining's rmse: 0.207646\tvalid_1's rmse: 0.220168\n",
      "[1160]\ttraining's rmse: 0.207524\tvalid_1's rmse: 0.220158\n",
      "[1170]\ttraining's rmse: 0.207411\tvalid_1's rmse: 0.220145\n",
      "[1180]\ttraining's rmse: 0.207304\tvalid_1's rmse: 0.220136\n",
      "[1190]\ttraining's rmse: 0.207195\tvalid_1's rmse: 0.220141\n",
      "[1200]\ttraining's rmse: 0.207081\tvalid_1's rmse: 0.220142\n",
      "[1210]\ttraining's rmse: 0.206963\tvalid_1's rmse: 0.220139\n",
      "[1220]\ttraining's rmse: 0.206849\tvalid_1's rmse: 0.220139\n",
      "[1230]\ttraining's rmse: 0.206734\tvalid_1's rmse: 0.220133\n",
      "[1240]\ttraining's rmse: 0.206629\tvalid_1's rmse: 0.220132\n",
      "[1250]\ttraining's rmse: 0.206506\tvalid_1's rmse: 0.220136\n",
      "[1260]\ttraining's rmse: 0.206402\tvalid_1's rmse: 0.220132\n",
      "[1270]\ttraining's rmse: 0.206293\tvalid_1's rmse: 0.220132\n",
      "[1280]\ttraining's rmse: 0.206178\tvalid_1's rmse: 0.220132\n",
      "[1290]\ttraining's rmse: 0.206084\tvalid_1's rmse: 0.220134\n",
      "[1300]\ttraining's rmse: 0.205992\tvalid_1's rmse: 0.220139\n",
      "[1310]\ttraining's rmse: 0.205883\tvalid_1's rmse: 0.220128\n",
      "[1320]\ttraining's rmse: 0.205772\tvalid_1's rmse: 0.220122\n",
      "[1330]\ttraining's rmse: 0.205668\tvalid_1's rmse: 0.220119\n",
      "[1340]\ttraining's rmse: 0.205555\tvalid_1's rmse: 0.220122\n",
      "[1350]\ttraining's rmse: 0.205457\tvalid_1's rmse: 0.220129\n",
      "[1360]\ttraining's rmse: 0.205346\tvalid_1's rmse: 0.22012\n",
      "[1370]\ttraining's rmse: 0.205236\tvalid_1's rmse: 0.22012\n",
      "[1380]\ttraining's rmse: 0.205124\tvalid_1's rmse: 0.220123\n",
      "[1390]\ttraining's rmse: 0.205019\tvalid_1's rmse: 0.220121\n",
      "[1400]\ttraining's rmse: 0.204915\tvalid_1's rmse: 0.220116\n",
      "[1410]\ttraining's rmse: 0.204809\tvalid_1's rmse: 0.22011\n",
      "[1420]\ttraining's rmse: 0.204705\tvalid_1's rmse: 0.220115\n",
      "[1430]\ttraining's rmse: 0.204595\tvalid_1's rmse: 0.220121\n",
      "[1440]\ttraining's rmse: 0.204482\tvalid_1's rmse: 0.220133\n",
      "[1450]\ttraining's rmse: 0.204379\tvalid_1's rmse: 0.220137\n",
      "[1460]\ttraining's rmse: 0.204277\tvalid_1's rmse: 0.220129\n",
      "[1470]\ttraining's rmse: 0.204172\tvalid_1's rmse: 0.220127\n",
      "[1480]\ttraining's rmse: 0.204059\tvalid_1's rmse: 0.220125\n",
      "[1490]\ttraining's rmse: 0.203946\tvalid_1's rmse: 0.220129\n",
      "[1500]\ttraining's rmse: 0.203828\tvalid_1's rmse: 0.220115\n",
      "[1510]\ttraining's rmse: 0.203723\tvalid_1's rmse: 0.220114\n",
      "Early stopping, best iteration is:\n",
      "[1416]\ttraining's rmse: 0.204749\tvalid_1's rmse: 0.220109\n",
      "rmse:0.22005905770417455\n",
      "params: \n",
      "max_depth: 12\n",
      "num_leaves: 65\n",
      "min_data_per_leaf: 78\n",
      "min_child_weight: 1.32294111816\n",
      "subsample: 0.790517662081\n",
      "subsample_freq: 6\n",
      "lambda_l1: 8.72775494158\n",
      "lambda_l2: 6.2155288234\n",
      "feature_fraction: 0.637348751144\n",
      "learning rate: 0.107410606068\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.235114\tvalid_1's rmse: 0.233291\n",
      "[20]\ttraining's rmse: 0.230082\tvalid_1's rmse: 0.228081\n",
      "[30]\ttraining's rmse: 0.228195\tvalid_1's rmse: 0.22626\n",
      "[40]\ttraining's rmse: 0.227233\tvalid_1's rmse: 0.225439\n",
      "[50]\ttraining's rmse: 0.226426\tvalid_1's rmse: 0.22479\n",
      "[60]\ttraining's rmse: 0.225741\tvalid_1's rmse: 0.224253\n",
      "[70]\ttraining's rmse: 0.225225\tvalid_1's rmse: 0.223891\n",
      "[80]\ttraining's rmse: 0.224753\tvalid_1's rmse: 0.223566\n",
      "[90]\ttraining's rmse: 0.224289\tvalid_1's rmse: 0.223243\n",
      "[100]\ttraining's rmse: 0.22391\tvalid_1's rmse: 0.222997\n",
      "[110]\ttraining's rmse: 0.223597\tvalid_1's rmse: 0.222799\n",
      "[120]\ttraining's rmse: 0.223309\tvalid_1's rmse: 0.222673\n",
      "[130]\ttraining's rmse: 0.223016\tvalid_1's rmse: 0.222513\n",
      "[140]\ttraining's rmse: 0.222715\tvalid_1's rmse: 0.222335\n",
      "[150]\ttraining's rmse: 0.22247\tvalid_1's rmse: 0.222229\n",
      "[160]\ttraining's rmse: 0.222214\tvalid_1's rmse: 0.222108\n",
      "[170]\ttraining's rmse: 0.221968\tvalid_1's rmse: 0.221991\n",
      "[180]\ttraining's rmse: 0.221763\tvalid_1's rmse: 0.221914\n",
      "[190]\ttraining's rmse: 0.221541\tvalid_1's rmse: 0.221835\n",
      "[200]\ttraining's rmse: 0.221333\tvalid_1's rmse: 0.221758\n",
      "[210]\ttraining's rmse: 0.221108\tvalid_1's rmse: 0.221682\n",
      "[220]\ttraining's rmse: 0.220919\tvalid_1's rmse: 0.221622\n",
      "[230]\ttraining's rmse: 0.220714\tvalid_1's rmse: 0.221541\n",
      "[240]\ttraining's rmse: 0.220516\tvalid_1's rmse: 0.221485\n",
      "[250]\ttraining's rmse: 0.220326\tvalid_1's rmse: 0.221409\n",
      "[260]\ttraining's rmse: 0.220139\tvalid_1's rmse: 0.221346\n",
      "[270]\ttraining's rmse: 0.219952\tvalid_1's rmse: 0.221269\n",
      "[280]\ttraining's rmse: 0.21978\tvalid_1's rmse: 0.221228\n",
      "[290]\ttraining's rmse: 0.219625\tvalid_1's rmse: 0.221191\n",
      "[300]\ttraining's rmse: 0.219451\tvalid_1's rmse: 0.221147\n",
      "[310]\ttraining's rmse: 0.219271\tvalid_1's rmse: 0.221109\n",
      "[320]\ttraining's rmse: 0.219111\tvalid_1's rmse: 0.22108\n",
      "[330]\ttraining's rmse: 0.218952\tvalid_1's rmse: 0.221045\n",
      "[340]\ttraining's rmse: 0.218782\tvalid_1's rmse: 0.220998\n",
      "[350]\ttraining's rmse: 0.218645\tvalid_1's rmse: 0.220987\n",
      "[360]\ttraining's rmse: 0.218498\tvalid_1's rmse: 0.220966\n",
      "[370]\ttraining's rmse: 0.218356\tvalid_1's rmse: 0.220955\n",
      "[380]\ttraining's rmse: 0.218218\tvalid_1's rmse: 0.220924\n",
      "[390]\ttraining's rmse: 0.218081\tvalid_1's rmse: 0.220919\n",
      "[400]\ttraining's rmse: 0.21794\tvalid_1's rmse: 0.220902\n",
      "[410]\ttraining's rmse: 0.21781\tvalid_1's rmse: 0.220871\n",
      "[420]\ttraining's rmse: 0.217662\tvalid_1's rmse: 0.220843\n",
      "[430]\ttraining's rmse: 0.217522\tvalid_1's rmse: 0.220808\n",
      "[440]\ttraining's rmse: 0.217383\tvalid_1's rmse: 0.220782\n",
      "[450]\ttraining's rmse: 0.217246\tvalid_1's rmse: 0.22077\n",
      "[460]\ttraining's rmse: 0.217102\tvalid_1's rmse: 0.220743\n",
      "[470]\ttraining's rmse: 0.21697\tvalid_1's rmse: 0.220709\n",
      "[480]\ttraining's rmse: 0.216821\tvalid_1's rmse: 0.220671\n",
      "[490]\ttraining's rmse: 0.216696\tvalid_1's rmse: 0.220661\n",
      "[500]\ttraining's rmse: 0.216558\tvalid_1's rmse: 0.220634\n",
      "[510]\ttraining's rmse: 0.216426\tvalid_1's rmse: 0.220617\n",
      "[520]\ttraining's rmse: 0.216293\tvalid_1's rmse: 0.220596\n",
      "[530]\ttraining's rmse: 0.216172\tvalid_1's rmse: 0.220584\n",
      "[540]\ttraining's rmse: 0.216036\tvalid_1's rmse: 0.220565\n",
      "[550]\ttraining's rmse: 0.215893\tvalid_1's rmse: 0.220524\n",
      "[560]\ttraining's rmse: 0.215773\tvalid_1's rmse: 0.220508\n",
      "[570]\ttraining's rmse: 0.215642\tvalid_1's rmse: 0.220488\n",
      "[580]\ttraining's rmse: 0.215527\tvalid_1's rmse: 0.220486\n",
      "[590]\ttraining's rmse: 0.215405\tvalid_1's rmse: 0.220479\n",
      "[600]\ttraining's rmse: 0.215287\tvalid_1's rmse: 0.220464\n",
      "[610]\ttraining's rmse: 0.215153\tvalid_1's rmse: 0.220431\n",
      "[620]\ttraining's rmse: 0.215039\tvalid_1's rmse: 0.220431\n",
      "[630]\ttraining's rmse: 0.214935\tvalid_1's rmse: 0.220425\n",
      "[640]\ttraining's rmse: 0.214803\tvalid_1's rmse: 0.22042\n",
      "[650]\ttraining's rmse: 0.214685\tvalid_1's rmse: 0.220409\n",
      "[660]\ttraining's rmse: 0.214568\tvalid_1's rmse: 0.220402\n",
      "[670]\ttraining's rmse: 0.214448\tvalid_1's rmse: 0.220391\n",
      "[680]\ttraining's rmse: 0.214325\tvalid_1's rmse: 0.22036\n",
      "[690]\ttraining's rmse: 0.214201\tvalid_1's rmse: 0.220354\n",
      "[700]\ttraining's rmse: 0.214077\tvalid_1's rmse: 0.220339\n",
      "[710]\ttraining's rmse: 0.21396\tvalid_1's rmse: 0.220342\n",
      "[720]\ttraining's rmse: 0.213848\tvalid_1's rmse: 0.220341\n",
      "[730]\ttraining's rmse: 0.21374\tvalid_1's rmse: 0.220339\n",
      "[740]\ttraining's rmse: 0.213629\tvalid_1's rmse: 0.220319\n",
      "[750]\ttraining's rmse: 0.213511\tvalid_1's rmse: 0.220317\n",
      "[760]\ttraining's rmse: 0.213403\tvalid_1's rmse: 0.220306\n",
      "[770]\ttraining's rmse: 0.213283\tvalid_1's rmse: 0.2203\n",
      "[780]\ttraining's rmse: 0.213164\tvalid_1's rmse: 0.220289\n",
      "[790]\ttraining's rmse: 0.213047\tvalid_1's rmse: 0.220279\n",
      "[800]\ttraining's rmse: 0.212936\tvalid_1's rmse: 0.22028\n",
      "[810]\ttraining's rmse: 0.212819\tvalid_1's rmse: 0.220271\n",
      "[820]\ttraining's rmse: 0.212711\tvalid_1's rmse: 0.220262\n",
      "[830]\ttraining's rmse: 0.212604\tvalid_1's rmse: 0.220249\n",
      "[840]\ttraining's rmse: 0.212488\tvalid_1's rmse: 0.220243\n",
      "[850]\ttraining's rmse: 0.212368\tvalid_1's rmse: 0.220232\n",
      "[860]\ttraining's rmse: 0.21226\tvalid_1's rmse: 0.220234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[870]\ttraining's rmse: 0.212145\tvalid_1's rmse: 0.220232\n",
      "[880]\ttraining's rmse: 0.212043\tvalid_1's rmse: 0.220236\n",
      "[890]\ttraining's rmse: 0.211937\tvalid_1's rmse: 0.220244\n",
      "[900]\ttraining's rmse: 0.211827\tvalid_1's rmse: 0.220242\n",
      "[910]\ttraining's rmse: 0.211714\tvalid_1's rmse: 0.220246\n",
      "[920]\ttraining's rmse: 0.211606\tvalid_1's rmse: 0.22024\n",
      "[930]\ttraining's rmse: 0.211494\tvalid_1's rmse: 0.220233\n",
      "[940]\ttraining's rmse: 0.211394\tvalid_1's rmse: 0.220231\n",
      "[950]\ttraining's rmse: 0.211287\tvalid_1's rmse: 0.220216\n",
      "[960]\ttraining's rmse: 0.211176\tvalid_1's rmse: 0.220223\n",
      "[970]\ttraining's rmse: 0.211072\tvalid_1's rmse: 0.220211\n",
      "[980]\ttraining's rmse: 0.210968\tvalid_1's rmse: 0.220204\n",
      "[990]\ttraining's rmse: 0.210862\tvalid_1's rmse: 0.220204\n",
      "[1000]\ttraining's rmse: 0.210741\tvalid_1's rmse: 0.220189\n",
      "[1010]\ttraining's rmse: 0.210636\tvalid_1's rmse: 0.220178\n",
      "[1020]\ttraining's rmse: 0.21053\tvalid_1's rmse: 0.220179\n",
      "[1030]\ttraining's rmse: 0.210428\tvalid_1's rmse: 0.220172\n",
      "[1040]\ttraining's rmse: 0.210321\tvalid_1's rmse: 0.220164\n",
      "[1050]\ttraining's rmse: 0.210216\tvalid_1's rmse: 0.220159\n",
      "[1060]\ttraining's rmse: 0.210102\tvalid_1's rmse: 0.220155\n",
      "[1070]\ttraining's rmse: 0.209998\tvalid_1's rmse: 0.220143\n",
      "[1080]\ttraining's rmse: 0.209899\tvalid_1's rmse: 0.220153\n",
      "[1090]\ttraining's rmse: 0.209786\tvalid_1's rmse: 0.22014\n",
      "[1100]\ttraining's rmse: 0.209678\tvalid_1's rmse: 0.220128\n",
      "[1110]\ttraining's rmse: 0.209572\tvalid_1's rmse: 0.220131\n",
      "[1120]\ttraining's rmse: 0.209461\tvalid_1's rmse: 0.220132\n",
      "[1130]\ttraining's rmse: 0.209353\tvalid_1's rmse: 0.22012\n",
      "[1140]\ttraining's rmse: 0.209252\tvalid_1's rmse: 0.22012\n",
      "[1150]\ttraining's rmse: 0.209148\tvalid_1's rmse: 0.220113\n",
      "[1160]\ttraining's rmse: 0.209045\tvalid_1's rmse: 0.220104\n",
      "[1170]\ttraining's rmse: 0.208939\tvalid_1's rmse: 0.220093\n",
      "[1180]\ttraining's rmse: 0.208841\tvalid_1's rmse: 0.220097\n",
      "[1190]\ttraining's rmse: 0.208746\tvalid_1's rmse: 0.220099\n",
      "[1200]\ttraining's rmse: 0.208652\tvalid_1's rmse: 0.220107\n",
      "[1210]\ttraining's rmse: 0.208552\tvalid_1's rmse: 0.220115\n",
      "[1220]\ttraining's rmse: 0.208452\tvalid_1's rmse: 0.220111\n",
      "[1230]\ttraining's rmse: 0.208358\tvalid_1's rmse: 0.220121\n",
      "[1240]\ttraining's rmse: 0.208259\tvalid_1's rmse: 0.220129\n",
      "[1250]\ttraining's rmse: 0.208145\tvalid_1's rmse: 0.220122\n",
      "[1260]\ttraining's rmse: 0.208048\tvalid_1's rmse: 0.220127\n",
      "[1270]\ttraining's rmse: 0.20795\tvalid_1's rmse: 0.22012\n",
      "Early stopping, best iteration is:\n",
      "[1174]\ttraining's rmse: 0.208897\tvalid_1's rmse: 0.22009\n",
      "rmse:0.22002900174406145\n",
      "params: \n",
      "max_depth: 27\n",
      "num_leaves: 67\n",
      "min_data_per_leaf: 53\n",
      "min_child_weight: 1.56777368995\n",
      "subsample: 0.884871316035\n",
      "subsample_freq: 4\n",
      "lambda_l1: 0.203574534182\n",
      "lambda_l2: 9.3426914332\n",
      "feature_fraction: 0.736829815418\n",
      "learning rate: 0.12831670015\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.233021\tvalid_1's rmse: 0.231141\n",
      "[20]\ttraining's rmse: 0.228884\tvalid_1's rmse: 0.226911\n",
      "[30]\ttraining's rmse: 0.227292\tvalid_1's rmse: 0.225467\n",
      "[40]\ttraining's rmse: 0.226256\tvalid_1's rmse: 0.224585\n",
      "[50]\ttraining's rmse: 0.225435\tvalid_1's rmse: 0.223962\n",
      "[60]\ttraining's rmse: 0.224739\tvalid_1's rmse: 0.223469\n",
      "[70]\ttraining's rmse: 0.224226\tvalid_1's rmse: 0.223188\n",
      "[80]\ttraining's rmse: 0.223683\tvalid_1's rmse: 0.222864\n",
      "[90]\ttraining's rmse: 0.223204\tvalid_1's rmse: 0.222604\n",
      "[100]\ttraining's rmse: 0.222778\tvalid_1's rmse: 0.222397\n",
      "[110]\ttraining's rmse: 0.222389\tvalid_1's rmse: 0.222216\n",
      "[120]\ttraining's rmse: 0.222026\tvalid_1's rmse: 0.222121\n",
      "[130]\ttraining's rmse: 0.221673\tvalid_1's rmse: 0.222003\n",
      "[140]\ttraining's rmse: 0.221323\tvalid_1's rmse: 0.221888\n",
      "[150]\ttraining's rmse: 0.221016\tvalid_1's rmse: 0.221811\n",
      "[160]\ttraining's rmse: 0.220725\tvalid_1's rmse: 0.221754\n",
      "[170]\ttraining's rmse: 0.220409\tvalid_1's rmse: 0.221663\n",
      "[180]\ttraining's rmse: 0.220145\tvalid_1's rmse: 0.221616\n",
      "[190]\ttraining's rmse: 0.219884\tvalid_1's rmse: 0.221577\n",
      "[200]\ttraining's rmse: 0.219585\tvalid_1's rmse: 0.221454\n",
      "[210]\ttraining's rmse: 0.219329\tvalid_1's rmse: 0.221436\n",
      "[220]\ttraining's rmse: 0.219085\tvalid_1's rmse: 0.221429\n",
      "[230]\ttraining's rmse: 0.218809\tvalid_1's rmse: 0.221375\n",
      "[240]\ttraining's rmse: 0.218545\tvalid_1's rmse: 0.221337\n",
      "[250]\ttraining's rmse: 0.218299\tvalid_1's rmse: 0.221288\n",
      "[260]\ttraining's rmse: 0.21806\tvalid_1's rmse: 0.221244\n",
      "[270]\ttraining's rmse: 0.217813\tvalid_1's rmse: 0.221228\n",
      "[280]\ttraining's rmse: 0.217577\tvalid_1's rmse: 0.22122\n",
      "[290]\ttraining's rmse: 0.217347\tvalid_1's rmse: 0.221206\n",
      "[300]\ttraining's rmse: 0.217114\tvalid_1's rmse: 0.221165\n",
      "[310]\ttraining's rmse: 0.216873\tvalid_1's rmse: 0.22113\n",
      "[320]\ttraining's rmse: 0.216645\tvalid_1's rmse: 0.221109\n",
      "[330]\ttraining's rmse: 0.216433\tvalid_1's rmse: 0.221116\n",
      "[340]\ttraining's rmse: 0.216194\tvalid_1's rmse: 0.221077\n",
      "[350]\ttraining's rmse: 0.215982\tvalid_1's rmse: 0.221044\n",
      "[360]\ttraining's rmse: 0.215749\tvalid_1's rmse: 0.221001\n",
      "[370]\ttraining's rmse: 0.215501\tvalid_1's rmse: 0.220974\n",
      "[380]\ttraining's rmse: 0.215306\tvalid_1's rmse: 0.220986\n",
      "[390]\ttraining's rmse: 0.215086\tvalid_1's rmse: 0.220967\n",
      "[400]\ttraining's rmse: 0.214864\tvalid_1's rmse: 0.220943\n",
      "[410]\ttraining's rmse: 0.214657\tvalid_1's rmse: 0.220913\n",
      "[420]\ttraining's rmse: 0.214477\tvalid_1's rmse: 0.220902\n",
      "[430]\ttraining's rmse: 0.214234\tvalid_1's rmse: 0.220883\n",
      "[440]\ttraining's rmse: 0.214027\tvalid_1's rmse: 0.220875\n",
      "[450]\ttraining's rmse: 0.21382\tvalid_1's rmse: 0.220864\n",
      "[460]\ttraining's rmse: 0.213608\tvalid_1's rmse: 0.220828\n",
      "[470]\ttraining's rmse: 0.213413\tvalid_1's rmse: 0.220815\n",
      "[480]\ttraining's rmse: 0.213191\tvalid_1's rmse: 0.220797\n",
      "[490]\ttraining's rmse: 0.212995\tvalid_1's rmse: 0.220799\n",
      "[500]\ttraining's rmse: 0.212779\tvalid_1's rmse: 0.220777\n",
      "[510]\ttraining's rmse: 0.212562\tvalid_1's rmse: 0.22077\n",
      "[520]\ttraining's rmse: 0.212382\tvalid_1's rmse: 0.220775\n",
      "[530]\ttraining's rmse: 0.212171\tvalid_1's rmse: 0.220752\n",
      "[540]\ttraining's rmse: 0.211976\tvalid_1's rmse: 0.220753\n",
      "[550]\ttraining's rmse: 0.211775\tvalid_1's rmse: 0.220735\n",
      "[560]\ttraining's rmse: 0.211583\tvalid_1's rmse: 0.220737\n",
      "[570]\ttraining's rmse: 0.211387\tvalid_1's rmse: 0.220708\n",
      "[580]\ttraining's rmse: 0.211204\tvalid_1's rmse: 0.220718\n",
      "[590]\ttraining's rmse: 0.211013\tvalid_1's rmse: 0.220706\n",
      "[600]\ttraining's rmse: 0.210831\tvalid_1's rmse: 0.220695\n",
      "[610]\ttraining's rmse: 0.210607\tvalid_1's rmse: 0.220688\n",
      "[620]\ttraining's rmse: 0.210402\tvalid_1's rmse: 0.220687\n",
      "[630]\ttraining's rmse: 0.210199\tvalid_1's rmse: 0.220678\n",
      "[640]\ttraining's rmse: 0.210013\tvalid_1's rmse: 0.220683\n",
      "[650]\ttraining's rmse: 0.209824\tvalid_1's rmse: 0.220664\n",
      "[660]\ttraining's rmse: 0.209652\tvalid_1's rmse: 0.220663\n",
      "[670]\ttraining's rmse: 0.209465\tvalid_1's rmse: 0.220656\n",
      "[680]\ttraining's rmse: 0.209279\tvalid_1's rmse: 0.220661\n",
      "[690]\ttraining's rmse: 0.209094\tvalid_1's rmse: 0.220666\n",
      "[700]\ttraining's rmse: 0.208895\tvalid_1's rmse: 0.22066\n",
      "[710]\ttraining's rmse: 0.208713\tvalid_1's rmse: 0.220662\n",
      "[720]\ttraining's rmse: 0.208521\tvalid_1's rmse: 0.220671\n",
      "[730]\ttraining's rmse: 0.208331\tvalid_1's rmse: 0.220671\n",
      "[740]\ttraining's rmse: 0.20815\tvalid_1's rmse: 0.22068\n",
      "[750]\ttraining's rmse: 0.207964\tvalid_1's rmse: 0.220688\n",
      "[760]\ttraining's rmse: 0.207778\tvalid_1's rmse: 0.220673\n",
      "[770]\ttraining's rmse: 0.207603\tvalid_1's rmse: 0.220682\n",
      "[780]\ttraining's rmse: 0.207421\tvalid_1's rmse: 0.220675\n",
      "[790]\ttraining's rmse: 0.207238\tvalid_1's rmse: 0.220664\n",
      "Early stopping, best iteration is:\n",
      "[694]\ttraining's rmse: 0.209003\tvalid_1's rmse: 0.220651\n",
      "rmse:0.22061444170498087\n",
      "params: \n",
      "max_depth: 54\n",
      "num_leaves: 60\n",
      "min_data_per_leaf: 42\n",
      "min_child_weight: 0.929760468316\n",
      "subsample: 0.605996299623\n",
      "subsample_freq: 3\n",
      "lambda_l1: 14.6938009322\n",
      "lambda_l2: 5.39166695954\n",
      "feature_fraction: 0.640446765418\n",
      "learning rate: 0.111319670968\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.235059\tvalid_1's rmse: 0.233209\n",
      "[20]\ttraining's rmse: 0.230214\tvalid_1's rmse: 0.228171\n",
      "[30]\ttraining's rmse: 0.228411\tvalid_1's rmse: 0.226387\n",
      "[40]\ttraining's rmse: 0.227497\tvalid_1's rmse: 0.225574\n",
      "[50]\ttraining's rmse: 0.226663\tvalid_1's rmse: 0.224893\n",
      "[60]\ttraining's rmse: 0.226052\tvalid_1's rmse: 0.224422\n",
      "[70]\ttraining's rmse: 0.225591\tvalid_1's rmse: 0.224098\n",
      "[80]\ttraining's rmse: 0.225148\tvalid_1's rmse: 0.22376\n",
      "[90]\ttraining's rmse: 0.22479\tvalid_1's rmse: 0.223537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 0.22447\tvalid_1's rmse: 0.223331\n",
      "[110]\ttraining's rmse: 0.224171\tvalid_1's rmse: 0.223148\n",
      "[120]\ttraining's rmse: 0.223872\tvalid_1's rmse: 0.222948\n",
      "[130]\ttraining's rmse: 0.223599\tvalid_1's rmse: 0.222797\n",
      "[140]\ttraining's rmse: 0.223354\tvalid_1's rmse: 0.222684\n",
      "[150]\ttraining's rmse: 0.223095\tvalid_1's rmse: 0.222555\n",
      "[160]\ttraining's rmse: 0.222827\tvalid_1's rmse: 0.222415\n",
      "[170]\ttraining's rmse: 0.222594\tvalid_1's rmse: 0.222288\n",
      "[180]\ttraining's rmse: 0.222368\tvalid_1's rmse: 0.222176\n",
      "[190]\ttraining's rmse: 0.222171\tvalid_1's rmse: 0.222101\n",
      "[200]\ttraining's rmse: 0.22197\tvalid_1's rmse: 0.222015\n",
      "[210]\ttraining's rmse: 0.221781\tvalid_1's rmse: 0.221934\n",
      "[220]\ttraining's rmse: 0.221611\tvalid_1's rmse: 0.221874\n",
      "[230]\ttraining's rmse: 0.221437\tvalid_1's rmse: 0.221795\n",
      "[240]\ttraining's rmse: 0.221257\tvalid_1's rmse: 0.221731\n",
      "[250]\ttraining's rmse: 0.221089\tvalid_1's rmse: 0.22168\n",
      "[260]\ttraining's rmse: 0.22092\tvalid_1's rmse: 0.221618\n",
      "[270]\ttraining's rmse: 0.22077\tvalid_1's rmse: 0.221562\n",
      "[280]\ttraining's rmse: 0.220624\tvalid_1's rmse: 0.221532\n",
      "[290]\ttraining's rmse: 0.220463\tvalid_1's rmse: 0.221502\n",
      "[300]\ttraining's rmse: 0.220306\tvalid_1's rmse: 0.221441\n",
      "[310]\ttraining's rmse: 0.220158\tvalid_1's rmse: 0.221425\n",
      "[320]\ttraining's rmse: 0.220015\tvalid_1's rmse: 0.221398\n",
      "[330]\ttraining's rmse: 0.219879\tvalid_1's rmse: 0.221377\n",
      "[340]\ttraining's rmse: 0.219726\tvalid_1's rmse: 0.221325\n",
      "[350]\ttraining's rmse: 0.219585\tvalid_1's rmse: 0.221291\n",
      "[360]\ttraining's rmse: 0.219444\tvalid_1's rmse: 0.221255\n",
      "[370]\ttraining's rmse: 0.219317\tvalid_1's rmse: 0.221216\n",
      "[380]\ttraining's rmse: 0.219176\tvalid_1's rmse: 0.221175\n",
      "[390]\ttraining's rmse: 0.219055\tvalid_1's rmse: 0.221156\n",
      "[400]\ttraining's rmse: 0.218921\tvalid_1's rmse: 0.221113\n",
      "[410]\ttraining's rmse: 0.218796\tvalid_1's rmse: 0.221079\n",
      "[420]\ttraining's rmse: 0.218672\tvalid_1's rmse: 0.221063\n",
      "[430]\ttraining's rmse: 0.218552\tvalid_1's rmse: 0.221075\n",
      "[440]\ttraining's rmse: 0.218433\tvalid_1's rmse: 0.221055\n",
      "[450]\ttraining's rmse: 0.218319\tvalid_1's rmse: 0.221049\n",
      "[460]\ttraining's rmse: 0.218196\tvalid_1's rmse: 0.221019\n",
      "[470]\ttraining's rmse: 0.218088\tvalid_1's rmse: 0.220999\n",
      "[480]\ttraining's rmse: 0.217977\tvalid_1's rmse: 0.220995\n",
      "[490]\ttraining's rmse: 0.217862\tvalid_1's rmse: 0.220969\n",
      "[500]\ttraining's rmse: 0.217746\tvalid_1's rmse: 0.220951\n",
      "[510]\ttraining's rmse: 0.217633\tvalid_1's rmse: 0.220938\n",
      "[520]\ttraining's rmse: 0.217517\tvalid_1's rmse: 0.220916\n",
      "[530]\ttraining's rmse: 0.217394\tvalid_1's rmse: 0.220895\n",
      "[540]\ttraining's rmse: 0.217278\tvalid_1's rmse: 0.220875\n",
      "[550]\ttraining's rmse: 0.217162\tvalid_1's rmse: 0.220857\n",
      "[560]\ttraining's rmse: 0.217051\tvalid_1's rmse: 0.220856\n",
      "[570]\ttraining's rmse: 0.216937\tvalid_1's rmse: 0.220845\n",
      "[580]\ttraining's rmse: 0.216834\tvalid_1's rmse: 0.220828\n",
      "[590]\ttraining's rmse: 0.216721\tvalid_1's rmse: 0.220817\n",
      "[600]\ttraining's rmse: 0.216619\tvalid_1's rmse: 0.220802\n",
      "[610]\ttraining's rmse: 0.216513\tvalid_1's rmse: 0.220781\n",
      "[620]\ttraining's rmse: 0.216405\tvalid_1's rmse: 0.220755\n",
      "[630]\ttraining's rmse: 0.216298\tvalid_1's rmse: 0.22074\n",
      "[640]\ttraining's rmse: 0.216191\tvalid_1's rmse: 0.220726\n",
      "[650]\ttraining's rmse: 0.216092\tvalid_1's rmse: 0.220721\n",
      "[660]\ttraining's rmse: 0.21599\tvalid_1's rmse: 0.22071\n",
      "[670]\ttraining's rmse: 0.215889\tvalid_1's rmse: 0.2207\n",
      "[680]\ttraining's rmse: 0.21578\tvalid_1's rmse: 0.220694\n",
      "[690]\ttraining's rmse: 0.215671\tvalid_1's rmse: 0.220686\n",
      "[700]\ttraining's rmse: 0.215563\tvalid_1's rmse: 0.220685\n",
      "[710]\ttraining's rmse: 0.215459\tvalid_1's rmse: 0.220674\n",
      "[720]\ttraining's rmse: 0.215361\tvalid_1's rmse: 0.220679\n",
      "[730]\ttraining's rmse: 0.215261\tvalid_1's rmse: 0.220662\n",
      "[740]\ttraining's rmse: 0.215157\tvalid_1's rmse: 0.220643\n",
      "[750]\ttraining's rmse: 0.215066\tvalid_1's rmse: 0.220648\n",
      "[760]\ttraining's rmse: 0.214964\tvalid_1's rmse: 0.220632\n",
      "[770]\ttraining's rmse: 0.214862\tvalid_1's rmse: 0.220626\n",
      "[780]\ttraining's rmse: 0.214758\tvalid_1's rmse: 0.220623\n",
      "[790]\ttraining's rmse: 0.214651\tvalid_1's rmse: 0.220618\n",
      "[800]\ttraining's rmse: 0.214545\tvalid_1's rmse: 0.22059\n",
      "[810]\ttraining's rmse: 0.214452\tvalid_1's rmse: 0.220595\n",
      "[820]\ttraining's rmse: 0.21436\tvalid_1's rmse: 0.220597\n",
      "[830]\ttraining's rmse: 0.214265\tvalid_1's rmse: 0.220589\n",
      "[840]\ttraining's rmse: 0.214167\tvalid_1's rmse: 0.220584\n",
      "[850]\ttraining's rmse: 0.214077\tvalid_1's rmse: 0.220583\n",
      "[860]\ttraining's rmse: 0.213981\tvalid_1's rmse: 0.220565\n",
      "[870]\ttraining's rmse: 0.213881\tvalid_1's rmse: 0.220549\n",
      "[880]\ttraining's rmse: 0.213787\tvalid_1's rmse: 0.220548\n",
      "[890]\ttraining's rmse: 0.213694\tvalid_1's rmse: 0.22053\n",
      "[900]\ttraining's rmse: 0.213593\tvalid_1's rmse: 0.220544\n",
      "[910]\ttraining's rmse: 0.213501\tvalid_1's rmse: 0.220535\n",
      "[920]\ttraining's rmse: 0.213411\tvalid_1's rmse: 0.220511\n",
      "[930]\ttraining's rmse: 0.213315\tvalid_1's rmse: 0.220526\n",
      "[940]\ttraining's rmse: 0.21322\tvalid_1's rmse: 0.220529\n",
      "[950]\ttraining's rmse: 0.213126\tvalid_1's rmse: 0.220528\n",
      "[960]\ttraining's rmse: 0.213032\tvalid_1's rmse: 0.220514\n",
      "[970]\ttraining's rmse: 0.212941\tvalid_1's rmse: 0.220517\n",
      "[980]\ttraining's rmse: 0.212856\tvalid_1's rmse: 0.220521\n",
      "[990]\ttraining's rmse: 0.212766\tvalid_1's rmse: 0.220521\n",
      "[1000]\ttraining's rmse: 0.212674\tvalid_1's rmse: 0.220516\n",
      "[1010]\ttraining's rmse: 0.212585\tvalid_1's rmse: 0.220523\n",
      "[1020]\ttraining's rmse: 0.212497\tvalid_1's rmse: 0.220522\n",
      "Early stopping, best iteration is:\n",
      "[922]\ttraining's rmse: 0.213393\tvalid_1's rmse: 0.22051\n",
      "rmse:0.22045138287639246\n",
      "params: \n",
      "max_depth: 54\n",
      "num_leaves: 77\n",
      "min_data_per_leaf: 49\n",
      "min_child_weight: 1.15211828988\n",
      "subsample: 0.859530768392\n",
      "subsample_freq: 4\n",
      "lambda_l1: 13.7508443103\n",
      "lambda_l2: 13.8173641536\n",
      "feature_fraction: 0.441556246315\n",
      "learning rate: 0.0866631136769\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.237374\tvalid_1's rmse: 0.235672\n",
      "[20]\ttraining's rmse: 0.231385\tvalid_1's rmse: 0.229419\n",
      "[30]\ttraining's rmse: 0.22905\tvalid_1's rmse: 0.227115\n",
      "[40]\ttraining's rmse: 0.227769\tvalid_1's rmse: 0.225912\n",
      "[50]\ttraining's rmse: 0.226997\tvalid_1's rmse: 0.225259\n",
      "[60]\ttraining's rmse: 0.226326\tvalid_1's rmse: 0.224709\n",
      "[70]\ttraining's rmse: 0.225757\tvalid_1's rmse: 0.224277\n",
      "[80]\ttraining's rmse: 0.225268\tvalid_1's rmse: 0.223911\n",
      "[90]\ttraining's rmse: 0.224818\tvalid_1's rmse: 0.223566\n",
      "[100]\ttraining's rmse: 0.224451\tvalid_1's rmse: 0.22332\n",
      "[110]\ttraining's rmse: 0.224117\tvalid_1's rmse: 0.223119\n",
      "[120]\ttraining's rmse: 0.223794\tvalid_1's rmse: 0.22291\n",
      "[130]\ttraining's rmse: 0.223546\tvalid_1's rmse: 0.222782\n",
      "[140]\ttraining's rmse: 0.223247\tvalid_1's rmse: 0.222606\n",
      "[150]\ttraining's rmse: 0.223\tvalid_1's rmse: 0.222463\n",
      "[160]\ttraining's rmse: 0.222733\tvalid_1's rmse: 0.222321\n",
      "[170]\ttraining's rmse: 0.22249\tvalid_1's rmse: 0.222203\n",
      "[180]\ttraining's rmse: 0.22229\tvalid_1's rmse: 0.22211\n",
      "[190]\ttraining's rmse: 0.222074\tvalid_1's rmse: 0.222015\n",
      "[200]\ttraining's rmse: 0.221877\tvalid_1's rmse: 0.221932\n",
      "[210]\ttraining's rmse: 0.221657\tvalid_1's rmse: 0.22182\n",
      "[220]\ttraining's rmse: 0.22148\tvalid_1's rmse: 0.221748\n",
      "[230]\ttraining's rmse: 0.221277\tvalid_1's rmse: 0.221631\n",
      "[240]\ttraining's rmse: 0.221097\tvalid_1's rmse: 0.221557\n",
      "[250]\ttraining's rmse: 0.220926\tvalid_1's rmse: 0.221487\n",
      "[260]\ttraining's rmse: 0.220763\tvalid_1's rmse: 0.221436\n",
      "[270]\ttraining's rmse: 0.220595\tvalid_1's rmse: 0.221374\n",
      "[280]\ttraining's rmse: 0.220443\tvalid_1's rmse: 0.22133\n",
      "[290]\ttraining's rmse: 0.220304\tvalid_1's rmse: 0.221294\n",
      "[300]\ttraining's rmse: 0.220145\tvalid_1's rmse: 0.221238\n",
      "[310]\ttraining's rmse: 0.219995\tvalid_1's rmse: 0.221187\n",
      "[320]\ttraining's rmse: 0.219847\tvalid_1's rmse: 0.221161\n",
      "[330]\ttraining's rmse: 0.219707\tvalid_1's rmse: 0.221128\n",
      "[340]\ttraining's rmse: 0.219562\tvalid_1's rmse: 0.221083\n",
      "[350]\ttraining's rmse: 0.219421\tvalid_1's rmse: 0.221037\n",
      "[360]\ttraining's rmse: 0.219289\tvalid_1's rmse: 0.221014\n",
      "[370]\ttraining's rmse: 0.219152\tvalid_1's rmse: 0.220978\n",
      "[380]\ttraining's rmse: 0.219021\tvalid_1's rmse: 0.220944\n",
      "[390]\ttraining's rmse: 0.218894\tvalid_1's rmse: 0.220919\n",
      "[400]\ttraining's rmse: 0.218764\tvalid_1's rmse: 0.220885\n",
      "[410]\ttraining's rmse: 0.218648\tvalid_1's rmse: 0.220854\n",
      "[420]\ttraining's rmse: 0.218519\tvalid_1's rmse: 0.220816\n",
      "[430]\ttraining's rmse: 0.218385\tvalid_1's rmse: 0.220785\n",
      "[440]\ttraining's rmse: 0.218273\tvalid_1's rmse: 0.22077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[450]\ttraining's rmse: 0.218144\tvalid_1's rmse: 0.220758\n",
      "[460]\ttraining's rmse: 0.218027\tvalid_1's rmse: 0.220733\n",
      "[470]\ttraining's rmse: 0.217915\tvalid_1's rmse: 0.220719\n",
      "[480]\ttraining's rmse: 0.217802\tvalid_1's rmse: 0.220705\n",
      "[490]\ttraining's rmse: 0.217684\tvalid_1's rmse: 0.220688\n",
      "[500]\ttraining's rmse: 0.217582\tvalid_1's rmse: 0.220677\n",
      "[510]\ttraining's rmse: 0.217468\tvalid_1's rmse: 0.220656\n",
      "[520]\ttraining's rmse: 0.217356\tvalid_1's rmse: 0.220641\n",
      "[530]\ttraining's rmse: 0.21725\tvalid_1's rmse: 0.220633\n",
      "[540]\ttraining's rmse: 0.217138\tvalid_1's rmse: 0.220609\n",
      "[550]\ttraining's rmse: 0.217021\tvalid_1's rmse: 0.220588\n",
      "[560]\ttraining's rmse: 0.216905\tvalid_1's rmse: 0.220577\n",
      "[570]\ttraining's rmse: 0.21679\tvalid_1's rmse: 0.220547\n",
      "[580]\ttraining's rmse: 0.216686\tvalid_1's rmse: 0.220533\n",
      "[590]\ttraining's rmse: 0.216579\tvalid_1's rmse: 0.220514\n",
      "[600]\ttraining's rmse: 0.21647\tvalid_1's rmse: 0.220508\n",
      "[610]\ttraining's rmse: 0.216355\tvalid_1's rmse: 0.220483\n",
      "[620]\ttraining's rmse: 0.216247\tvalid_1's rmse: 0.220473\n",
      "[630]\ttraining's rmse: 0.216146\tvalid_1's rmse: 0.220461\n",
      "[640]\ttraining's rmse: 0.216046\tvalid_1's rmse: 0.220458\n",
      "[650]\ttraining's rmse: 0.215935\tvalid_1's rmse: 0.220446\n",
      "[660]\ttraining's rmse: 0.215838\tvalid_1's rmse: 0.220432\n",
      "[670]\ttraining's rmse: 0.21574\tvalid_1's rmse: 0.22042\n",
      "[680]\ttraining's rmse: 0.215639\tvalid_1's rmse: 0.220414\n",
      "[690]\ttraining's rmse: 0.215531\tvalid_1's rmse: 0.220394\n",
      "[700]\ttraining's rmse: 0.215423\tvalid_1's rmse: 0.220379\n",
      "[710]\ttraining's rmse: 0.215325\tvalid_1's rmse: 0.220372\n",
      "[720]\ttraining's rmse: 0.215225\tvalid_1's rmse: 0.220362\n",
      "[730]\ttraining's rmse: 0.215116\tvalid_1's rmse: 0.220343\n",
      "[740]\ttraining's rmse: 0.215014\tvalid_1's rmse: 0.220338\n",
      "[750]\ttraining's rmse: 0.214922\tvalid_1's rmse: 0.220337\n",
      "[760]\ttraining's rmse: 0.214827\tvalid_1's rmse: 0.220333\n",
      "[770]\ttraining's rmse: 0.214732\tvalid_1's rmse: 0.220327\n",
      "[780]\ttraining's rmse: 0.214631\tvalid_1's rmse: 0.220313\n",
      "[790]\ttraining's rmse: 0.214538\tvalid_1's rmse: 0.220308\n",
      "[800]\ttraining's rmse: 0.214447\tvalid_1's rmse: 0.220305\n",
      "[810]\ttraining's rmse: 0.214354\tvalid_1's rmse: 0.220297\n",
      "[820]\ttraining's rmse: 0.214263\tvalid_1's rmse: 0.220294\n",
      "[830]\ttraining's rmse: 0.214163\tvalid_1's rmse: 0.220289\n",
      "[840]\ttraining's rmse: 0.21407\tvalid_1's rmse: 0.220278\n",
      "[850]\ttraining's rmse: 0.213977\tvalid_1's rmse: 0.220277\n",
      "[860]\ttraining's rmse: 0.213884\tvalid_1's rmse: 0.22027\n",
      "[870]\ttraining's rmse: 0.213786\tvalid_1's rmse: 0.22026\n",
      "[880]\ttraining's rmse: 0.213692\tvalid_1's rmse: 0.220248\n",
      "[890]\ttraining's rmse: 0.213598\tvalid_1's rmse: 0.220246\n",
      "[900]\ttraining's rmse: 0.213501\tvalid_1's rmse: 0.220243\n",
      "[910]\ttraining's rmse: 0.213404\tvalid_1's rmse: 0.220234\n",
      "[920]\ttraining's rmse: 0.21331\tvalid_1's rmse: 0.220236\n",
      "[930]\ttraining's rmse: 0.213214\tvalid_1's rmse: 0.22023\n",
      "[940]\ttraining's rmse: 0.213126\tvalid_1's rmse: 0.220238\n",
      "[950]\ttraining's rmse: 0.213038\tvalid_1's rmse: 0.220231\n",
      "[960]\ttraining's rmse: 0.212945\tvalid_1's rmse: 0.220226\n",
      "[970]\ttraining's rmse: 0.21285\tvalid_1's rmse: 0.220214\n",
      "[980]\ttraining's rmse: 0.212756\tvalid_1's rmse: 0.220205\n",
      "[990]\ttraining's rmse: 0.212669\tvalid_1's rmse: 0.220203\n",
      "[1000]\ttraining's rmse: 0.212579\tvalid_1's rmse: 0.2202\n",
      "[1010]\ttraining's rmse: 0.212488\tvalid_1's rmse: 0.220189\n",
      "[1020]\ttraining's rmse: 0.212401\tvalid_1's rmse: 0.220175\n",
      "[1030]\ttraining's rmse: 0.212309\tvalid_1's rmse: 0.220167\n",
      "[1040]\ttraining's rmse: 0.212211\tvalid_1's rmse: 0.220155\n",
      "[1050]\ttraining's rmse: 0.212125\tvalid_1's rmse: 0.220153\n",
      "[1060]\ttraining's rmse: 0.212035\tvalid_1's rmse: 0.220146\n",
      "[1070]\ttraining's rmse: 0.211952\tvalid_1's rmse: 0.220139\n",
      "[1080]\ttraining's rmse: 0.211864\tvalid_1's rmse: 0.220144\n",
      "[1090]\ttraining's rmse: 0.211774\tvalid_1's rmse: 0.220142\n",
      "[1100]\ttraining's rmse: 0.211694\tvalid_1's rmse: 0.220136\n",
      "[1110]\ttraining's rmse: 0.211612\tvalid_1's rmse: 0.220133\n",
      "[1120]\ttraining's rmse: 0.211529\tvalid_1's rmse: 0.220124\n",
      "[1130]\ttraining's rmse: 0.211452\tvalid_1's rmse: 0.220116\n",
      "[1140]\ttraining's rmse: 0.211376\tvalid_1's rmse: 0.220123\n",
      "[1150]\ttraining's rmse: 0.211291\tvalid_1's rmse: 0.220123\n",
      "[1160]\ttraining's rmse: 0.211212\tvalid_1's rmse: 0.220124\n",
      "[1170]\ttraining's rmse: 0.211125\tvalid_1's rmse: 0.220116\n",
      "[1180]\ttraining's rmse: 0.211043\tvalid_1's rmse: 0.220106\n",
      "[1190]\ttraining's rmse: 0.210964\tvalid_1's rmse: 0.220098\n",
      "[1200]\ttraining's rmse: 0.210879\tvalid_1's rmse: 0.220098\n",
      "[1210]\ttraining's rmse: 0.210795\tvalid_1's rmse: 0.220092\n",
      "[1220]\ttraining's rmse: 0.210713\tvalid_1's rmse: 0.22009\n",
      "[1230]\ttraining's rmse: 0.210626\tvalid_1's rmse: 0.220088\n",
      "[1240]\ttraining's rmse: 0.210539\tvalid_1's rmse: 0.220077\n",
      "[1250]\ttraining's rmse: 0.210454\tvalid_1's rmse: 0.220078\n",
      "[1260]\ttraining's rmse: 0.210373\tvalid_1's rmse: 0.220076\n",
      "[1270]\ttraining's rmse: 0.21029\tvalid_1's rmse: 0.220073\n",
      "[1280]\ttraining's rmse: 0.210206\tvalid_1's rmse: 0.220071\n",
      "[1290]\ttraining's rmse: 0.210122\tvalid_1's rmse: 0.220076\n",
      "[1300]\ttraining's rmse: 0.210044\tvalid_1's rmse: 0.220063\n",
      "[1310]\ttraining's rmse: 0.209971\tvalid_1's rmse: 0.22006\n",
      "[1320]\ttraining's rmse: 0.209885\tvalid_1's rmse: 0.22006\n",
      "[1330]\ttraining's rmse: 0.209806\tvalid_1's rmse: 0.220064\n",
      "[1340]\ttraining's rmse: 0.209723\tvalid_1's rmse: 0.22007\n",
      "[1350]\ttraining's rmse: 0.20964\tvalid_1's rmse: 0.220068\n",
      "[1360]\ttraining's rmse: 0.209551\tvalid_1's rmse: 0.220064\n",
      "[1370]\ttraining's rmse: 0.209475\tvalid_1's rmse: 0.220064\n",
      "[1380]\ttraining's rmse: 0.209396\tvalid_1's rmse: 0.220064\n",
      "[1390]\ttraining's rmse: 0.209313\tvalid_1's rmse: 0.220062\n",
      "[1400]\ttraining's rmse: 0.209233\tvalid_1's rmse: 0.220063\n",
      "[1410]\ttraining's rmse: 0.209153\tvalid_1's rmse: 0.220058\n",
      "Early stopping, best iteration is:\n",
      "[1315]\ttraining's rmse: 0.209926\tvalid_1's rmse: 0.220055\n",
      "rmse:0.2200039752548931\n",
      "params: \n",
      "max_depth: 10\n",
      "num_leaves: 74\n",
      "min_data_per_leaf: 66\n",
      "min_child_weight: 1.69070293243\n",
      "subsample: 0.679419049285\n",
      "subsample_freq: 3\n",
      "lambda_l1: 8.29232219831\n",
      "lambda_l2: 2.47410690363\n",
      "feature_fraction: 0.584904046374\n",
      "learning rate: 0.0787865057637\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.238198\tvalid_1's rmse: 0.236505\n",
      "[20]\ttraining's rmse: 0.231812\tvalid_1's rmse: 0.229838\n",
      "[30]\ttraining's rmse: 0.229352\tvalid_1's rmse: 0.227331\n",
      "[40]\ttraining's rmse: 0.228082\tvalid_1's rmse: 0.226129\n",
      "[50]\ttraining's rmse: 0.227209\tvalid_1's rmse: 0.225379\n",
      "[60]\ttraining's rmse: 0.226584\tvalid_1's rmse: 0.224875\n",
      "[70]\ttraining's rmse: 0.226044\tvalid_1's rmse: 0.224461\n",
      "[80]\ttraining's rmse: 0.225535\tvalid_1's rmse: 0.224067\n",
      "[90]\ttraining's rmse: 0.225108\tvalid_1's rmse: 0.223767\n",
      "[100]\ttraining's rmse: 0.224745\tvalid_1's rmse: 0.223515\n",
      "[110]\ttraining's rmse: 0.224452\tvalid_1's rmse: 0.223321\n",
      "[120]\ttraining's rmse: 0.224153\tvalid_1's rmse: 0.223139\n",
      "[130]\ttraining's rmse: 0.223905\tvalid_1's rmse: 0.222999\n",
      "[140]\ttraining's rmse: 0.223607\tvalid_1's rmse: 0.222814\n",
      "[150]\ttraining's rmse: 0.223356\tvalid_1's rmse: 0.22268\n",
      "[160]\ttraining's rmse: 0.22311\tvalid_1's rmse: 0.222567\n",
      "[170]\ttraining's rmse: 0.222874\tvalid_1's rmse: 0.222435\n",
      "[180]\ttraining's rmse: 0.222629\tvalid_1's rmse: 0.222311\n",
      "[190]\ttraining's rmse: 0.222392\tvalid_1's rmse: 0.222173\n",
      "[200]\ttraining's rmse: 0.222199\tvalid_1's rmse: 0.222081\n",
      "[210]\ttraining's rmse: 0.221976\tvalid_1's rmse: 0.221959\n",
      "[220]\ttraining's rmse: 0.221802\tvalid_1's rmse: 0.22187\n",
      "[230]\ttraining's rmse: 0.22162\tvalid_1's rmse: 0.221796\n",
      "[240]\ttraining's rmse: 0.221446\tvalid_1's rmse: 0.221722\n",
      "[250]\ttraining's rmse: 0.221256\tvalid_1's rmse: 0.221643\n",
      "[260]\ttraining's rmse: 0.221069\tvalid_1's rmse: 0.221575\n",
      "[270]\ttraining's rmse: 0.220918\tvalid_1's rmse: 0.22153\n",
      "[280]\ttraining's rmse: 0.220762\tvalid_1's rmse: 0.221485\n",
      "[290]\ttraining's rmse: 0.220613\tvalid_1's rmse: 0.221452\n",
      "[300]\ttraining's rmse: 0.220456\tvalid_1's rmse: 0.221403\n",
      "[310]\ttraining's rmse: 0.220321\tvalid_1's rmse: 0.221367\n",
      "[320]\ttraining's rmse: 0.220157\tvalid_1's rmse: 0.221308\n",
      "[330]\ttraining's rmse: 0.220013\tvalid_1's rmse: 0.221277\n",
      "[340]\ttraining's rmse: 0.219874\tvalid_1's rmse: 0.221252\n",
      "[350]\ttraining's rmse: 0.21973\tvalid_1's rmse: 0.221207\n",
      "[360]\ttraining's rmse: 0.219593\tvalid_1's rmse: 0.221159\n",
      "[370]\ttraining's rmse: 0.219469\tvalid_1's rmse: 0.221138\n",
      "[380]\ttraining's rmse: 0.219331\tvalid_1's rmse: 0.221107\n",
      "[390]\ttraining's rmse: 0.219194\tvalid_1's rmse: 0.221067\n",
      "[400]\ttraining's rmse: 0.219058\tvalid_1's rmse: 0.221031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[410]\ttraining's rmse: 0.218925\tvalid_1's rmse: 0.220998\n",
      "[420]\ttraining's rmse: 0.218798\tvalid_1's rmse: 0.220969\n",
      "[430]\ttraining's rmse: 0.218676\tvalid_1's rmse: 0.220952\n",
      "[440]\ttraining's rmse: 0.21855\tvalid_1's rmse: 0.220933\n",
      "[450]\ttraining's rmse: 0.218437\tvalid_1's rmse: 0.220912\n",
      "[460]\ttraining's rmse: 0.218299\tvalid_1's rmse: 0.220889\n",
      "[470]\ttraining's rmse: 0.218187\tvalid_1's rmse: 0.220873\n",
      "[480]\ttraining's rmse: 0.218068\tvalid_1's rmse: 0.220862\n",
      "[490]\ttraining's rmse: 0.217948\tvalid_1's rmse: 0.220832\n",
      "[500]\ttraining's rmse: 0.217824\tvalid_1's rmse: 0.220806\n",
      "[510]\ttraining's rmse: 0.217703\tvalid_1's rmse: 0.220783\n",
      "[520]\ttraining's rmse: 0.217583\tvalid_1's rmse: 0.220762\n",
      "[530]\ttraining's rmse: 0.217469\tvalid_1's rmse: 0.220739\n",
      "[540]\ttraining's rmse: 0.217349\tvalid_1's rmse: 0.220719\n",
      "[550]\ttraining's rmse: 0.217223\tvalid_1's rmse: 0.220687\n",
      "[560]\ttraining's rmse: 0.2171\tvalid_1's rmse: 0.220667\n",
      "[570]\ttraining's rmse: 0.216988\tvalid_1's rmse: 0.220654\n",
      "[580]\ttraining's rmse: 0.216861\tvalid_1's rmse: 0.220629\n",
      "[590]\ttraining's rmse: 0.21675\tvalid_1's rmse: 0.220613\n",
      "[600]\ttraining's rmse: 0.216639\tvalid_1's rmse: 0.220592\n",
      "[610]\ttraining's rmse: 0.216524\tvalid_1's rmse: 0.22057\n",
      "[620]\ttraining's rmse: 0.216408\tvalid_1's rmse: 0.220543\n",
      "[630]\ttraining's rmse: 0.216304\tvalid_1's rmse: 0.220535\n",
      "[640]\ttraining's rmse: 0.216198\tvalid_1's rmse: 0.220517\n",
      "[650]\ttraining's rmse: 0.216087\tvalid_1's rmse: 0.220495\n",
      "[660]\ttraining's rmse: 0.215986\tvalid_1's rmse: 0.220491\n",
      "[670]\ttraining's rmse: 0.215877\tvalid_1's rmse: 0.220493\n",
      "[680]\ttraining's rmse: 0.215766\tvalid_1's rmse: 0.22049\n",
      "[690]\ttraining's rmse: 0.215663\tvalid_1's rmse: 0.220492\n",
      "[700]\ttraining's rmse: 0.215555\tvalid_1's rmse: 0.220475\n",
      "[710]\ttraining's rmse: 0.215441\tvalid_1's rmse: 0.22045\n",
      "[720]\ttraining's rmse: 0.215331\tvalid_1's rmse: 0.220448\n",
      "[730]\ttraining's rmse: 0.215234\tvalid_1's rmse: 0.220434\n",
      "[740]\ttraining's rmse: 0.215139\tvalid_1's rmse: 0.220431\n",
      "[750]\ttraining's rmse: 0.215034\tvalid_1's rmse: 0.220418\n",
      "[760]\ttraining's rmse: 0.214935\tvalid_1's rmse: 0.220415\n",
      "[770]\ttraining's rmse: 0.214834\tvalid_1's rmse: 0.220411\n",
      "[780]\ttraining's rmse: 0.214732\tvalid_1's rmse: 0.220406\n",
      "[790]\ttraining's rmse: 0.214622\tvalid_1's rmse: 0.220398\n",
      "[800]\ttraining's rmse: 0.214522\tvalid_1's rmse: 0.220394\n",
      "[810]\ttraining's rmse: 0.21441\tvalid_1's rmse: 0.220378\n",
      "[820]\ttraining's rmse: 0.214312\tvalid_1's rmse: 0.22037\n",
      "[830]\ttraining's rmse: 0.21421\tvalid_1's rmse: 0.220343\n",
      "[840]\ttraining's rmse: 0.214118\tvalid_1's rmse: 0.220336\n",
      "[850]\ttraining's rmse: 0.214014\tvalid_1's rmse: 0.220339\n",
      "[860]\ttraining's rmse: 0.213917\tvalid_1's rmse: 0.220328\n",
      "[870]\ttraining's rmse: 0.213818\tvalid_1's rmse: 0.220328\n",
      "[880]\ttraining's rmse: 0.213722\tvalid_1's rmse: 0.220324\n",
      "[890]\ttraining's rmse: 0.213625\tvalid_1's rmse: 0.220308\n",
      "[900]\ttraining's rmse: 0.213518\tvalid_1's rmse: 0.220293\n",
      "[910]\ttraining's rmse: 0.213429\tvalid_1's rmse: 0.220292\n",
      "[920]\ttraining's rmse: 0.213331\tvalid_1's rmse: 0.220294\n",
      "[930]\ttraining's rmse: 0.213238\tvalid_1's rmse: 0.220288\n",
      "[940]\ttraining's rmse: 0.213145\tvalid_1's rmse: 0.220284\n",
      "[950]\ttraining's rmse: 0.213049\tvalid_1's rmse: 0.220275\n",
      "[960]\ttraining's rmse: 0.212956\tvalid_1's rmse: 0.220264\n",
      "[970]\ttraining's rmse: 0.212859\tvalid_1's rmse: 0.220259\n",
      "[980]\ttraining's rmse: 0.21277\tvalid_1's rmse: 0.220256\n",
      "[990]\ttraining's rmse: 0.212676\tvalid_1's rmse: 0.220249\n",
      "[1000]\ttraining's rmse: 0.212572\tvalid_1's rmse: 0.220226\n",
      "[1010]\ttraining's rmse: 0.212482\tvalid_1's rmse: 0.220233\n",
      "[1020]\ttraining's rmse: 0.212393\tvalid_1's rmse: 0.220234\n",
      "[1030]\ttraining's rmse: 0.2123\tvalid_1's rmse: 0.220233\n",
      "[1040]\ttraining's rmse: 0.212202\tvalid_1's rmse: 0.220232\n",
      "[1050]\ttraining's rmse: 0.212107\tvalid_1's rmse: 0.220222\n",
      "[1060]\ttraining's rmse: 0.212007\tvalid_1's rmse: 0.220206\n",
      "[1070]\ttraining's rmse: 0.211923\tvalid_1's rmse: 0.220204\n",
      "[1080]\ttraining's rmse: 0.211826\tvalid_1's rmse: 0.220197\n",
      "[1090]\ttraining's rmse: 0.211732\tvalid_1's rmse: 0.220191\n",
      "[1100]\ttraining's rmse: 0.211642\tvalid_1's rmse: 0.22019\n",
      "[1110]\ttraining's rmse: 0.21156\tvalid_1's rmse: 0.220189\n",
      "[1120]\ttraining's rmse: 0.211465\tvalid_1's rmse: 0.220185\n",
      "[1130]\ttraining's rmse: 0.211377\tvalid_1's rmse: 0.22019\n",
      "[1140]\ttraining's rmse: 0.21129\tvalid_1's rmse: 0.220188\n",
      "[1150]\ttraining's rmse: 0.211194\tvalid_1's rmse: 0.220192\n",
      "[1160]\ttraining's rmse: 0.211095\tvalid_1's rmse: 0.220191\n",
      "[1170]\ttraining's rmse: 0.21101\tvalid_1's rmse: 0.220193\n",
      "[1180]\ttraining's rmse: 0.210924\tvalid_1's rmse: 0.220195\n",
      "[1190]\ttraining's rmse: 0.210837\tvalid_1's rmse: 0.220186\n",
      "[1200]\ttraining's rmse: 0.210745\tvalid_1's rmse: 0.220179\n",
      "[1210]\ttraining's rmse: 0.210656\tvalid_1's rmse: 0.220181\n",
      "[1220]\ttraining's rmse: 0.210571\tvalid_1's rmse: 0.220182\n",
      "[1230]\ttraining's rmse: 0.210486\tvalid_1's rmse: 0.220181\n",
      "[1240]\ttraining's rmse: 0.210399\tvalid_1's rmse: 0.220173\n",
      "[1250]\ttraining's rmse: 0.21031\tvalid_1's rmse: 0.220168\n",
      "[1260]\ttraining's rmse: 0.210224\tvalid_1's rmse: 0.220172\n",
      "[1270]\ttraining's rmse: 0.210133\tvalid_1's rmse: 0.220168\n",
      "[1280]\ttraining's rmse: 0.210049\tvalid_1's rmse: 0.220172\n",
      "[1290]\ttraining's rmse: 0.209966\tvalid_1's rmse: 0.220179\n",
      "[1300]\ttraining's rmse: 0.209874\tvalid_1's rmse: 0.220178\n",
      "[1310]\ttraining's rmse: 0.209788\tvalid_1's rmse: 0.220178\n",
      "[1320]\ttraining's rmse: 0.209707\tvalid_1's rmse: 0.220162\n",
      "[1330]\ttraining's rmse: 0.209616\tvalid_1's rmse: 0.220161\n",
      "[1340]\ttraining's rmse: 0.209527\tvalid_1's rmse: 0.220179\n",
      "[1350]\ttraining's rmse: 0.209434\tvalid_1's rmse: 0.220179\n",
      "[1360]\ttraining's rmse: 0.209351\tvalid_1's rmse: 0.220169\n",
      "[1370]\ttraining's rmse: 0.209271\tvalid_1's rmse: 0.220166\n",
      "[1380]\ttraining's rmse: 0.209188\tvalid_1's rmse: 0.220165\n",
      "[1390]\ttraining's rmse: 0.209103\tvalid_1's rmse: 0.220155\n",
      "[1400]\ttraining's rmse: 0.209015\tvalid_1's rmse: 0.220153\n",
      "[1410]\ttraining's rmse: 0.208929\tvalid_1's rmse: 0.220149\n",
      "[1420]\ttraining's rmse: 0.208844\tvalid_1's rmse: 0.220154\n",
      "[1430]\ttraining's rmse: 0.208756\tvalid_1's rmse: 0.220145\n",
      "[1440]\ttraining's rmse: 0.208674\tvalid_1's rmse: 0.220143\n",
      "[1450]\ttraining's rmse: 0.208592\tvalid_1's rmse: 0.220147\n",
      "[1460]\ttraining's rmse: 0.208514\tvalid_1's rmse: 0.220143\n",
      "[1470]\ttraining's rmse: 0.208428\tvalid_1's rmse: 0.220137\n",
      "[1480]\ttraining's rmse: 0.208341\tvalid_1's rmse: 0.220147\n",
      "[1490]\ttraining's rmse: 0.208253\tvalid_1's rmse: 0.220138\n",
      "[1500]\ttraining's rmse: 0.20817\tvalid_1's rmse: 0.220132\n",
      "[1510]\ttraining's rmse: 0.208081\tvalid_1's rmse: 0.220141\n",
      "[1520]\ttraining's rmse: 0.208001\tvalid_1's rmse: 0.220139\n",
      "[1530]\ttraining's rmse: 0.207921\tvalid_1's rmse: 0.22014\n",
      "[1540]\ttraining's rmse: 0.207833\tvalid_1's rmse: 0.220131\n",
      "[1550]\ttraining's rmse: 0.207752\tvalid_1's rmse: 0.220134\n",
      "[1560]\ttraining's rmse: 0.20767\tvalid_1's rmse: 0.220129\n",
      "[1570]\ttraining's rmse: 0.20759\tvalid_1's rmse: 0.220123\n",
      "[1580]\ttraining's rmse: 0.207515\tvalid_1's rmse: 0.220119\n",
      "[1590]\ttraining's rmse: 0.207435\tvalid_1's rmse: 0.220124\n",
      "[1600]\ttraining's rmse: 0.20735\tvalid_1's rmse: 0.220127\n",
      "[1610]\ttraining's rmse: 0.207269\tvalid_1's rmse: 0.220119\n",
      "[1620]\ttraining's rmse: 0.207179\tvalid_1's rmse: 0.220123\n",
      "[1630]\ttraining's rmse: 0.207096\tvalid_1's rmse: 0.220122\n",
      "[1640]\ttraining's rmse: 0.207015\tvalid_1's rmse: 0.220113\n",
      "[1650]\ttraining's rmse: 0.206931\tvalid_1's rmse: 0.220119\n",
      "[1660]\ttraining's rmse: 0.206854\tvalid_1's rmse: 0.220128\n",
      "[1670]\ttraining's rmse: 0.206774\tvalid_1's rmse: 0.220124\n",
      "[1680]\ttraining's rmse: 0.206689\tvalid_1's rmse: 0.220122\n",
      "[1690]\ttraining's rmse: 0.206605\tvalid_1's rmse: 0.22012\n",
      "[1700]\ttraining's rmse: 0.206524\tvalid_1's rmse: 0.22011\n",
      "[1710]\ttraining's rmse: 0.206449\tvalid_1's rmse: 0.220109\n",
      "[1720]\ttraining's rmse: 0.206373\tvalid_1's rmse: 0.220118\n",
      "[1730]\ttraining's rmse: 0.206301\tvalid_1's rmse: 0.22012\n",
      "[1740]\ttraining's rmse: 0.206222\tvalid_1's rmse: 0.220126\n",
      "[1750]\ttraining's rmse: 0.20614\tvalid_1's rmse: 0.220129\n",
      "[1760]\ttraining's rmse: 0.20606\tvalid_1's rmse: 0.220127\n",
      "[1770]\ttraining's rmse: 0.205983\tvalid_1's rmse: 0.22013\n",
      "[1780]\ttraining's rmse: 0.205906\tvalid_1's rmse: 0.220132\n",
      "[1790]\ttraining's rmse: 0.205826\tvalid_1's rmse: 0.220135\n",
      "[1800]\ttraining's rmse: 0.205748\tvalid_1's rmse: 0.220135\n",
      "[1810]\ttraining's rmse: 0.205667\tvalid_1's rmse: 0.220132\n",
      "Early stopping, best iteration is:\n",
      "[1713]\ttraining's rmse: 0.206425\tvalid_1's rmse: 0.220107\n",
      "rmse:0.22004206496049264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: \n",
      "max_depth: 60\n",
      "num_leaves: 42\n",
      "min_data_per_leaf: 77\n",
      "min_child_weight: 0.367226622965\n",
      "subsample: 0.71577452492\n",
      "subsample_freq: 1\n",
      "lambda_l1: 13.3884228105\n",
      "lambda_l2: 3.45185815455\n",
      "feature_fraction: 0.869544606361\n",
      "learning rate: 0.0736512563215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.239323\tvalid_1's rmse: 0.23767\n",
      "[20]\ttraining's rmse: 0.233152\tvalid_1's rmse: 0.231118\n",
      "[30]\ttraining's rmse: 0.230867\tvalid_1's rmse: 0.228725\n",
      "[40]\ttraining's rmse: 0.22966\tvalid_1's rmse: 0.227503\n",
      "[50]\ttraining's rmse: 0.228805\tvalid_1's rmse: 0.22668\n",
      "[60]\ttraining's rmse: 0.228233\tvalid_1's rmse: 0.226155\n",
      "[70]\ttraining's rmse: 0.22775\tvalid_1's rmse: 0.225713\n",
      "[80]\ttraining's rmse: 0.2273\tvalid_1's rmse: 0.225321\n",
      "[90]\ttraining's rmse: 0.226918\tvalid_1's rmse: 0.224987\n",
      "[100]\ttraining's rmse: 0.226566\tvalid_1's rmse: 0.224715\n",
      "[110]\ttraining's rmse: 0.226267\tvalid_1's rmse: 0.224488\n",
      "[120]\ttraining's rmse: 0.226013\tvalid_1's rmse: 0.224306\n",
      "[130]\ttraining's rmse: 0.225762\tvalid_1's rmse: 0.22414\n",
      "[140]\ttraining's rmse: 0.225512\tvalid_1's rmse: 0.223943\n",
      "[150]\ttraining's rmse: 0.225288\tvalid_1's rmse: 0.223783\n",
      "[160]\ttraining's rmse: 0.225041\tvalid_1's rmse: 0.22361\n",
      "[170]\ttraining's rmse: 0.224832\tvalid_1's rmse: 0.22347\n",
      "[180]\ttraining's rmse: 0.224635\tvalid_1's rmse: 0.223334\n",
      "[190]\ttraining's rmse: 0.224465\tvalid_1's rmse: 0.223226\n",
      "[200]\ttraining's rmse: 0.224297\tvalid_1's rmse: 0.223117\n",
      "[210]\ttraining's rmse: 0.224129\tvalid_1's rmse: 0.223001\n",
      "[220]\ttraining's rmse: 0.223963\tvalid_1's rmse: 0.222895\n",
      "[230]\ttraining's rmse: 0.223802\tvalid_1's rmse: 0.222795\n",
      "[240]\ttraining's rmse: 0.223662\tvalid_1's rmse: 0.222716\n",
      "[250]\ttraining's rmse: 0.223529\tvalid_1's rmse: 0.222644\n",
      "[260]\ttraining's rmse: 0.223391\tvalid_1's rmse: 0.222568\n",
      "[270]\ttraining's rmse: 0.223252\tvalid_1's rmse: 0.22249\n",
      "[280]\ttraining's rmse: 0.223121\tvalid_1's rmse: 0.222432\n",
      "[290]\ttraining's rmse: 0.223007\tvalid_1's rmse: 0.222378\n",
      "[300]\ttraining's rmse: 0.222893\tvalid_1's rmse: 0.222334\n",
      "[310]\ttraining's rmse: 0.222778\tvalid_1's rmse: 0.222278\n",
      "[320]\ttraining's rmse: 0.222679\tvalid_1's rmse: 0.222236\n",
      "[330]\ttraining's rmse: 0.22257\tvalid_1's rmse: 0.222183\n",
      "[340]\ttraining's rmse: 0.222451\tvalid_1's rmse: 0.222114\n",
      "[350]\ttraining's rmse: 0.222347\tvalid_1's rmse: 0.222076\n",
      "[360]\ttraining's rmse: 0.222243\tvalid_1's rmse: 0.222026\n",
      "[370]\ttraining's rmse: 0.222144\tvalid_1's rmse: 0.22199\n",
      "[380]\ttraining's rmse: 0.222049\tvalid_1's rmse: 0.221949\n",
      "[390]\ttraining's rmse: 0.221956\tvalid_1's rmse: 0.221907\n",
      "[400]\ttraining's rmse: 0.221864\tvalid_1's rmse: 0.22187\n",
      "[410]\ttraining's rmse: 0.22177\tvalid_1's rmse: 0.221842\n",
      "[420]\ttraining's rmse: 0.221678\tvalid_1's rmse: 0.221806\n",
      "[430]\ttraining's rmse: 0.221584\tvalid_1's rmse: 0.221769\n",
      "[440]\ttraining's rmse: 0.221496\tvalid_1's rmse: 0.22174\n",
      "[450]\ttraining's rmse: 0.221409\tvalid_1's rmse: 0.221708\n",
      "[460]\ttraining's rmse: 0.221321\tvalid_1's rmse: 0.221686\n",
      "[470]\ttraining's rmse: 0.221231\tvalid_1's rmse: 0.221654\n",
      "[480]\ttraining's rmse: 0.221153\tvalid_1's rmse: 0.221642\n",
      "[490]\ttraining's rmse: 0.221065\tvalid_1's rmse: 0.221611\n",
      "[500]\ttraining's rmse: 0.22097\tvalid_1's rmse: 0.221565\n",
      "[510]\ttraining's rmse: 0.22088\tvalid_1's rmse: 0.22153\n",
      "[520]\ttraining's rmse: 0.220802\tvalid_1's rmse: 0.221498\n",
      "[530]\ttraining's rmse: 0.220721\tvalid_1's rmse: 0.22148\n",
      "[540]\ttraining's rmse: 0.220637\tvalid_1's rmse: 0.221451\n",
      "[550]\ttraining's rmse: 0.220567\tvalid_1's rmse: 0.221437\n",
      "[560]\ttraining's rmse: 0.220484\tvalid_1's rmse: 0.221417\n",
      "[570]\ttraining's rmse: 0.220412\tvalid_1's rmse: 0.221399\n",
      "[580]\ttraining's rmse: 0.220334\tvalid_1's rmse: 0.221377\n",
      "[590]\ttraining's rmse: 0.220263\tvalid_1's rmse: 0.221361\n",
      "[600]\ttraining's rmse: 0.22018\tvalid_1's rmse: 0.221333\n",
      "[610]\ttraining's rmse: 0.220093\tvalid_1's rmse: 0.221299\n",
      "[620]\ttraining's rmse: 0.220023\tvalid_1's rmse: 0.221288\n",
      "[630]\ttraining's rmse: 0.219945\tvalid_1's rmse: 0.22126\n",
      "[640]\ttraining's rmse: 0.219869\tvalid_1's rmse: 0.22124\n",
      "[650]\ttraining's rmse: 0.219793\tvalid_1's rmse: 0.221225\n",
      "[660]\ttraining's rmse: 0.219724\tvalid_1's rmse: 0.221203\n",
      "[670]\ttraining's rmse: 0.219648\tvalid_1's rmse: 0.221182\n",
      "[680]\ttraining's rmse: 0.219575\tvalid_1's rmse: 0.221168\n",
      "[690]\ttraining's rmse: 0.219505\tvalid_1's rmse: 0.221149\n",
      "[700]\ttraining's rmse: 0.219439\tvalid_1's rmse: 0.221142\n",
      "[710]\ttraining's rmse: 0.219377\tvalid_1's rmse: 0.22113\n",
      "[720]\ttraining's rmse: 0.219314\tvalid_1's rmse: 0.221115\n",
      "[730]\ttraining's rmse: 0.219244\tvalid_1's rmse: 0.221106\n",
      "[740]\ttraining's rmse: 0.219188\tvalid_1's rmse: 0.221097\n",
      "[750]\ttraining's rmse: 0.219114\tvalid_1's rmse: 0.221089\n",
      "[760]\ttraining's rmse: 0.219039\tvalid_1's rmse: 0.221063\n",
      "[770]\ttraining's rmse: 0.21898\tvalid_1's rmse: 0.221052\n",
      "[780]\ttraining's rmse: 0.218913\tvalid_1's rmse: 0.221032\n",
      "[790]\ttraining's rmse: 0.21885\tvalid_1's rmse: 0.221021\n",
      "[800]\ttraining's rmse: 0.218784\tvalid_1's rmse: 0.221013\n",
      "[810]\ttraining's rmse: 0.218715\tvalid_1's rmse: 0.220988\n",
      "[820]\ttraining's rmse: 0.218655\tvalid_1's rmse: 0.22098\n",
      "[830]\ttraining's rmse: 0.218593\tvalid_1's rmse: 0.220962\n",
      "[840]\ttraining's rmse: 0.218534\tvalid_1's rmse: 0.220952\n",
      "[850]\ttraining's rmse: 0.21847\tvalid_1's rmse: 0.22094\n",
      "[860]\ttraining's rmse: 0.218404\tvalid_1's rmse: 0.22093\n",
      "[870]\ttraining's rmse: 0.218341\tvalid_1's rmse: 0.220916\n",
      "[880]\ttraining's rmse: 0.218271\tvalid_1's rmse: 0.220899\n",
      "[890]\ttraining's rmse: 0.218205\tvalid_1's rmse: 0.220884\n",
      "[900]\ttraining's rmse: 0.218141\tvalid_1's rmse: 0.220868\n",
      "[910]\ttraining's rmse: 0.21808\tvalid_1's rmse: 0.220858\n",
      "[920]\ttraining's rmse: 0.218017\tvalid_1's rmse: 0.220841\n",
      "[930]\ttraining's rmse: 0.217957\tvalid_1's rmse: 0.220833\n",
      "[940]\ttraining's rmse: 0.217899\tvalid_1's rmse: 0.22082\n",
      "[950]\ttraining's rmse: 0.217838\tvalid_1's rmse: 0.220808\n",
      "[960]\ttraining's rmse: 0.217773\tvalid_1's rmse: 0.22079\n",
      "[970]\ttraining's rmse: 0.217713\tvalid_1's rmse: 0.220783\n",
      "[980]\ttraining's rmse: 0.217659\tvalid_1's rmse: 0.220785\n",
      "[990]\ttraining's rmse: 0.217602\tvalid_1's rmse: 0.220774\n",
      "[1000]\ttraining's rmse: 0.217544\tvalid_1's rmse: 0.220759\n",
      "[1010]\ttraining's rmse: 0.217487\tvalid_1's rmse: 0.220745\n",
      "[1020]\ttraining's rmse: 0.21743\tvalid_1's rmse: 0.220736\n",
      "[1030]\ttraining's rmse: 0.217369\tvalid_1's rmse: 0.220719\n",
      "[1040]\ttraining's rmse: 0.217311\tvalid_1's rmse: 0.220706\n",
      "[1050]\ttraining's rmse: 0.217247\tvalid_1's rmse: 0.220707\n",
      "[1060]\ttraining's rmse: 0.217185\tvalid_1's rmse: 0.220699\n",
      "[1070]\ttraining's rmse: 0.217123\tvalid_1's rmse: 0.220695\n",
      "[1080]\ttraining's rmse: 0.217063\tvalid_1's rmse: 0.220682\n",
      "[1090]\ttraining's rmse: 0.217006\tvalid_1's rmse: 0.22067\n",
      "[1100]\ttraining's rmse: 0.216946\tvalid_1's rmse: 0.220656\n",
      "[1110]\ttraining's rmse: 0.216887\tvalid_1's rmse: 0.220644\n",
      "[1120]\ttraining's rmse: 0.216828\tvalid_1's rmse: 0.220632\n",
      "[1130]\ttraining's rmse: 0.216775\tvalid_1's rmse: 0.220627\n",
      "[1140]\ttraining's rmse: 0.216722\tvalid_1's rmse: 0.220617\n",
      "[1150]\ttraining's rmse: 0.216664\tvalid_1's rmse: 0.220605\n",
      "[1160]\ttraining's rmse: 0.216601\tvalid_1's rmse: 0.220592\n",
      "[1170]\ttraining's rmse: 0.216541\tvalid_1's rmse: 0.220583\n",
      "[1180]\ttraining's rmse: 0.216486\tvalid_1's rmse: 0.220572\n",
      "[1190]\ttraining's rmse: 0.216431\tvalid_1's rmse: 0.220569\n",
      "[1200]\ttraining's rmse: 0.216372\tvalid_1's rmse: 0.220562\n",
      "[1210]\ttraining's rmse: 0.216318\tvalid_1's rmse: 0.220554\n",
      "[1220]\ttraining's rmse: 0.216265\tvalid_1's rmse: 0.220553\n",
      "[1230]\ttraining's rmse: 0.216206\tvalid_1's rmse: 0.220548\n",
      "[1240]\ttraining's rmse: 0.216153\tvalid_1's rmse: 0.220531\n",
      "[1250]\ttraining's rmse: 0.216097\tvalid_1's rmse: 0.220535\n",
      "[1260]\ttraining's rmse: 0.216045\tvalid_1's rmse: 0.220526\n",
      "[1270]\ttraining's rmse: 0.215991\tvalid_1's rmse: 0.220511\n",
      "[1280]\ttraining's rmse: 0.215938\tvalid_1's rmse: 0.220505\n",
      "[1290]\ttraining's rmse: 0.215886\tvalid_1's rmse: 0.220501\n",
      "[1300]\ttraining's rmse: 0.215832\tvalid_1's rmse: 0.220491\n",
      "[1310]\ttraining's rmse: 0.215779\tvalid_1's rmse: 0.220483\n",
      "[1320]\ttraining's rmse: 0.215719\tvalid_1's rmse: 0.220483\n",
      "[1330]\ttraining's rmse: 0.215662\tvalid_1's rmse: 0.220469\n",
      "[1340]\ttraining's rmse: 0.215602\tvalid_1's rmse: 0.220458\n",
      "[1350]\ttraining's rmse: 0.215548\tvalid_1's rmse: 0.220451\n",
      "[1360]\ttraining's rmse: 0.215498\tvalid_1's rmse: 0.220442\n",
      "[1370]\ttraining's rmse: 0.215449\tvalid_1's rmse: 0.220438\n",
      "[1380]\ttraining's rmse: 0.215398\tvalid_1's rmse: 0.22043\n",
      "[1390]\ttraining's rmse: 0.215345\tvalid_1's rmse: 0.220425\n",
      "[1400]\ttraining's rmse: 0.215292\tvalid_1's rmse: 0.220415\n",
      "[1410]\ttraining's rmse: 0.215238\tvalid_1's rmse: 0.220415\n",
      "[1420]\ttraining's rmse: 0.215183\tvalid_1's rmse: 0.22041\n",
      "[1430]\ttraining's rmse: 0.215131\tvalid_1's rmse: 0.220403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1440]\ttraining's rmse: 0.215072\tvalid_1's rmse: 0.220392\n",
      "[1450]\ttraining's rmse: 0.215022\tvalid_1's rmse: 0.220391\n",
      "[1460]\ttraining's rmse: 0.21497\tvalid_1's rmse: 0.220387\n",
      "[1470]\ttraining's rmse: 0.21492\tvalid_1's rmse: 0.220379\n",
      "[1480]\ttraining's rmse: 0.214868\tvalid_1's rmse: 0.220369\n",
      "[1490]\ttraining's rmse: 0.214818\tvalid_1's rmse: 0.220358\n",
      "[1500]\ttraining's rmse: 0.214767\tvalid_1's rmse: 0.220358\n",
      "[1510]\ttraining's rmse: 0.214715\tvalid_1's rmse: 0.220359\n",
      "[1520]\ttraining's rmse: 0.214665\tvalid_1's rmse: 0.220358\n",
      "[1530]\ttraining's rmse: 0.214609\tvalid_1's rmse: 0.220343\n",
      "[1540]\ttraining's rmse: 0.21456\tvalid_1's rmse: 0.220343\n",
      "[1550]\ttraining's rmse: 0.214511\tvalid_1's rmse: 0.22034\n",
      "[1560]\ttraining's rmse: 0.214462\tvalid_1's rmse: 0.220333\n",
      "[1570]\ttraining's rmse: 0.214409\tvalid_1's rmse: 0.220323\n",
      "[1580]\ttraining's rmse: 0.214356\tvalid_1's rmse: 0.22032\n",
      "[1590]\ttraining's rmse: 0.214307\tvalid_1's rmse: 0.220318\n",
      "[1600]\ttraining's rmse: 0.214255\tvalid_1's rmse: 0.22032\n",
      "[1610]\ttraining's rmse: 0.214207\tvalid_1's rmse: 0.220321\n",
      "[1620]\ttraining's rmse: 0.214159\tvalid_1's rmse: 0.22032\n",
      "[1630]\ttraining's rmse: 0.214107\tvalid_1's rmse: 0.220303\n",
      "[1640]\ttraining's rmse: 0.214054\tvalid_1's rmse: 0.220292\n",
      "[1650]\ttraining's rmse: 0.214001\tvalid_1's rmse: 0.220293\n",
      "[1660]\ttraining's rmse: 0.213953\tvalid_1's rmse: 0.220301\n",
      "[1670]\ttraining's rmse: 0.213896\tvalid_1's rmse: 0.220298\n",
      "[1680]\ttraining's rmse: 0.213844\tvalid_1's rmse: 0.220293\n",
      "[1690]\ttraining's rmse: 0.213794\tvalid_1's rmse: 0.220287\n",
      "[1700]\ttraining's rmse: 0.213743\tvalid_1's rmse: 0.220274\n",
      "[1710]\ttraining's rmse: 0.213694\tvalid_1's rmse: 0.22027\n",
      "[1720]\ttraining's rmse: 0.213642\tvalid_1's rmse: 0.220268\n",
      "[1730]\ttraining's rmse: 0.21359\tvalid_1's rmse: 0.220265\n",
      "[1740]\ttraining's rmse: 0.21354\tvalid_1's rmse: 0.220266\n",
      "[1750]\ttraining's rmse: 0.213493\tvalid_1's rmse: 0.22026\n",
      "[1760]\ttraining's rmse: 0.213448\tvalid_1's rmse: 0.220262\n",
      "[1770]\ttraining's rmse: 0.213399\tvalid_1's rmse: 0.220254\n",
      "[1780]\ttraining's rmse: 0.213349\tvalid_1's rmse: 0.220243\n",
      "[1790]\ttraining's rmse: 0.213302\tvalid_1's rmse: 0.220239\n",
      "[1800]\ttraining's rmse: 0.213251\tvalid_1's rmse: 0.220234\n",
      "[1810]\ttraining's rmse: 0.213198\tvalid_1's rmse: 0.220226\n",
      "[1820]\ttraining's rmse: 0.21315\tvalid_1's rmse: 0.220224\n",
      "[1830]\ttraining's rmse: 0.2131\tvalid_1's rmse: 0.220219\n",
      "[1840]\ttraining's rmse: 0.213049\tvalid_1's rmse: 0.220224\n",
      "[1850]\ttraining's rmse: 0.213\tvalid_1's rmse: 0.220224\n",
      "[1860]\ttraining's rmse: 0.212952\tvalid_1's rmse: 0.220217\n",
      "[1870]\ttraining's rmse: 0.212905\tvalid_1's rmse: 0.220206\n",
      "[1880]\ttraining's rmse: 0.212858\tvalid_1's rmse: 0.220205\n",
      "[1890]\ttraining's rmse: 0.212812\tvalid_1's rmse: 0.22021\n",
      "[1900]\ttraining's rmse: 0.21276\tvalid_1's rmse: 0.220205\n",
      "[1910]\ttraining's rmse: 0.212713\tvalid_1's rmse: 0.220202\n",
      "[1920]\ttraining's rmse: 0.212666\tvalid_1's rmse: 0.220197\n",
      "[1930]\ttraining's rmse: 0.212617\tvalid_1's rmse: 0.220188\n",
      "[1940]\ttraining's rmse: 0.212563\tvalid_1's rmse: 0.220177\n",
      "[1950]\ttraining's rmse: 0.212516\tvalid_1's rmse: 0.220177\n",
      "[1960]\ttraining's rmse: 0.212467\tvalid_1's rmse: 0.220168\n",
      "[1970]\ttraining's rmse: 0.212422\tvalid_1's rmse: 0.220169\n",
      "[1980]\ttraining's rmse: 0.212373\tvalid_1's rmse: 0.220161\n",
      "[1990]\ttraining's rmse: 0.212324\tvalid_1's rmse: 0.220159\n",
      "[2000]\ttraining's rmse: 0.212278\tvalid_1's rmse: 0.220156\n",
      "[2010]\ttraining's rmse: 0.21223\tvalid_1's rmse: 0.220159\n",
      "[2020]\ttraining's rmse: 0.212182\tvalid_1's rmse: 0.220157\n",
      "[2030]\ttraining's rmse: 0.212137\tvalid_1's rmse: 0.220153\n",
      "[2040]\ttraining's rmse: 0.212088\tvalid_1's rmse: 0.220144\n",
      "[2050]\ttraining's rmse: 0.212033\tvalid_1's rmse: 0.220137\n",
      "[2060]\ttraining's rmse: 0.211986\tvalid_1's rmse: 0.220146\n",
      "[2070]\ttraining's rmse: 0.211941\tvalid_1's rmse: 0.220144\n",
      "[2080]\ttraining's rmse: 0.211897\tvalid_1's rmse: 0.22014\n",
      "[2090]\ttraining's rmse: 0.211849\tvalid_1's rmse: 0.220133\n",
      "[2100]\ttraining's rmse: 0.211801\tvalid_1's rmse: 0.220126\n",
      "[2110]\ttraining's rmse: 0.211753\tvalid_1's rmse: 0.22013\n",
      "[2120]\ttraining's rmse: 0.211707\tvalid_1's rmse: 0.220123\n",
      "[2130]\ttraining's rmse: 0.21166\tvalid_1's rmse: 0.220117\n",
      "[2140]\ttraining's rmse: 0.211614\tvalid_1's rmse: 0.220116\n",
      "[2150]\ttraining's rmse: 0.211567\tvalid_1's rmse: 0.22011\n",
      "[2160]\ttraining's rmse: 0.21152\tvalid_1's rmse: 0.220107\n",
      "[2170]\ttraining's rmse: 0.211467\tvalid_1's rmse: 0.220097\n",
      "[2180]\ttraining's rmse: 0.211421\tvalid_1's rmse: 0.220092\n",
      "[2190]\ttraining's rmse: 0.211375\tvalid_1's rmse: 0.220095\n",
      "[2200]\ttraining's rmse: 0.211331\tvalid_1's rmse: 0.220093\n",
      "[2210]\ttraining's rmse: 0.211288\tvalid_1's rmse: 0.220098\n",
      "[2220]\ttraining's rmse: 0.211244\tvalid_1's rmse: 0.220104\n",
      "[2230]\ttraining's rmse: 0.2112\tvalid_1's rmse: 0.220101\n",
      "[2240]\ttraining's rmse: 0.211157\tvalid_1's rmse: 0.220098\n",
      "[2250]\ttraining's rmse: 0.211109\tvalid_1's rmse: 0.220094\n",
      "[2260]\ttraining's rmse: 0.211066\tvalid_1's rmse: 0.220086\n",
      "[2270]\ttraining's rmse: 0.211019\tvalid_1's rmse: 0.220087\n",
      "[2280]\ttraining's rmse: 0.21097\tvalid_1's rmse: 0.220086\n",
      "[2290]\ttraining's rmse: 0.210927\tvalid_1's rmse: 0.220082\n",
      "[2300]\ttraining's rmse: 0.210881\tvalid_1's rmse: 0.220084\n",
      "[2310]\ttraining's rmse: 0.210837\tvalid_1's rmse: 0.220088\n",
      "[2320]\ttraining's rmse: 0.210791\tvalid_1's rmse: 0.22008\n",
      "[2330]\ttraining's rmse: 0.210749\tvalid_1's rmse: 0.220072\n",
      "[2340]\ttraining's rmse: 0.210702\tvalid_1's rmse: 0.220068\n",
      "[2350]\ttraining's rmse: 0.210657\tvalid_1's rmse: 0.22007\n",
      "[2360]\ttraining's rmse: 0.210612\tvalid_1's rmse: 0.220075\n",
      "[2370]\ttraining's rmse: 0.21057\tvalid_1's rmse: 0.220074\n",
      "[2380]\ttraining's rmse: 0.210522\tvalid_1's rmse: 0.220074\n",
      "[2390]\ttraining's rmse: 0.210479\tvalid_1's rmse: 0.220078\n",
      "[2400]\ttraining's rmse: 0.210434\tvalid_1's rmse: 0.220071\n",
      "[2410]\ttraining's rmse: 0.210389\tvalid_1's rmse: 0.220071\n",
      "[2420]\ttraining's rmse: 0.210348\tvalid_1's rmse: 0.220063\n",
      "[2430]\ttraining's rmse: 0.210306\tvalid_1's rmse: 0.220062\n",
      "[2440]\ttraining's rmse: 0.210262\tvalid_1's rmse: 0.220062\n",
      "[2450]\ttraining's rmse: 0.210216\tvalid_1's rmse: 0.220065\n",
      "[2460]\ttraining's rmse: 0.210173\tvalid_1's rmse: 0.220062\n",
      "[2470]\ttraining's rmse: 0.210127\tvalid_1's rmse: 0.22006\n",
      "[2480]\ttraining's rmse: 0.210085\tvalid_1's rmse: 0.220066\n",
      "[2490]\ttraining's rmse: 0.210042\tvalid_1's rmse: 0.220059\n",
      "[2500]\ttraining's rmse: 0.209998\tvalid_1's rmse: 0.220057\n",
      "[2510]\ttraining's rmse: 0.20996\tvalid_1's rmse: 0.220055\n",
      "[2520]\ttraining's rmse: 0.209914\tvalid_1's rmse: 0.220051\n",
      "[2530]\ttraining's rmse: 0.209868\tvalid_1's rmse: 0.220047\n",
      "[2540]\ttraining's rmse: 0.209823\tvalid_1's rmse: 0.220044\n",
      "[2550]\ttraining's rmse: 0.209777\tvalid_1's rmse: 0.220034\n",
      "[2560]\ttraining's rmse: 0.209735\tvalid_1's rmse: 0.220041\n",
      "[2570]\ttraining's rmse: 0.209689\tvalid_1's rmse: 0.220045\n",
      "[2580]\ttraining's rmse: 0.209647\tvalid_1's rmse: 0.220043\n",
      "[2590]\ttraining's rmse: 0.209604\tvalid_1's rmse: 0.22004\n",
      "[2600]\ttraining's rmse: 0.209559\tvalid_1's rmse: 0.220047\n",
      "[2610]\ttraining's rmse: 0.209514\tvalid_1's rmse: 0.220046\n",
      "[2620]\ttraining's rmse: 0.209471\tvalid_1's rmse: 0.220039\n",
      "[2630]\ttraining's rmse: 0.209425\tvalid_1's rmse: 0.220041\n",
      "[2640]\ttraining's rmse: 0.20938\tvalid_1's rmse: 0.220037\n",
      "Early stopping, best iteration is:\n",
      "[2549]\ttraining's rmse: 0.209782\tvalid_1's rmse: 0.220033\n",
      "rmse:0.2199785517403002\n",
      "params: \n",
      "max_depth: 11\n",
      "num_leaves: 41\n",
      "min_data_per_leaf: 80\n",
      "min_child_weight: 0.2102038826\n",
      "subsample: 0.726049868824\n",
      "subsample_freq: 1\n",
      "lambda_l1: 0.440292593804\n",
      "lambda_l2: 9.31862564419\n",
      "feature_fraction: 0.468428961481\n",
      "learning rate: 0.116635324847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.235244\tvalid_1's rmse: 0.233304\n",
      "[20]\ttraining's rmse: 0.230784\tvalid_1's rmse: 0.228654\n",
      "[30]\ttraining's rmse: 0.229014\tvalid_1's rmse: 0.226895\n",
      "[40]\ttraining's rmse: 0.228111\tvalid_1's rmse: 0.226092\n",
      "[50]\ttraining's rmse: 0.227385\tvalid_1's rmse: 0.225466\n",
      "[60]\ttraining's rmse: 0.226814\tvalid_1's rmse: 0.225003\n",
      "[70]\ttraining's rmse: 0.226324\tvalid_1's rmse: 0.224577\n",
      "[80]\ttraining's rmse: 0.225865\tvalid_1's rmse: 0.224235\n",
      "[90]\ttraining's rmse: 0.225446\tvalid_1's rmse: 0.223911\n",
      "[100]\ttraining's rmse: 0.225104\tvalid_1's rmse: 0.223659\n",
      "[110]\ttraining's rmse: 0.224824\tvalid_1's rmse: 0.223501\n",
      "[120]\ttraining's rmse: 0.224528\tvalid_1's rmse: 0.223307\n",
      "[130]\ttraining's rmse: 0.224269\tvalid_1's rmse: 0.223156\n",
      "[140]\ttraining's rmse: 0.223964\tvalid_1's rmse: 0.22295\n",
      "[150]\ttraining's rmse: 0.223722\tvalid_1's rmse: 0.222841\n",
      "[160]\ttraining's rmse: 0.223477\tvalid_1's rmse: 0.222706\n",
      "[170]\ttraining's rmse: 0.223273\tvalid_1's rmse: 0.22259\n",
      "[180]\ttraining's rmse: 0.22308\tvalid_1's rmse: 0.222522\n",
      "[190]\ttraining's rmse: 0.222871\tvalid_1's rmse: 0.222434\n",
      "[200]\ttraining's rmse: 0.222706\tvalid_1's rmse: 0.222398\n",
      "[210]\ttraining's rmse: 0.222493\tvalid_1's rmse: 0.222278\n",
      "[220]\ttraining's rmse: 0.222304\tvalid_1's rmse: 0.222211\n",
      "[230]\ttraining's rmse: 0.222151\tvalid_1's rmse: 0.222168\n",
      "[240]\ttraining's rmse: 0.221989\tvalid_1's rmse: 0.222128\n",
      "[250]\ttraining's rmse: 0.221826\tvalid_1's rmse: 0.222062\n",
      "[260]\ttraining's rmse: 0.221658\tvalid_1's rmse: 0.222001\n",
      "[270]\ttraining's rmse: 0.221499\tvalid_1's rmse: 0.22195\n",
      "[280]\ttraining's rmse: 0.221352\tvalid_1's rmse: 0.221933\n",
      "[290]\ttraining's rmse: 0.221218\tvalid_1's rmse: 0.221919\n",
      "[300]\ttraining's rmse: 0.221084\tvalid_1's rmse: 0.221901\n",
      "[310]\ttraining's rmse: 0.220927\tvalid_1's rmse: 0.221864\n",
      "[320]\ttraining's rmse: 0.220784\tvalid_1's rmse: 0.221824\n",
      "[330]\ttraining's rmse: 0.220649\tvalid_1's rmse: 0.221785\n",
      "[340]\ttraining's rmse: 0.220498\tvalid_1's rmse: 0.22174\n",
      "[350]\ttraining's rmse: 0.220356\tvalid_1's rmse: 0.221706\n",
      "[360]\ttraining's rmse: 0.220223\tvalid_1's rmse: 0.221698\n",
      "[370]\ttraining's rmse: 0.22009\tvalid_1's rmse: 0.221686\n",
      "[380]\ttraining's rmse: 0.219962\tvalid_1's rmse: 0.221673\n",
      "[390]\ttraining's rmse: 0.219841\tvalid_1's rmse: 0.221659\n",
      "[400]\ttraining's rmse: 0.219716\tvalid_1's rmse: 0.221633\n",
      "[410]\ttraining's rmse: 0.219588\tvalid_1's rmse: 0.221606\n",
      "[420]\ttraining's rmse: 0.219448\tvalid_1's rmse: 0.221569\n",
      "[430]\ttraining's rmse: 0.219318\tvalid_1's rmse: 0.221545\n",
      "[440]\ttraining's rmse: 0.219187\tvalid_1's rmse: 0.221523\n",
      "[450]\ttraining's rmse: 0.219075\tvalid_1's rmse: 0.221503\n",
      "[460]\ttraining's rmse: 0.218933\tvalid_1's rmse: 0.221487\n",
      "[470]\ttraining's rmse: 0.218808\tvalid_1's rmse: 0.22147\n",
      "[480]\ttraining's rmse: 0.218693\tvalid_1's rmse: 0.221462\n",
      "[490]\ttraining's rmse: 0.218567\tvalid_1's rmse: 0.221447\n",
      "[500]\ttraining's rmse: 0.218456\tvalid_1's rmse: 0.221424\n",
      "[510]\ttraining's rmse: 0.218328\tvalid_1's rmse: 0.221406\n",
      "[520]\ttraining's rmse: 0.2182\tvalid_1's rmse: 0.22139\n",
      "[530]\ttraining's rmse: 0.21808\tvalid_1's rmse: 0.22136\n",
      "[540]\ttraining's rmse: 0.217969\tvalid_1's rmse: 0.22135\n",
      "[550]\ttraining's rmse: 0.217844\tvalid_1's rmse: 0.221331\n",
      "[560]\ttraining's rmse: 0.21773\tvalid_1's rmse: 0.221328\n",
      "[570]\ttraining's rmse: 0.217606\tvalid_1's rmse: 0.221307\n",
      "[580]\ttraining's rmse: 0.21749\tvalid_1's rmse: 0.221303\n",
      "[590]\ttraining's rmse: 0.217372\tvalid_1's rmse: 0.221288\n",
      "[600]\ttraining's rmse: 0.217242\tvalid_1's rmse: 0.221241\n",
      "[610]\ttraining's rmse: 0.217124\tvalid_1's rmse: 0.221212\n",
      "[620]\ttraining's rmse: 0.217007\tvalid_1's rmse: 0.221202\n",
      "[630]\ttraining's rmse: 0.216901\tvalid_1's rmse: 0.221212\n",
      "[640]\ttraining's rmse: 0.216786\tvalid_1's rmse: 0.221187\n",
      "[650]\ttraining's rmse: 0.216673\tvalid_1's rmse: 0.221176\n",
      "[660]\ttraining's rmse: 0.216553\tvalid_1's rmse: 0.221169\n",
      "[670]\ttraining's rmse: 0.216447\tvalid_1's rmse: 0.221167\n",
      "[680]\ttraining's rmse: 0.216321\tvalid_1's rmse: 0.221145\n",
      "[690]\ttraining's rmse: 0.216213\tvalid_1's rmse: 0.22114\n",
      "[700]\ttraining's rmse: 0.216101\tvalid_1's rmse: 0.221117\n",
      "[710]\ttraining's rmse: 0.215991\tvalid_1's rmse: 0.221111\n",
      "[720]\ttraining's rmse: 0.215883\tvalid_1's rmse: 0.221094\n",
      "[730]\ttraining's rmse: 0.215785\tvalid_1's rmse: 0.221079\n",
      "[740]\ttraining's rmse: 0.215683\tvalid_1's rmse: 0.22107\n",
      "[750]\ttraining's rmse: 0.215576\tvalid_1's rmse: 0.221071\n",
      "[760]\ttraining's rmse: 0.215468\tvalid_1's rmse: 0.221051\n",
      "[770]\ttraining's rmse: 0.215366\tvalid_1's rmse: 0.221038\n",
      "[780]\ttraining's rmse: 0.215241\tvalid_1's rmse: 0.221013\n",
      "[790]\ttraining's rmse: 0.215121\tvalid_1's rmse: 0.220988\n",
      "[800]\ttraining's rmse: 0.215007\tvalid_1's rmse: 0.220981\n",
      "[810]\ttraining's rmse: 0.214884\tvalid_1's rmse: 0.220972\n",
      "[820]\ttraining's rmse: 0.214773\tvalid_1's rmse: 0.220983\n",
      "[830]\ttraining's rmse: 0.214682\tvalid_1's rmse: 0.220984\n",
      "[840]\ttraining's rmse: 0.214583\tvalid_1's rmse: 0.22098\n",
      "[850]\ttraining's rmse: 0.214483\tvalid_1's rmse: 0.220974\n",
      "[860]\ttraining's rmse: 0.214383\tvalid_1's rmse: 0.220976\n",
      "[870]\ttraining's rmse: 0.21428\tvalid_1's rmse: 0.220976\n",
      "[880]\ttraining's rmse: 0.214179\tvalid_1's rmse: 0.220982\n",
      "[890]\ttraining's rmse: 0.214087\tvalid_1's rmse: 0.220981\n",
      "[900]\ttraining's rmse: 0.213983\tvalid_1's rmse: 0.22097\n",
      "[910]\ttraining's rmse: 0.213877\tvalid_1's rmse: 0.220964\n",
      "[920]\ttraining's rmse: 0.213776\tvalid_1's rmse: 0.220957\n",
      "[930]\ttraining's rmse: 0.21367\tvalid_1's rmse: 0.220972\n",
      "[940]\ttraining's rmse: 0.213561\tvalid_1's rmse: 0.220957\n",
      "[950]\ttraining's rmse: 0.213453\tvalid_1's rmse: 0.220955\n",
      "[960]\ttraining's rmse: 0.21335\tvalid_1's rmse: 0.220942\n",
      "[970]\ttraining's rmse: 0.213249\tvalid_1's rmse: 0.220944\n",
      "[980]\ttraining's rmse: 0.213161\tvalid_1's rmse: 0.22094\n",
      "[990]\ttraining's rmse: 0.213066\tvalid_1's rmse: 0.220937\n",
      "[1000]\ttraining's rmse: 0.212971\tvalid_1's rmse: 0.220929\n",
      "[1010]\ttraining's rmse: 0.212878\tvalid_1's rmse: 0.220946\n",
      "[1020]\ttraining's rmse: 0.212772\tvalid_1's rmse: 0.22093\n",
      "[1030]\ttraining's rmse: 0.212674\tvalid_1's rmse: 0.220918\n",
      "[1040]\ttraining's rmse: 0.212582\tvalid_1's rmse: 0.220905\n",
      "[1050]\ttraining's rmse: 0.212489\tvalid_1's rmse: 0.22091\n",
      "[1060]\ttraining's rmse: 0.212391\tvalid_1's rmse: 0.22091\n",
      "[1070]\ttraining's rmse: 0.212293\tvalid_1's rmse: 0.220919\n",
      "[1080]\ttraining's rmse: 0.212191\tvalid_1's rmse: 0.220904\n",
      "[1090]\ttraining's rmse: 0.212095\tvalid_1's rmse: 0.220893\n",
      "[1100]\ttraining's rmse: 0.212006\tvalid_1's rmse: 0.220883\n",
      "[1110]\ttraining's rmse: 0.211916\tvalid_1's rmse: 0.220875\n",
      "[1120]\ttraining's rmse: 0.211814\tvalid_1's rmse: 0.220877\n",
      "[1130]\ttraining's rmse: 0.211726\tvalid_1's rmse: 0.220868\n",
      "[1140]\ttraining's rmse: 0.211624\tvalid_1's rmse: 0.22089\n",
      "[1150]\ttraining's rmse: 0.211533\tvalid_1's rmse: 0.220877\n",
      "[1160]\ttraining's rmse: 0.211446\tvalid_1's rmse: 0.220888\n",
      "[1170]\ttraining's rmse: 0.211348\tvalid_1's rmse: 0.220874\n",
      "[1180]\ttraining's rmse: 0.211252\tvalid_1's rmse: 0.220865\n",
      "[1190]\ttraining's rmse: 0.211162\tvalid_1's rmse: 0.220858\n",
      "[1200]\ttraining's rmse: 0.211065\tvalid_1's rmse: 0.22085\n",
      "[1210]\ttraining's rmse: 0.210971\tvalid_1's rmse: 0.220848\n",
      "[1220]\ttraining's rmse: 0.210865\tvalid_1's rmse: 0.220853\n",
      "[1230]\ttraining's rmse: 0.210776\tvalid_1's rmse: 0.220857\n",
      "[1240]\ttraining's rmse: 0.210687\tvalid_1's rmse: 0.220857\n",
      "[1250]\ttraining's rmse: 0.210597\tvalid_1's rmse: 0.220852\n",
      "[1260]\ttraining's rmse: 0.210503\tvalid_1's rmse: 0.220855\n",
      "[1270]\ttraining's rmse: 0.210412\tvalid_1's rmse: 0.22086\n",
      "[1280]\ttraining's rmse: 0.210312\tvalid_1's rmse: 0.220856\n",
      "[1290]\ttraining's rmse: 0.210212\tvalid_1's rmse: 0.220847\n",
      "[1300]\ttraining's rmse: 0.210118\tvalid_1's rmse: 0.220845\n",
      "[1310]\ttraining's rmse: 0.210022\tvalid_1's rmse: 0.220835\n",
      "[1320]\ttraining's rmse: 0.20993\tvalid_1's rmse: 0.220832\n",
      "[1330]\ttraining's rmse: 0.209843\tvalid_1's rmse: 0.22083\n",
      "[1340]\ttraining's rmse: 0.209751\tvalid_1's rmse: 0.220829\n",
      "[1350]\ttraining's rmse: 0.209663\tvalid_1's rmse: 0.220831\n",
      "[1360]\ttraining's rmse: 0.209573\tvalid_1's rmse: 0.220829\n",
      "[1370]\ttraining's rmse: 0.209478\tvalid_1's rmse: 0.220829\n",
      "[1380]\ttraining's rmse: 0.209394\tvalid_1's rmse: 0.220821\n",
      "[1390]\ttraining's rmse: 0.209316\tvalid_1's rmse: 0.220827\n",
      "[1400]\ttraining's rmse: 0.20923\tvalid_1's rmse: 0.220822\n",
      "[1410]\ttraining's rmse: 0.209151\tvalid_1's rmse: 0.220824\n",
      "[1420]\ttraining's rmse: 0.209062\tvalid_1's rmse: 0.220836\n",
      "[1430]\ttraining's rmse: 0.208981\tvalid_1's rmse: 0.220835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1440]\ttraining's rmse: 0.208902\tvalid_1's rmse: 0.220835\n",
      "[1450]\ttraining's rmse: 0.208813\tvalid_1's rmse: 0.220839\n",
      "[1460]\ttraining's rmse: 0.208718\tvalid_1's rmse: 0.22085\n",
      "[1470]\ttraining's rmse: 0.208636\tvalid_1's rmse: 0.220853\n",
      "[1480]\ttraining's rmse: 0.20855\tvalid_1's rmse: 0.220845\n",
      "[1490]\ttraining's rmse: 0.208463\tvalid_1's rmse: 0.220824\n",
      "Early stopping, best iteration is:\n",
      "[1394]\ttraining's rmse: 0.209285\tvalid_1's rmse: 0.22082\n",
      "rmse:0.2207560193592283\n",
      "params: \n",
      "max_depth: 10\n",
      "num_leaves: 40\n",
      "min_data_per_leaf: 40\n",
      "min_child_weight: 0.05\n",
      "subsample: 0.9\n",
      "subsample_freq: 6\n",
      "lambda_l1: 15.0\n",
      "lambda_l2: 0.0\n",
      "feature_fraction: 0.4\n",
      "learning rate: 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.240761\tvalid_1's rmse: 0.239108\n",
      "[20]\ttraining's rmse: 0.234334\tvalid_1's rmse: 0.232304\n",
      "[30]\ttraining's rmse: 0.231658\tvalid_1's rmse: 0.229461\n",
      "[40]\ttraining's rmse: 0.230254\tvalid_1's rmse: 0.228045\n",
      "[50]\ttraining's rmse: 0.229331\tvalid_1's rmse: 0.22715\n",
      "[60]\ttraining's rmse: 0.228721\tvalid_1's rmse: 0.226575\n",
      "[70]\ttraining's rmse: 0.228302\tvalid_1's rmse: 0.22619\n",
      "[80]\ttraining's rmse: 0.227839\tvalid_1's rmse: 0.225767\n",
      "[90]\ttraining's rmse: 0.227401\tvalid_1's rmse: 0.225367\n",
      "[100]\ttraining's rmse: 0.227069\tvalid_1's rmse: 0.225085\n",
      "[110]\ttraining's rmse: 0.2268\tvalid_1's rmse: 0.22485\n",
      "[120]\ttraining's rmse: 0.226533\tvalid_1's rmse: 0.224626\n",
      "[130]\ttraining's rmse: 0.226313\tvalid_1's rmse: 0.224465\n",
      "[140]\ttraining's rmse: 0.226054\tvalid_1's rmse: 0.224254\n",
      "[150]\ttraining's rmse: 0.225827\tvalid_1's rmse: 0.224067\n",
      "[160]\ttraining's rmse: 0.225609\tvalid_1's rmse: 0.223893\n",
      "[170]\ttraining's rmse: 0.225389\tvalid_1's rmse: 0.22372\n",
      "[180]\ttraining's rmse: 0.225215\tvalid_1's rmse: 0.223597\n",
      "[190]\ttraining's rmse: 0.225052\tvalid_1's rmse: 0.22348\n",
      "[200]\ttraining's rmse: 0.224898\tvalid_1's rmse: 0.223368\n",
      "[210]\ttraining's rmse: 0.224743\tvalid_1's rmse: 0.223262\n",
      "[220]\ttraining's rmse: 0.224611\tvalid_1's rmse: 0.223176\n",
      "[230]\ttraining's rmse: 0.224473\tvalid_1's rmse: 0.223087\n",
      "[240]\ttraining's rmse: 0.224345\tvalid_1's rmse: 0.223015\n",
      "[250]\ttraining's rmse: 0.224222\tvalid_1's rmse: 0.222933\n",
      "[260]\ttraining's rmse: 0.224108\tvalid_1's rmse: 0.222863\n",
      "[270]\ttraining's rmse: 0.223975\tvalid_1's rmse: 0.222776\n",
      "[280]\ttraining's rmse: 0.223868\tvalid_1's rmse: 0.222709\n",
      "[290]\ttraining's rmse: 0.223733\tvalid_1's rmse: 0.222612\n",
      "[300]\ttraining's rmse: 0.22361\tvalid_1's rmse: 0.222542\n",
      "[310]\ttraining's rmse: 0.223503\tvalid_1's rmse: 0.222469\n",
      "[320]\ttraining's rmse: 0.223411\tvalid_1's rmse: 0.222424\n",
      "[330]\ttraining's rmse: 0.223318\tvalid_1's rmse: 0.222389\n",
      "[340]\ttraining's rmse: 0.223206\tvalid_1's rmse: 0.222321\n",
      "[350]\ttraining's rmse: 0.223116\tvalid_1's rmse: 0.222281\n",
      "[360]\ttraining's rmse: 0.223025\tvalid_1's rmse: 0.222235\n",
      "[370]\ttraining's rmse: 0.222935\tvalid_1's rmse: 0.222183\n",
      "[380]\ttraining's rmse: 0.222852\tvalid_1's rmse: 0.222147\n",
      "[390]\ttraining's rmse: 0.222758\tvalid_1's rmse: 0.222098\n",
      "[400]\ttraining's rmse: 0.222678\tvalid_1's rmse: 0.222059\n",
      "[410]\ttraining's rmse: 0.222581\tvalid_1's rmse: 0.222012\n",
      "[420]\ttraining's rmse: 0.222498\tvalid_1's rmse: 0.221968\n",
      "[430]\ttraining's rmse: 0.222412\tvalid_1's rmse: 0.221934\n",
      "[440]\ttraining's rmse: 0.222332\tvalid_1's rmse: 0.221896\n",
      "[450]\ttraining's rmse: 0.222261\tvalid_1's rmse: 0.221875\n",
      "[460]\ttraining's rmse: 0.222178\tvalid_1's rmse: 0.221835\n",
      "[470]\ttraining's rmse: 0.222107\tvalid_1's rmse: 0.221813\n",
      "[480]\ttraining's rmse: 0.222033\tvalid_1's rmse: 0.221781\n",
      "[490]\ttraining's rmse: 0.221962\tvalid_1's rmse: 0.221756\n",
      "[500]\ttraining's rmse: 0.221891\tvalid_1's rmse: 0.221726\n",
      "[510]\ttraining's rmse: 0.221822\tvalid_1's rmse: 0.221701\n",
      "[520]\ttraining's rmse: 0.221749\tvalid_1's rmse: 0.221664\n",
      "[530]\ttraining's rmse: 0.221677\tvalid_1's rmse: 0.221639\n",
      "[540]\ttraining's rmse: 0.221612\tvalid_1's rmse: 0.221619\n",
      "[550]\ttraining's rmse: 0.221539\tvalid_1's rmse: 0.221585\n",
      "[560]\ttraining's rmse: 0.221468\tvalid_1's rmse: 0.221551\n",
      "[570]\ttraining's rmse: 0.221398\tvalid_1's rmse: 0.221526\n",
      "[580]\ttraining's rmse: 0.221337\tvalid_1's rmse: 0.221502\n",
      "[590]\ttraining's rmse: 0.221268\tvalid_1's rmse: 0.221478\n",
      "[600]\ttraining's rmse: 0.221205\tvalid_1's rmse: 0.221458\n",
      "[610]\ttraining's rmse: 0.22114\tvalid_1's rmse: 0.221429\n",
      "[620]\ttraining's rmse: 0.221077\tvalid_1's rmse: 0.221408\n",
      "[630]\ttraining's rmse: 0.221013\tvalid_1's rmse: 0.221381\n",
      "[640]\ttraining's rmse: 0.220954\tvalid_1's rmse: 0.221361\n",
      "[650]\ttraining's rmse: 0.220896\tvalid_1's rmse: 0.221343\n",
      "[660]\ttraining's rmse: 0.220839\tvalid_1's rmse: 0.221329\n",
      "[670]\ttraining's rmse: 0.220783\tvalid_1's rmse: 0.221314\n",
      "[680]\ttraining's rmse: 0.220722\tvalid_1's rmse: 0.221291\n",
      "[690]\ttraining's rmse: 0.220663\tvalid_1's rmse: 0.221275\n",
      "[700]\ttraining's rmse: 0.220607\tvalid_1's rmse: 0.221258\n",
      "[710]\ttraining's rmse: 0.220545\tvalid_1's rmse: 0.22123\n",
      "[720]\ttraining's rmse: 0.220489\tvalid_1's rmse: 0.221211\n",
      "[730]\ttraining's rmse: 0.220436\tvalid_1's rmse: 0.221197\n",
      "[740]\ttraining's rmse: 0.220375\tvalid_1's rmse: 0.22118\n",
      "[750]\ttraining's rmse: 0.220319\tvalid_1's rmse: 0.221166\n",
      "[760]\ttraining's rmse: 0.22026\tvalid_1's rmse: 0.221148\n",
      "[770]\ttraining's rmse: 0.220204\tvalid_1's rmse: 0.221133\n",
      "[780]\ttraining's rmse: 0.220138\tvalid_1's rmse: 0.221097\n",
      "[790]\ttraining's rmse: 0.220084\tvalid_1's rmse: 0.221085\n",
      "[800]\ttraining's rmse: 0.220038\tvalid_1's rmse: 0.221079\n",
      "[810]\ttraining's rmse: 0.219983\tvalid_1's rmse: 0.221061\n",
      "[820]\ttraining's rmse: 0.219938\tvalid_1's rmse: 0.221044\n",
      "[830]\ttraining's rmse: 0.219886\tvalid_1's rmse: 0.221031\n",
      "[840]\ttraining's rmse: 0.219828\tvalid_1's rmse: 0.221007\n",
      "[850]\ttraining's rmse: 0.219772\tvalid_1's rmse: 0.220995\n",
      "[860]\ttraining's rmse: 0.219715\tvalid_1's rmse: 0.220984\n",
      "[870]\ttraining's rmse: 0.219662\tvalid_1's rmse: 0.220967\n",
      "[880]\ttraining's rmse: 0.219612\tvalid_1's rmse: 0.220953\n",
      "[890]\ttraining's rmse: 0.219554\tvalid_1's rmse: 0.220925\n",
      "[900]\ttraining's rmse: 0.219498\tvalid_1's rmse: 0.220902\n",
      "[910]\ttraining's rmse: 0.219444\tvalid_1's rmse: 0.220888\n",
      "[920]\ttraining's rmse: 0.219388\tvalid_1's rmse: 0.220869\n",
      "[930]\ttraining's rmse: 0.219331\tvalid_1's rmse: 0.220851\n",
      "[940]\ttraining's rmse: 0.219279\tvalid_1's rmse: 0.22084\n",
      "[950]\ttraining's rmse: 0.219231\tvalid_1's rmse: 0.220831\n",
      "[960]\ttraining's rmse: 0.219175\tvalid_1's rmse: 0.220812\n",
      "[970]\ttraining's rmse: 0.219124\tvalid_1's rmse: 0.220799\n",
      "[980]\ttraining's rmse: 0.219078\tvalid_1's rmse: 0.220788\n",
      "[990]\ttraining's rmse: 0.219029\tvalid_1's rmse: 0.220774\n",
      "[1000]\ttraining's rmse: 0.218973\tvalid_1's rmse: 0.220759\n",
      "[1010]\ttraining's rmse: 0.218919\tvalid_1's rmse: 0.220748\n",
      "[1020]\ttraining's rmse: 0.218861\tvalid_1's rmse: 0.22074\n",
      "[1030]\ttraining's rmse: 0.218812\tvalid_1's rmse: 0.220731\n",
      "[1040]\ttraining's rmse: 0.218768\tvalid_1's rmse: 0.220719\n",
      "[1050]\ttraining's rmse: 0.218723\tvalid_1's rmse: 0.220706\n",
      "[1060]\ttraining's rmse: 0.218667\tvalid_1's rmse: 0.220696\n",
      "[1070]\ttraining's rmse: 0.218624\tvalid_1's rmse: 0.22069\n",
      "[1080]\ttraining's rmse: 0.21857\tvalid_1's rmse: 0.220679\n",
      "[1090]\ttraining's rmse: 0.218518\tvalid_1's rmse: 0.220663\n",
      "[1100]\ttraining's rmse: 0.218472\tvalid_1's rmse: 0.220652\n",
      "[1110]\ttraining's rmse: 0.218429\tvalid_1's rmse: 0.220648\n",
      "[1120]\ttraining's rmse: 0.218382\tvalid_1's rmse: 0.22064\n",
      "[1130]\ttraining's rmse: 0.21834\tvalid_1's rmse: 0.220627\n",
      "[1140]\ttraining's rmse: 0.218292\tvalid_1's rmse: 0.220613\n",
      "[1150]\ttraining's rmse: 0.218247\tvalid_1's rmse: 0.220607\n",
      "[1160]\ttraining's rmse: 0.218196\tvalid_1's rmse: 0.220596\n",
      "[1170]\ttraining's rmse: 0.218157\tvalid_1's rmse: 0.220589\n",
      "[1180]\ttraining's rmse: 0.21811\tvalid_1's rmse: 0.220583\n",
      "[1190]\ttraining's rmse: 0.218059\tvalid_1's rmse: 0.220569\n",
      "[1200]\ttraining's rmse: 0.218012\tvalid_1's rmse: 0.220569\n",
      "[1210]\ttraining's rmse: 0.217966\tvalid_1's rmse: 0.220564\n",
      "[1220]\ttraining's rmse: 0.217922\tvalid_1's rmse: 0.22056\n",
      "[1230]\ttraining's rmse: 0.217878\tvalid_1's rmse: 0.220556\n",
      "[1240]\ttraining's rmse: 0.217833\tvalid_1's rmse: 0.220542\n",
      "[1250]\ttraining's rmse: 0.217784\tvalid_1's rmse: 0.220531\n",
      "[1260]\ttraining's rmse: 0.217739\tvalid_1's rmse: 0.220519\n",
      "[1270]\ttraining's rmse: 0.21769\tvalid_1's rmse: 0.220512\n",
      "[1280]\ttraining's rmse: 0.217643\tvalid_1's rmse: 0.220502\n",
      "[1290]\ttraining's rmse: 0.2176\tvalid_1's rmse: 0.220494\n",
      "[1300]\ttraining's rmse: 0.217556\tvalid_1's rmse: 0.220487\n",
      "[1310]\ttraining's rmse: 0.217516\tvalid_1's rmse: 0.220484\n",
      "[1320]\ttraining's rmse: 0.217473\tvalid_1's rmse: 0.220473\n",
      "[1330]\ttraining's rmse: 0.217428\tvalid_1's rmse: 0.22046\n",
      "[1340]\ttraining's rmse: 0.217385\tvalid_1's rmse: 0.220454\n",
      "[1350]\ttraining's rmse: 0.217337\tvalid_1's rmse: 0.220444\n",
      "[1360]\ttraining's rmse: 0.217295\tvalid_1's rmse: 0.22044\n",
      "[1370]\ttraining's rmse: 0.217255\tvalid_1's rmse: 0.220437\n",
      "[1380]\ttraining's rmse: 0.217211\tvalid_1's rmse: 0.220431\n",
      "[1390]\ttraining's rmse: 0.217166\tvalid_1's rmse: 0.220423\n",
      "[1400]\ttraining's rmse: 0.217125\tvalid_1's rmse: 0.220417\n",
      "[1410]\ttraining's rmse: 0.217086\tvalid_1's rmse: 0.220407\n",
      "[1420]\ttraining's rmse: 0.217042\tvalid_1's rmse: 0.220408\n",
      "[1430]\ttraining's rmse: 0.216998\tvalid_1's rmse: 0.220399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1440]\ttraining's rmse: 0.216959\tvalid_1's rmse: 0.220401\n",
      "[1450]\ttraining's rmse: 0.216922\tvalid_1's rmse: 0.220396\n",
      "[1460]\ttraining's rmse: 0.216881\tvalid_1's rmse: 0.220393\n",
      "[1470]\ttraining's rmse: 0.21684\tvalid_1's rmse: 0.220392\n",
      "[1480]\ttraining's rmse: 0.2168\tvalid_1's rmse: 0.220387\n",
      "[1490]\ttraining's rmse: 0.216758\tvalid_1's rmse: 0.220383\n",
      "[1500]\ttraining's rmse: 0.216713\tvalid_1's rmse: 0.22037\n",
      "[1510]\ttraining's rmse: 0.216669\tvalid_1's rmse: 0.220363\n",
      "[1520]\ttraining's rmse: 0.216629\tvalid_1's rmse: 0.220362\n",
      "[1530]\ttraining's rmse: 0.216589\tvalid_1's rmse: 0.220357\n",
      "[1540]\ttraining's rmse: 0.216547\tvalid_1's rmse: 0.220356\n",
      "[1550]\ttraining's rmse: 0.216505\tvalid_1's rmse: 0.22035\n",
      "[1560]\ttraining's rmse: 0.216467\tvalid_1's rmse: 0.220346\n",
      "[1570]\ttraining's rmse: 0.216429\tvalid_1's rmse: 0.220341\n",
      "[1580]\ttraining's rmse: 0.216388\tvalid_1's rmse: 0.220332\n",
      "[1590]\ttraining's rmse: 0.216352\tvalid_1's rmse: 0.220324\n",
      "[1600]\ttraining's rmse: 0.216304\tvalid_1's rmse: 0.220316\n",
      "[1610]\ttraining's rmse: 0.216257\tvalid_1's rmse: 0.220317\n",
      "[1620]\ttraining's rmse: 0.216218\tvalid_1's rmse: 0.220306\n",
      "[1630]\ttraining's rmse: 0.216177\tvalid_1's rmse: 0.220299\n",
      "[1640]\ttraining's rmse: 0.216138\tvalid_1's rmse: 0.220294\n",
      "[1650]\ttraining's rmse: 0.216101\tvalid_1's rmse: 0.220288\n",
      "[1660]\ttraining's rmse: 0.21606\tvalid_1's rmse: 0.220277\n",
      "[1670]\ttraining's rmse: 0.216019\tvalid_1's rmse: 0.220272\n",
      "[1680]\ttraining's rmse: 0.215982\tvalid_1's rmse: 0.220268\n",
      "[1690]\ttraining's rmse: 0.215943\tvalid_1's rmse: 0.220264\n",
      "[1700]\ttraining's rmse: 0.215903\tvalid_1's rmse: 0.220259\n",
      "[1710]\ttraining's rmse: 0.215867\tvalid_1's rmse: 0.220252\n",
      "[1720]\ttraining's rmse: 0.215828\tvalid_1's rmse: 0.220249\n",
      "[1730]\ttraining's rmse: 0.21579\tvalid_1's rmse: 0.220246\n",
      "[1740]\ttraining's rmse: 0.21575\tvalid_1's rmse: 0.220244\n",
      "[1750]\ttraining's rmse: 0.215712\tvalid_1's rmse: 0.220241\n",
      "[1760]\ttraining's rmse: 0.215668\tvalid_1's rmse: 0.220229\n",
      "[1770]\ttraining's rmse: 0.215629\tvalid_1's rmse: 0.220221\n",
      "[1780]\ttraining's rmse: 0.215588\tvalid_1's rmse: 0.220215\n",
      "[1790]\ttraining's rmse: 0.215549\tvalid_1's rmse: 0.220207\n",
      "[1800]\ttraining's rmse: 0.215512\tvalid_1's rmse: 0.220204\n",
      "[1810]\ttraining's rmse: 0.21547\tvalid_1's rmse: 0.220196\n",
      "[1820]\ttraining's rmse: 0.215428\tvalid_1's rmse: 0.22019\n",
      "[1830]\ttraining's rmse: 0.215391\tvalid_1's rmse: 0.22019\n",
      "[1840]\ttraining's rmse: 0.215353\tvalid_1's rmse: 0.220192\n",
      "[1850]\ttraining's rmse: 0.215314\tvalid_1's rmse: 0.220188\n",
      "[1860]\ttraining's rmse: 0.215272\tvalid_1's rmse: 0.22018\n",
      "[1870]\ttraining's rmse: 0.215234\tvalid_1's rmse: 0.220174\n",
      "[1880]\ttraining's rmse: 0.215194\tvalid_1's rmse: 0.220173\n",
      "[1890]\ttraining's rmse: 0.215159\tvalid_1's rmse: 0.22017\n",
      "[1900]\ttraining's rmse: 0.215122\tvalid_1's rmse: 0.220168\n",
      "[1910]\ttraining's rmse: 0.21508\tvalid_1's rmse: 0.220155\n",
      "[1920]\ttraining's rmse: 0.215042\tvalid_1's rmse: 0.220148\n",
      "[1930]\ttraining's rmse: 0.215006\tvalid_1's rmse: 0.220148\n",
      "[1940]\ttraining's rmse: 0.21497\tvalid_1's rmse: 0.220144\n",
      "[1950]\ttraining's rmse: 0.214932\tvalid_1's rmse: 0.220143\n",
      "[1960]\ttraining's rmse: 0.214895\tvalid_1's rmse: 0.22014\n",
      "[1970]\ttraining's rmse: 0.214857\tvalid_1's rmse: 0.22014\n",
      "[1980]\ttraining's rmse: 0.214819\tvalid_1's rmse: 0.220139\n",
      "[1990]\ttraining's rmse: 0.214782\tvalid_1's rmse: 0.220131\n",
      "[2000]\ttraining's rmse: 0.214741\tvalid_1's rmse: 0.220119\n",
      "[2010]\ttraining's rmse: 0.214698\tvalid_1's rmse: 0.220108\n",
      "[2020]\ttraining's rmse: 0.214661\tvalid_1's rmse: 0.220104\n",
      "[2030]\ttraining's rmse: 0.214622\tvalid_1's rmse: 0.220102\n",
      "[2040]\ttraining's rmse: 0.214581\tvalid_1's rmse: 0.220098\n",
      "[2050]\ttraining's rmse: 0.214543\tvalid_1's rmse: 0.220095\n",
      "[2060]\ttraining's rmse: 0.214505\tvalid_1's rmse: 0.220091\n",
      "[2070]\ttraining's rmse: 0.214469\tvalid_1's rmse: 0.220088\n",
      "[2080]\ttraining's rmse: 0.214432\tvalid_1's rmse: 0.220087\n",
      "[2090]\ttraining's rmse: 0.214393\tvalid_1's rmse: 0.22008\n",
      "[2100]\ttraining's rmse: 0.214353\tvalid_1's rmse: 0.220073\n",
      "[2110]\ttraining's rmse: 0.214314\tvalid_1's rmse: 0.220066\n",
      "[2120]\ttraining's rmse: 0.214278\tvalid_1's rmse: 0.220067\n",
      "[2130]\ttraining's rmse: 0.21424\tvalid_1's rmse: 0.22006\n",
      "[2140]\ttraining's rmse: 0.214198\tvalid_1's rmse: 0.220056\n",
      "[2150]\ttraining's rmse: 0.214157\tvalid_1's rmse: 0.220049\n",
      "[2160]\ttraining's rmse: 0.214122\tvalid_1's rmse: 0.220049\n",
      "[2170]\ttraining's rmse: 0.214082\tvalid_1's rmse: 0.22005\n",
      "[2180]\ttraining's rmse: 0.214044\tvalid_1's rmse: 0.220057\n",
      "[2190]\ttraining's rmse: 0.21401\tvalid_1's rmse: 0.220056\n",
      "[2200]\ttraining's rmse: 0.213975\tvalid_1's rmse: 0.220058\n",
      "[2210]\ttraining's rmse: 0.21394\tvalid_1's rmse: 0.220051\n",
      "[2220]\ttraining's rmse: 0.213907\tvalid_1's rmse: 0.220051\n",
      "[2230]\ttraining's rmse: 0.213873\tvalid_1's rmse: 0.220051\n",
      "[2240]\ttraining's rmse: 0.213836\tvalid_1's rmse: 0.22005\n",
      "[2250]\ttraining's rmse: 0.213796\tvalid_1's rmse: 0.220045\n",
      "[2260]\ttraining's rmse: 0.21376\tvalid_1's rmse: 0.220044\n",
      "[2270]\ttraining's rmse: 0.213726\tvalid_1's rmse: 0.220041\n",
      "[2280]\ttraining's rmse: 0.213686\tvalid_1's rmse: 0.220032\n",
      "[2290]\ttraining's rmse: 0.213649\tvalid_1's rmse: 0.220027\n",
      "[2300]\ttraining's rmse: 0.213614\tvalid_1's rmse: 0.220029\n",
      "[2310]\ttraining's rmse: 0.213578\tvalid_1's rmse: 0.220029\n",
      "[2320]\ttraining's rmse: 0.213542\tvalid_1's rmse: 0.220026\n",
      "[2330]\ttraining's rmse: 0.213506\tvalid_1's rmse: 0.220023\n",
      "[2340]\ttraining's rmse: 0.213464\tvalid_1's rmse: 0.220014\n",
      "[2350]\ttraining's rmse: 0.213432\tvalid_1's rmse: 0.220011\n",
      "[2360]\ttraining's rmse: 0.213394\tvalid_1's rmse: 0.220008\n",
      "[2370]\ttraining's rmse: 0.213359\tvalid_1's rmse: 0.220004\n",
      "[2380]\ttraining's rmse: 0.213321\tvalid_1's rmse: 0.219997\n",
      "[2390]\ttraining's rmse: 0.213285\tvalid_1's rmse: 0.219996\n",
      "[2400]\ttraining's rmse: 0.213249\tvalid_1's rmse: 0.219993\n",
      "[2410]\ttraining's rmse: 0.213213\tvalid_1's rmse: 0.219998\n",
      "[2420]\ttraining's rmse: 0.213177\tvalid_1's rmse: 0.219996\n",
      "[2430]\ttraining's rmse: 0.213145\tvalid_1's rmse: 0.219994\n",
      "[2440]\ttraining's rmse: 0.213111\tvalid_1's rmse: 0.219998\n",
      "[2450]\ttraining's rmse: 0.213072\tvalid_1's rmse: 0.219992\n",
      "[2460]\ttraining's rmse: 0.213038\tvalid_1's rmse: 0.21999\n",
      "[2470]\ttraining's rmse: 0.213003\tvalid_1's rmse: 0.219989\n",
      "[2480]\ttraining's rmse: 0.212967\tvalid_1's rmse: 0.219983\n",
      "[2490]\ttraining's rmse: 0.212931\tvalid_1's rmse: 0.21998\n",
      "[2500]\ttraining's rmse: 0.212896\tvalid_1's rmse: 0.219974\n",
      "[2510]\ttraining's rmse: 0.21286\tvalid_1's rmse: 0.219972\n",
      "[2520]\ttraining's rmse: 0.212824\tvalid_1's rmse: 0.219972\n",
      "[2530]\ttraining's rmse: 0.212791\tvalid_1's rmse: 0.219972\n",
      "[2540]\ttraining's rmse: 0.212756\tvalid_1's rmse: 0.219972\n",
      "[2550]\ttraining's rmse: 0.212721\tvalid_1's rmse: 0.219969\n",
      "[2560]\ttraining's rmse: 0.212687\tvalid_1's rmse: 0.219966\n",
      "[2570]\ttraining's rmse: 0.212653\tvalid_1's rmse: 0.219964\n",
      "[2580]\ttraining's rmse: 0.212618\tvalid_1's rmse: 0.219956\n",
      "[2590]\ttraining's rmse: 0.212579\tvalid_1's rmse: 0.219953\n",
      "[2600]\ttraining's rmse: 0.212542\tvalid_1's rmse: 0.219954\n",
      "[2610]\ttraining's rmse: 0.212507\tvalid_1's rmse: 0.219952\n",
      "[2620]\ttraining's rmse: 0.212474\tvalid_1's rmse: 0.219952\n",
      "[2630]\ttraining's rmse: 0.212437\tvalid_1's rmse: 0.219949\n",
      "[2640]\ttraining's rmse: 0.212397\tvalid_1's rmse: 0.219943\n",
      "[2650]\ttraining's rmse: 0.212363\tvalid_1's rmse: 0.21994\n",
      "[2660]\ttraining's rmse: 0.212329\tvalid_1's rmse: 0.219936\n",
      "[2670]\ttraining's rmse: 0.212296\tvalid_1's rmse: 0.219938\n",
      "[2680]\ttraining's rmse: 0.21226\tvalid_1's rmse: 0.219934\n",
      "[2690]\ttraining's rmse: 0.212225\tvalid_1's rmse: 0.219934\n",
      "[2700]\ttraining's rmse: 0.21219\tvalid_1's rmse: 0.219929\n",
      "[2710]\ttraining's rmse: 0.212156\tvalid_1's rmse: 0.219929\n",
      "[2720]\ttraining's rmse: 0.212118\tvalid_1's rmse: 0.219926\n",
      "[2730]\ttraining's rmse: 0.212087\tvalid_1's rmse: 0.219922\n",
      "[2740]\ttraining's rmse: 0.212054\tvalid_1's rmse: 0.21992\n",
      "[2750]\ttraining's rmse: 0.212021\tvalid_1's rmse: 0.219921\n",
      "[2760]\ttraining's rmse: 0.211988\tvalid_1's rmse: 0.219922\n",
      "[2770]\ttraining's rmse: 0.211953\tvalid_1's rmse: 0.21992\n",
      "[2780]\ttraining's rmse: 0.211918\tvalid_1's rmse: 0.219915\n",
      "[2790]\ttraining's rmse: 0.211882\tvalid_1's rmse: 0.219912\n",
      "[2800]\ttraining's rmse: 0.211848\tvalid_1's rmse: 0.219914\n",
      "[2810]\ttraining's rmse: 0.211812\tvalid_1's rmse: 0.219913\n",
      "[2820]\ttraining's rmse: 0.211779\tvalid_1's rmse: 0.219914\n",
      "[2830]\ttraining's rmse: 0.211741\tvalid_1's rmse: 0.219905\n",
      "[2840]\ttraining's rmse: 0.21171\tvalid_1's rmse: 0.219903\n",
      "[2850]\ttraining's rmse: 0.21168\tvalid_1's rmse: 0.219905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2860]\ttraining's rmse: 0.211641\tvalid_1's rmse: 0.219899\n",
      "[2870]\ttraining's rmse: 0.21161\tvalid_1's rmse: 0.219892\n",
      "[2880]\ttraining's rmse: 0.211577\tvalid_1's rmse: 0.219887\n",
      "[2890]\ttraining's rmse: 0.211537\tvalid_1's rmse: 0.219885\n",
      "[2900]\ttraining's rmse: 0.211505\tvalid_1's rmse: 0.219883\n",
      "[2910]\ttraining's rmse: 0.211472\tvalid_1's rmse: 0.219883\n",
      "[2920]\ttraining's rmse: 0.211436\tvalid_1's rmse: 0.219882\n",
      "[2930]\ttraining's rmse: 0.211403\tvalid_1's rmse: 0.219881\n",
      "[2940]\ttraining's rmse: 0.211369\tvalid_1's rmse: 0.21988\n",
      "[2950]\ttraining's rmse: 0.211335\tvalid_1's rmse: 0.219878\n",
      "[2960]\ttraining's rmse: 0.2113\tvalid_1's rmse: 0.21987\n",
      "[2970]\ttraining's rmse: 0.211265\tvalid_1's rmse: 0.219868\n",
      "[2980]\ttraining's rmse: 0.211231\tvalid_1's rmse: 0.219864\n",
      "[2990]\ttraining's rmse: 0.211198\tvalid_1's rmse: 0.219863\n",
      "[3000]\ttraining's rmse: 0.211167\tvalid_1's rmse: 0.219858\n",
      "[3010]\ttraining's rmse: 0.211133\tvalid_1's rmse: 0.219859\n",
      "[3020]\ttraining's rmse: 0.211098\tvalid_1's rmse: 0.219859\n",
      "[3030]\ttraining's rmse: 0.211067\tvalid_1's rmse: 0.219852\n",
      "[3040]\ttraining's rmse: 0.211035\tvalid_1's rmse: 0.219856\n",
      "[3050]\ttraining's rmse: 0.210999\tvalid_1's rmse: 0.219852\n",
      "[3060]\ttraining's rmse: 0.210966\tvalid_1's rmse: 0.219849\n",
      "[3070]\ttraining's rmse: 0.210931\tvalid_1's rmse: 0.219845\n",
      "[3080]\ttraining's rmse: 0.210891\tvalid_1's rmse: 0.219841\n",
      "[3090]\ttraining's rmse: 0.210855\tvalid_1's rmse: 0.219838\n",
      "[3100]\ttraining's rmse: 0.21082\tvalid_1's rmse: 0.219835\n",
      "[3110]\ttraining's rmse: 0.210786\tvalid_1's rmse: 0.219837\n",
      "[3120]\ttraining's rmse: 0.210752\tvalid_1's rmse: 0.219838\n",
      "[3130]\ttraining's rmse: 0.210716\tvalid_1's rmse: 0.219834\n",
      "[3140]\ttraining's rmse: 0.210685\tvalid_1's rmse: 0.219828\n",
      "[3150]\ttraining's rmse: 0.210648\tvalid_1's rmse: 0.219827\n",
      "[3160]\ttraining's rmse: 0.210613\tvalid_1's rmse: 0.219823\n",
      "[3170]\ttraining's rmse: 0.210579\tvalid_1's rmse: 0.219823\n",
      "[3180]\ttraining's rmse: 0.210548\tvalid_1's rmse: 0.219823\n",
      "[3190]\ttraining's rmse: 0.210512\tvalid_1's rmse: 0.219817\n",
      "[3200]\ttraining's rmse: 0.210479\tvalid_1's rmse: 0.219815\n",
      "[3210]\ttraining's rmse: 0.210448\tvalid_1's rmse: 0.219812\n",
      "[3220]\ttraining's rmse: 0.210416\tvalid_1's rmse: 0.219812\n",
      "[3230]\ttraining's rmse: 0.210379\tvalid_1's rmse: 0.219811\n",
      "[3240]\ttraining's rmse: 0.210349\tvalid_1's rmse: 0.219809\n",
      "[3250]\ttraining's rmse: 0.210316\tvalid_1's rmse: 0.219808\n",
      "[3260]\ttraining's rmse: 0.210281\tvalid_1's rmse: 0.219805\n",
      "[3270]\ttraining's rmse: 0.210248\tvalid_1's rmse: 0.219802\n",
      "[3280]\ttraining's rmse: 0.210212\tvalid_1's rmse: 0.219798\n",
      "[3290]\ttraining's rmse: 0.210182\tvalid_1's rmse: 0.219798\n",
      "[3300]\ttraining's rmse: 0.210151\tvalid_1's rmse: 0.219796\n",
      "[3310]\ttraining's rmse: 0.210119\tvalid_1's rmse: 0.219792\n",
      "[3320]\ttraining's rmse: 0.210087\tvalid_1's rmse: 0.219794\n",
      "[3330]\ttraining's rmse: 0.210056\tvalid_1's rmse: 0.219794\n",
      "[3340]\ttraining's rmse: 0.210026\tvalid_1's rmse: 0.21979\n",
      "[3350]\ttraining's rmse: 0.209994\tvalid_1's rmse: 0.219786\n",
      "[3360]\ttraining's rmse: 0.20996\tvalid_1's rmse: 0.219783\n",
      "[3370]\ttraining's rmse: 0.209932\tvalid_1's rmse: 0.219781\n",
      "[3380]\ttraining's rmse: 0.209899\tvalid_1's rmse: 0.219772\n",
      "[3390]\ttraining's rmse: 0.209868\tvalid_1's rmse: 0.21977\n",
      "[3400]\ttraining's rmse: 0.209831\tvalid_1's rmse: 0.219768\n",
      "[3410]\ttraining's rmse: 0.209799\tvalid_1's rmse: 0.219774\n",
      "[3420]\ttraining's rmse: 0.209767\tvalid_1's rmse: 0.219778\n",
      "[3430]\ttraining's rmse: 0.209738\tvalid_1's rmse: 0.219779\n",
      "[3440]\ttraining's rmse: 0.209707\tvalid_1's rmse: 0.21978\n",
      "[3450]\ttraining's rmse: 0.209674\tvalid_1's rmse: 0.219777\n",
      "[3460]\ttraining's rmse: 0.209641\tvalid_1's rmse: 0.219778\n",
      "[3470]\ttraining's rmse: 0.209607\tvalid_1's rmse: 0.219779\n",
      "[3480]\ttraining's rmse: 0.209573\tvalid_1's rmse: 0.219775\n",
      "[3490]\ttraining's rmse: 0.209543\tvalid_1's rmse: 0.219772\n",
      "[3500]\ttraining's rmse: 0.209514\tvalid_1's rmse: 0.21977\n",
      "Early stopping, best iteration is:\n",
      "[3400]\ttraining's rmse: 0.209831\tvalid_1's rmse: 0.219768\n",
      "rmse:0.21969906274245096\n",
      "params: \n",
      "max_depth: 10\n",
      "num_leaves: 40\n",
      "min_data_per_leaf: 80\n",
      "min_child_weight: 2.0\n",
      "subsample: 0.6\n",
      "subsample_freq: 6\n",
      "lambda_l1: 15.0\n",
      "lambda_l2: 15.0\n",
      "feature_fraction: 0.9\n",
      "learning rate: 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.240047\tvalid_1's rmse: 0.238388\n",
      "[20]\ttraining's rmse: 0.233766\tvalid_1's rmse: 0.231703\n",
      "[30]\ttraining's rmse: 0.231388\tvalid_1's rmse: 0.229201\n",
      "[40]\ttraining's rmse: 0.230142\tvalid_1's rmse: 0.227944\n",
      "[50]\ttraining's rmse: 0.229277\tvalid_1's rmse: 0.227122\n",
      "[60]\ttraining's rmse: 0.228667\tvalid_1's rmse: 0.226551\n",
      "[70]\ttraining's rmse: 0.228191\tvalid_1's rmse: 0.226112\n",
      "[80]\ttraining's rmse: 0.227753\tvalid_1's rmse: 0.225745\n",
      "[90]\ttraining's rmse: 0.227336\tvalid_1's rmse: 0.225373\n",
      "[100]\ttraining's rmse: 0.227004\tvalid_1's rmse: 0.225121\n",
      "[110]\ttraining's rmse: 0.226722\tvalid_1's rmse: 0.224893\n",
      "[120]\ttraining's rmse: 0.226428\tvalid_1's rmse: 0.224647\n",
      "[130]\ttraining's rmse: 0.226188\tvalid_1's rmse: 0.224464\n",
      "[140]\ttraining's rmse: 0.225952\tvalid_1's rmse: 0.22429\n",
      "[150]\ttraining's rmse: 0.225727\tvalid_1's rmse: 0.224111\n",
      "[160]\ttraining's rmse: 0.225512\tvalid_1's rmse: 0.223962\n",
      "[170]\ttraining's rmse: 0.225326\tvalid_1's rmse: 0.223837\n",
      "[180]\ttraining's rmse: 0.225142\tvalid_1's rmse: 0.223704\n",
      "[190]\ttraining's rmse: 0.224991\tvalid_1's rmse: 0.223608\n",
      "[200]\ttraining's rmse: 0.224843\tvalid_1's rmse: 0.223506\n",
      "[210]\ttraining's rmse: 0.224682\tvalid_1's rmse: 0.223402\n",
      "[220]\ttraining's rmse: 0.224522\tvalid_1's rmse: 0.223291\n",
      "[230]\ttraining's rmse: 0.224347\tvalid_1's rmse: 0.223171\n",
      "[240]\ttraining's rmse: 0.224212\tvalid_1's rmse: 0.223097\n",
      "[250]\ttraining's rmse: 0.224079\tvalid_1's rmse: 0.223017\n",
      "[260]\ttraining's rmse: 0.22397\tvalid_1's rmse: 0.222956\n",
      "[270]\ttraining's rmse: 0.22386\tvalid_1's rmse: 0.222894\n",
      "[280]\ttraining's rmse: 0.223743\tvalid_1's rmse: 0.222861\n",
      "[290]\ttraining's rmse: 0.223632\tvalid_1's rmse: 0.222796\n",
      "[300]\ttraining's rmse: 0.223499\tvalid_1's rmse: 0.222713\n",
      "[310]\ttraining's rmse: 0.223389\tvalid_1's rmse: 0.22267\n",
      "[320]\ttraining's rmse: 0.223288\tvalid_1's rmse: 0.222631\n",
      "[330]\ttraining's rmse: 0.223197\tvalid_1's rmse: 0.22259\n",
      "[340]\ttraining's rmse: 0.223093\tvalid_1's rmse: 0.222524\n",
      "[350]\ttraining's rmse: 0.222995\tvalid_1's rmse: 0.222479\n",
      "[360]\ttraining's rmse: 0.222908\tvalid_1's rmse: 0.222434\n",
      "[370]\ttraining's rmse: 0.222806\tvalid_1's rmse: 0.222392\n",
      "[380]\ttraining's rmse: 0.222714\tvalid_1's rmse: 0.222347\n",
      "[390]\ttraining's rmse: 0.222625\tvalid_1's rmse: 0.222309\n",
      "[400]\ttraining's rmse: 0.222536\tvalid_1's rmse: 0.222264\n",
      "[410]\ttraining's rmse: 0.222444\tvalid_1's rmse: 0.222211\n",
      "[420]\ttraining's rmse: 0.222355\tvalid_1's rmse: 0.222163\n",
      "[430]\ttraining's rmse: 0.222264\tvalid_1's rmse: 0.222151\n",
      "[440]\ttraining's rmse: 0.222175\tvalid_1's rmse: 0.222112\n",
      "[450]\ttraining's rmse: 0.222095\tvalid_1's rmse: 0.222088\n",
      "[460]\ttraining's rmse: 0.222012\tvalid_1's rmse: 0.222053\n",
      "[470]\ttraining's rmse: 0.221941\tvalid_1's rmse: 0.222026\n",
      "[480]\ttraining's rmse: 0.221862\tvalid_1's rmse: 0.221988\n",
      "[490]\ttraining's rmse: 0.221788\tvalid_1's rmse: 0.221961\n",
      "[500]\ttraining's rmse: 0.221717\tvalid_1's rmse: 0.221941\n",
      "[510]\ttraining's rmse: 0.221645\tvalid_1's rmse: 0.221922\n",
      "[520]\ttraining's rmse: 0.22156\tvalid_1's rmse: 0.22188\n",
      "[530]\ttraining's rmse: 0.221487\tvalid_1's rmse: 0.221849\n",
      "[540]\ttraining's rmse: 0.221408\tvalid_1's rmse: 0.221809\n",
      "[550]\ttraining's rmse: 0.221337\tvalid_1's rmse: 0.221782\n",
      "[560]\ttraining's rmse: 0.221256\tvalid_1's rmse: 0.221748\n",
      "[570]\ttraining's rmse: 0.221176\tvalid_1's rmse: 0.221724\n",
      "[580]\ttraining's rmse: 0.221112\tvalid_1's rmse: 0.221702\n",
      "[590]\ttraining's rmse: 0.221052\tvalid_1's rmse: 0.221691\n",
      "[600]\ttraining's rmse: 0.220984\tvalid_1's rmse: 0.221671\n",
      "[610]\ttraining's rmse: 0.220912\tvalid_1's rmse: 0.221625\n",
      "[620]\ttraining's rmse: 0.22085\tvalid_1's rmse: 0.22161\n",
      "[630]\ttraining's rmse: 0.220782\tvalid_1's rmse: 0.221577\n",
      "[640]\ttraining's rmse: 0.220713\tvalid_1's rmse: 0.221542\n",
      "[650]\ttraining's rmse: 0.220651\tvalid_1's rmse: 0.221529\n",
      "[660]\ttraining's rmse: 0.220592\tvalid_1's rmse: 0.221511\n",
      "[670]\ttraining's rmse: 0.220531\tvalid_1's rmse: 0.221489\n",
      "[680]\ttraining's rmse: 0.220466\tvalid_1's rmse: 0.221475\n",
      "[690]\ttraining's rmse: 0.220402\tvalid_1's rmse: 0.221458\n",
      "[700]\ttraining's rmse: 0.220344\tvalid_1's rmse: 0.221441\n",
      "[710]\ttraining's rmse: 0.220282\tvalid_1's rmse: 0.221422\n",
      "[720]\ttraining's rmse: 0.220224\tvalid_1's rmse: 0.221412\n",
      "[730]\ttraining's rmse: 0.220158\tvalid_1's rmse: 0.2214\n",
      "[740]\ttraining's rmse: 0.220089\tvalid_1's rmse: 0.221383\n",
      "[750]\ttraining's rmse: 0.220035\tvalid_1's rmse: 0.22138\n",
      "[760]\ttraining's rmse: 0.219973\tvalid_1's rmse: 0.221367\n",
      "[770]\ttraining's rmse: 0.21991\tvalid_1's rmse: 0.221341\n",
      "[780]\ttraining's rmse: 0.219852\tvalid_1's rmse: 0.221334\n",
      "[790]\ttraining's rmse: 0.219791\tvalid_1's rmse: 0.221318\n",
      "[800]\ttraining's rmse: 0.219727\tvalid_1's rmse: 0.221304\n",
      "[810]\ttraining's rmse: 0.21967\tvalid_1's rmse: 0.221303\n",
      "[820]\ttraining's rmse: 0.219616\tvalid_1's rmse: 0.221286\n",
      "[830]\ttraining's rmse: 0.219551\tvalid_1's rmse: 0.22127\n",
      "[840]\ttraining's rmse: 0.219492\tvalid_1's rmse: 0.221251\n",
      "[850]\ttraining's rmse: 0.219436\tvalid_1's rmse: 0.221237\n",
      "[860]\ttraining's rmse: 0.219377\tvalid_1's rmse: 0.221233\n",
      "[870]\ttraining's rmse: 0.21932\tvalid_1's rmse: 0.221218\n",
      "[880]\ttraining's rmse: 0.219268\tvalid_1's rmse: 0.221217\n",
      "[890]\ttraining's rmse: 0.219208\tvalid_1's rmse: 0.221199\n",
      "[900]\ttraining's rmse: 0.219145\tvalid_1's rmse: 0.221179\n",
      "[910]\ttraining's rmse: 0.219085\tvalid_1's rmse: 0.221163\n",
      "[920]\ttraining's rmse: 0.219024\tvalid_1's rmse: 0.221146\n",
      "[930]\ttraining's rmse: 0.218966\tvalid_1's rmse: 0.221133\n",
      "[940]\ttraining's rmse: 0.218912\tvalid_1's rmse: 0.221127\n",
      "[950]\ttraining's rmse: 0.218853\tvalid_1's rmse: 0.221116\n",
      "[960]\ttraining's rmse: 0.218798\tvalid_1's rmse: 0.221111\n",
      "[970]\ttraining's rmse: 0.218743\tvalid_1's rmse: 0.221095\n",
      "[980]\ttraining's rmse: 0.218694\tvalid_1's rmse: 0.221084\n",
      "[990]\ttraining's rmse: 0.218641\tvalid_1's rmse: 0.221072\n",
      "[1000]\ttraining's rmse: 0.218583\tvalid_1's rmse: 0.221071\n",
      "[1010]\ttraining's rmse: 0.218526\tvalid_1's rmse: 0.221059\n",
      "[1020]\ttraining's rmse: 0.218476\tvalid_1's rmse: 0.221051\n",
      "[1030]\ttraining's rmse: 0.21842\tvalid_1's rmse: 0.221041\n",
      "[1040]\ttraining's rmse: 0.218369\tvalid_1's rmse: 0.221031\n",
      "[1050]\ttraining's rmse: 0.218312\tvalid_1's rmse: 0.221012\n",
      "[1060]\ttraining's rmse: 0.218253\tvalid_1's rmse: 0.221003\n",
      "[1070]\ttraining's rmse: 0.218201\tvalid_1's rmse: 0.220992\n",
      "[1080]\ttraining's rmse: 0.218149\tvalid_1's rmse: 0.220989\n",
      "[1090]\ttraining's rmse: 0.218101\tvalid_1's rmse: 0.220987\n",
      "[1100]\ttraining's rmse: 0.21805\tvalid_1's rmse: 0.22098\n",
      "[1110]\ttraining's rmse: 0.218\tvalid_1's rmse: 0.220974\n",
      "[1120]\ttraining's rmse: 0.217949\tvalid_1's rmse: 0.220976\n",
      "[1130]\ttraining's rmse: 0.217901\tvalid_1's rmse: 0.220973\n",
      "[1140]\ttraining's rmse: 0.217854\tvalid_1's rmse: 0.220971\n",
      "[1150]\ttraining's rmse: 0.217803\tvalid_1's rmse: 0.220951\n",
      "[1160]\ttraining's rmse: 0.217752\tvalid_1's rmse: 0.220941\n",
      "[1170]\ttraining's rmse: 0.217698\tvalid_1's rmse: 0.220917\n",
      "[1180]\ttraining's rmse: 0.217647\tvalid_1's rmse: 0.220904\n",
      "[1190]\ttraining's rmse: 0.217595\tvalid_1's rmse: 0.220902\n",
      "[1200]\ttraining's rmse: 0.217548\tvalid_1's rmse: 0.220894\n",
      "[1210]\ttraining's rmse: 0.2175\tvalid_1's rmse: 0.220892\n",
      "[1220]\ttraining's rmse: 0.217449\tvalid_1's rmse: 0.220882\n",
      "[1230]\ttraining's rmse: 0.217404\tvalid_1's rmse: 0.220875\n",
      "[1240]\ttraining's rmse: 0.217359\tvalid_1's rmse: 0.220864\n",
      "[1250]\ttraining's rmse: 0.217311\tvalid_1's rmse: 0.220855\n",
      "[1260]\ttraining's rmse: 0.217265\tvalid_1's rmse: 0.220848\n",
      "[1270]\ttraining's rmse: 0.217215\tvalid_1's rmse: 0.220846\n",
      "[1280]\ttraining's rmse: 0.217165\tvalid_1's rmse: 0.22084\n",
      "[1290]\ttraining's rmse: 0.21712\tvalid_1's rmse: 0.220836\n",
      "[1300]\ttraining's rmse: 0.21707\tvalid_1's rmse: 0.220812\n",
      "[1310]\ttraining's rmse: 0.217025\tvalid_1's rmse: 0.220799\n",
      "[1320]\ttraining's rmse: 0.216974\tvalid_1's rmse: 0.220796\n",
      "[1330]\ttraining's rmse: 0.216927\tvalid_1's rmse: 0.2208\n",
      "[1340]\ttraining's rmse: 0.216878\tvalid_1's rmse: 0.220788\n",
      "[1350]\ttraining's rmse: 0.216829\tvalid_1's rmse: 0.220781\n",
      "[1360]\ttraining's rmse: 0.216779\tvalid_1's rmse: 0.220772\n",
      "[1370]\ttraining's rmse: 0.216727\tvalid_1's rmse: 0.22076\n",
      "[1380]\ttraining's rmse: 0.216681\tvalid_1's rmse: 0.220758\n",
      "[1390]\ttraining's rmse: 0.216637\tvalid_1's rmse: 0.220748\n",
      "[1400]\ttraining's rmse: 0.216588\tvalid_1's rmse: 0.220737\n",
      "[1410]\ttraining's rmse: 0.216544\tvalid_1's rmse: 0.220737\n",
      "[1420]\ttraining's rmse: 0.216494\tvalid_1's rmse: 0.22073\n",
      "[1430]\ttraining's rmse: 0.216443\tvalid_1's rmse: 0.220719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1440]\ttraining's rmse: 0.216401\tvalid_1's rmse: 0.220716\n",
      "[1450]\ttraining's rmse: 0.216355\tvalid_1's rmse: 0.220713\n",
      "[1460]\ttraining's rmse: 0.216311\tvalid_1's rmse: 0.220702\n",
      "[1470]\ttraining's rmse: 0.216267\tvalid_1's rmse: 0.220697\n",
      "[1480]\ttraining's rmse: 0.216217\tvalid_1's rmse: 0.220692\n",
      "[1490]\ttraining's rmse: 0.216172\tvalid_1's rmse: 0.220682\n",
      "[1500]\ttraining's rmse: 0.216129\tvalid_1's rmse: 0.22067\n",
      "[1510]\ttraining's rmse: 0.216083\tvalid_1's rmse: 0.22067\n",
      "[1520]\ttraining's rmse: 0.216037\tvalid_1's rmse: 0.22067\n",
      "[1530]\ttraining's rmse: 0.215998\tvalid_1's rmse: 0.22066\n",
      "[1540]\ttraining's rmse: 0.215953\tvalid_1's rmse: 0.220657\n",
      "[1550]\ttraining's rmse: 0.215903\tvalid_1's rmse: 0.220653\n",
      "[1560]\ttraining's rmse: 0.215856\tvalid_1's rmse: 0.220648\n",
      "[1570]\ttraining's rmse: 0.215813\tvalid_1's rmse: 0.220642\n",
      "[1580]\ttraining's rmse: 0.215764\tvalid_1's rmse: 0.220643\n",
      "[1590]\ttraining's rmse: 0.215718\tvalid_1's rmse: 0.22064\n",
      "[1600]\ttraining's rmse: 0.215675\tvalid_1's rmse: 0.220631\n",
      "[1610]\ttraining's rmse: 0.215631\tvalid_1's rmse: 0.220622\n",
      "[1620]\ttraining's rmse: 0.215588\tvalid_1's rmse: 0.220617\n",
      "[1630]\ttraining's rmse: 0.215543\tvalid_1's rmse: 0.220608\n",
      "[1640]\ttraining's rmse: 0.215501\tvalid_1's rmse: 0.22061\n",
      "[1650]\ttraining's rmse: 0.215448\tvalid_1's rmse: 0.220596\n",
      "[1660]\ttraining's rmse: 0.215408\tvalid_1's rmse: 0.220597\n",
      "[1670]\ttraining's rmse: 0.215362\tvalid_1's rmse: 0.220599\n",
      "[1680]\ttraining's rmse: 0.215314\tvalid_1's rmse: 0.220597\n",
      "[1690]\ttraining's rmse: 0.215269\tvalid_1's rmse: 0.220604\n",
      "[1700]\ttraining's rmse: 0.215225\tvalid_1's rmse: 0.220603\n",
      "[1710]\ttraining's rmse: 0.215183\tvalid_1's rmse: 0.220599\n",
      "[1720]\ttraining's rmse: 0.215141\tvalid_1's rmse: 0.220596\n",
      "[1730]\ttraining's rmse: 0.215099\tvalid_1's rmse: 0.220601\n",
      "[1740]\ttraining's rmse: 0.215056\tvalid_1's rmse: 0.220599\n",
      "[1750]\ttraining's rmse: 0.21501\tvalid_1's rmse: 0.220598\n",
      "[1760]\ttraining's rmse: 0.214968\tvalid_1's rmse: 0.220594\n",
      "[1770]\ttraining's rmse: 0.214926\tvalid_1's rmse: 0.220591\n",
      "[1780]\ttraining's rmse: 0.214884\tvalid_1's rmse: 0.220593\n",
      "[1790]\ttraining's rmse: 0.214844\tvalid_1's rmse: 0.220597\n",
      "[1800]\ttraining's rmse: 0.2148\tvalid_1's rmse: 0.220602\n",
      "[1810]\ttraining's rmse: 0.21476\tvalid_1's rmse: 0.220591\n",
      "[1820]\ttraining's rmse: 0.214721\tvalid_1's rmse: 0.220592\n",
      "[1830]\ttraining's rmse: 0.214679\tvalid_1's rmse: 0.220581\n",
      "[1840]\ttraining's rmse: 0.214635\tvalid_1's rmse: 0.220567\n",
      "[1850]\ttraining's rmse: 0.21459\tvalid_1's rmse: 0.220576\n",
      "[1860]\ttraining's rmse: 0.214548\tvalid_1's rmse: 0.22057\n",
      "[1870]\ttraining's rmse: 0.214506\tvalid_1's rmse: 0.220568\n",
      "[1880]\ttraining's rmse: 0.214461\tvalid_1's rmse: 0.220562\n",
      "[1890]\ttraining's rmse: 0.214421\tvalid_1's rmse: 0.220564\n",
      "[1900]\ttraining's rmse: 0.214382\tvalid_1's rmse: 0.220561\n",
      "[1910]\ttraining's rmse: 0.214339\tvalid_1's rmse: 0.220554\n",
      "[1920]\ttraining's rmse: 0.214296\tvalid_1's rmse: 0.220556\n",
      "[1930]\ttraining's rmse: 0.214253\tvalid_1's rmse: 0.220552\n",
      "[1940]\ttraining's rmse: 0.214209\tvalid_1's rmse: 0.220549\n",
      "[1950]\ttraining's rmse: 0.214166\tvalid_1's rmse: 0.220547\n",
      "[1960]\ttraining's rmse: 0.214121\tvalid_1's rmse: 0.220551\n",
      "[1970]\ttraining's rmse: 0.214079\tvalid_1's rmse: 0.220553\n",
      "[1980]\ttraining's rmse: 0.214041\tvalid_1's rmse: 0.220554\n",
      "[1990]\ttraining's rmse: 0.213996\tvalid_1's rmse: 0.220542\n",
      "[2000]\ttraining's rmse: 0.213959\tvalid_1's rmse: 0.220539\n",
      "[2010]\ttraining's rmse: 0.213917\tvalid_1's rmse: 0.220544\n",
      "[2020]\ttraining's rmse: 0.213874\tvalid_1's rmse: 0.220542\n",
      "[2030]\ttraining's rmse: 0.213832\tvalid_1's rmse: 0.220532\n",
      "[2040]\ttraining's rmse: 0.213791\tvalid_1's rmse: 0.220527\n",
      "[2050]\ttraining's rmse: 0.213748\tvalid_1's rmse: 0.220525\n",
      "[2060]\ttraining's rmse: 0.213704\tvalid_1's rmse: 0.220524\n",
      "[2070]\ttraining's rmse: 0.213665\tvalid_1's rmse: 0.220521\n",
      "[2080]\ttraining's rmse: 0.213623\tvalid_1's rmse: 0.220513\n",
      "[2090]\ttraining's rmse: 0.213581\tvalid_1's rmse: 0.220515\n",
      "[2100]\ttraining's rmse: 0.213534\tvalid_1's rmse: 0.220503\n",
      "[2110]\ttraining's rmse: 0.213492\tvalid_1's rmse: 0.220504\n",
      "[2120]\ttraining's rmse: 0.21345\tvalid_1's rmse: 0.220507\n",
      "[2130]\ttraining's rmse: 0.21341\tvalid_1's rmse: 0.220512\n",
      "[2140]\ttraining's rmse: 0.213367\tvalid_1's rmse: 0.220509\n",
      "[2150]\ttraining's rmse: 0.213329\tvalid_1's rmse: 0.220508\n",
      "[2160]\ttraining's rmse: 0.213289\tvalid_1's rmse: 0.22051\n",
      "[2170]\ttraining's rmse: 0.213245\tvalid_1's rmse: 0.220508\n",
      "[2180]\ttraining's rmse: 0.213205\tvalid_1's rmse: 0.220498\n",
      "[2190]\ttraining's rmse: 0.213167\tvalid_1's rmse: 0.220495\n",
      "[2200]\ttraining's rmse: 0.213128\tvalid_1's rmse: 0.220496\n",
      "[2210]\ttraining's rmse: 0.213087\tvalid_1's rmse: 0.220494\n",
      "[2220]\ttraining's rmse: 0.213045\tvalid_1's rmse: 0.22048\n",
      "[2230]\ttraining's rmse: 0.213002\tvalid_1's rmse: 0.220472\n",
      "[2240]\ttraining's rmse: 0.21296\tvalid_1's rmse: 0.220469\n",
      "[2250]\ttraining's rmse: 0.212921\tvalid_1's rmse: 0.220468\n",
      "[2260]\ttraining's rmse: 0.212879\tvalid_1's rmse: 0.220467\n",
      "[2270]\ttraining's rmse: 0.21284\tvalid_1's rmse: 0.220458\n",
      "[2280]\ttraining's rmse: 0.212799\tvalid_1's rmse: 0.220456\n",
      "[2290]\ttraining's rmse: 0.21276\tvalid_1's rmse: 0.220454\n",
      "[2300]\ttraining's rmse: 0.212721\tvalid_1's rmse: 0.220448\n",
      "[2310]\ttraining's rmse: 0.212687\tvalid_1's rmse: 0.220454\n",
      "[2320]\ttraining's rmse: 0.21265\tvalid_1's rmse: 0.220455\n",
      "[2330]\ttraining's rmse: 0.212611\tvalid_1's rmse: 0.220455\n",
      "[2340]\ttraining's rmse: 0.21257\tvalid_1's rmse: 0.220452\n",
      "[2350]\ttraining's rmse: 0.212533\tvalid_1's rmse: 0.220447\n",
      "[2360]\ttraining's rmse: 0.212492\tvalid_1's rmse: 0.220447\n",
      "[2370]\ttraining's rmse: 0.212456\tvalid_1's rmse: 0.22045\n",
      "[2380]\ttraining's rmse: 0.212416\tvalid_1's rmse: 0.220441\n",
      "[2390]\ttraining's rmse: 0.212377\tvalid_1's rmse: 0.220439\n",
      "[2400]\ttraining's rmse: 0.212337\tvalid_1's rmse: 0.220438\n",
      "[2410]\ttraining's rmse: 0.212299\tvalid_1's rmse: 0.220443\n",
      "[2420]\ttraining's rmse: 0.21226\tvalid_1's rmse: 0.220438\n",
      "[2430]\ttraining's rmse: 0.212221\tvalid_1's rmse: 0.220438\n",
      "[2440]\ttraining's rmse: 0.212182\tvalid_1's rmse: 0.220436\n",
      "[2450]\ttraining's rmse: 0.212142\tvalid_1's rmse: 0.220428\n",
      "[2460]\ttraining's rmse: 0.212104\tvalid_1's rmse: 0.220429\n",
      "[2470]\ttraining's rmse: 0.212063\tvalid_1's rmse: 0.220427\n",
      "[2480]\ttraining's rmse: 0.212025\tvalid_1's rmse: 0.220428\n",
      "[2490]\ttraining's rmse: 0.211986\tvalid_1's rmse: 0.220432\n",
      "[2500]\ttraining's rmse: 0.211951\tvalid_1's rmse: 0.220433\n",
      "[2510]\ttraining's rmse: 0.21191\tvalid_1's rmse: 0.220428\n",
      "[2520]\ttraining's rmse: 0.211874\tvalid_1's rmse: 0.220424\n",
      "[2530]\ttraining's rmse: 0.211837\tvalid_1's rmse: 0.220425\n",
      "[2540]\ttraining's rmse: 0.211798\tvalid_1's rmse: 0.220421\n",
      "[2550]\ttraining's rmse: 0.211759\tvalid_1's rmse: 0.220429\n",
      "[2560]\ttraining's rmse: 0.211714\tvalid_1's rmse: 0.220425\n",
      "[2570]\ttraining's rmse: 0.211675\tvalid_1's rmse: 0.220425\n",
      "[2580]\ttraining's rmse: 0.211635\tvalid_1's rmse: 0.220421\n",
      "[2590]\ttraining's rmse: 0.211595\tvalid_1's rmse: 0.220413\n",
      "[2600]\ttraining's rmse: 0.211556\tvalid_1's rmse: 0.220408\n",
      "[2610]\ttraining's rmse: 0.211518\tvalid_1's rmse: 0.220404\n",
      "[2620]\ttraining's rmse: 0.211478\tvalid_1's rmse: 0.220398\n",
      "[2630]\ttraining's rmse: 0.211439\tvalid_1's rmse: 0.220401\n",
      "[2640]\ttraining's rmse: 0.211396\tvalid_1's rmse: 0.220404\n",
      "[2650]\ttraining's rmse: 0.211358\tvalid_1's rmse: 0.220401\n",
      "[2660]\ttraining's rmse: 0.211325\tvalid_1's rmse: 0.220396\n",
      "[2670]\ttraining's rmse: 0.211287\tvalid_1's rmse: 0.220396\n",
      "[2680]\ttraining's rmse: 0.211246\tvalid_1's rmse: 0.220397\n",
      "[2690]\ttraining's rmse: 0.211208\tvalid_1's rmse: 0.220392\n",
      "[2700]\ttraining's rmse: 0.211173\tvalid_1's rmse: 0.22039\n",
      "[2710]\ttraining's rmse: 0.211134\tvalid_1's rmse: 0.220389\n",
      "[2720]\ttraining's rmse: 0.21109\tvalid_1's rmse: 0.220386\n",
      "[2730]\ttraining's rmse: 0.211053\tvalid_1's rmse: 0.220387\n",
      "[2740]\ttraining's rmse: 0.211013\tvalid_1's rmse: 0.220384\n",
      "[2750]\ttraining's rmse: 0.210973\tvalid_1's rmse: 0.22038\n",
      "[2760]\ttraining's rmse: 0.210934\tvalid_1's rmse: 0.220383\n",
      "[2770]\ttraining's rmse: 0.210894\tvalid_1's rmse: 0.220381\n",
      "[2780]\ttraining's rmse: 0.210858\tvalid_1's rmse: 0.220384\n",
      "[2790]\ttraining's rmse: 0.210819\tvalid_1's rmse: 0.220384\n",
      "[2800]\ttraining's rmse: 0.210783\tvalid_1's rmse: 0.220384\n",
      "[2810]\ttraining's rmse: 0.210745\tvalid_1's rmse: 0.220385\n",
      "[2820]\ttraining's rmse: 0.210707\tvalid_1's rmse: 0.220381\n",
      "[2830]\ttraining's rmse: 0.210667\tvalid_1's rmse: 0.220378\n",
      "[2840]\ttraining's rmse: 0.210631\tvalid_1's rmse: 0.220368\n",
      "[2850]\ttraining's rmse: 0.210594\tvalid_1's rmse: 0.22037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2860]\ttraining's rmse: 0.210555\tvalid_1's rmse: 0.220366\n",
      "[2870]\ttraining's rmse: 0.210517\tvalid_1's rmse: 0.220359\n",
      "[2880]\ttraining's rmse: 0.210482\tvalid_1's rmse: 0.22036\n",
      "[2890]\ttraining's rmse: 0.210448\tvalid_1's rmse: 0.22036\n",
      "[2900]\ttraining's rmse: 0.210412\tvalid_1's rmse: 0.220361\n",
      "[2910]\ttraining's rmse: 0.210374\tvalid_1's rmse: 0.220363\n",
      "[2920]\ttraining's rmse: 0.210337\tvalid_1's rmse: 0.220355\n",
      "[2930]\ttraining's rmse: 0.210299\tvalid_1's rmse: 0.220354\n",
      "[2940]\ttraining's rmse: 0.210262\tvalid_1's rmse: 0.220356\n",
      "[2950]\ttraining's rmse: 0.210224\tvalid_1's rmse: 0.220359\n",
      "[2960]\ttraining's rmse: 0.210188\tvalid_1's rmse: 0.220363\n",
      "[2970]\ttraining's rmse: 0.210152\tvalid_1's rmse: 0.220359\n",
      "[2980]\ttraining's rmse: 0.210114\tvalid_1's rmse: 0.220351\n",
      "[2990]\ttraining's rmse: 0.210075\tvalid_1's rmse: 0.220348\n",
      "[3000]\ttraining's rmse: 0.210038\tvalid_1's rmse: 0.220347\n",
      "[3010]\ttraining's rmse: 0.209995\tvalid_1's rmse: 0.220356\n",
      "[3020]\ttraining's rmse: 0.209958\tvalid_1's rmse: 0.220355\n",
      "[3030]\ttraining's rmse: 0.209924\tvalid_1's rmse: 0.220353\n",
      "[3040]\ttraining's rmse: 0.209884\tvalid_1's rmse: 0.220349\n",
      "[3050]\ttraining's rmse: 0.209846\tvalid_1's rmse: 0.220334\n",
      "[3060]\ttraining's rmse: 0.209811\tvalid_1's rmse: 0.220334\n",
      "[3070]\ttraining's rmse: 0.209771\tvalid_1's rmse: 0.22034\n",
      "[3080]\ttraining's rmse: 0.209733\tvalid_1's rmse: 0.220339\n",
      "[3090]\ttraining's rmse: 0.209694\tvalid_1's rmse: 0.220332\n",
      "[3100]\ttraining's rmse: 0.209656\tvalid_1's rmse: 0.220335\n",
      "[3110]\ttraining's rmse: 0.209623\tvalid_1's rmse: 0.220335\n",
      "[3120]\ttraining's rmse: 0.209588\tvalid_1's rmse: 0.22034\n",
      "[3130]\ttraining's rmse: 0.209551\tvalid_1's rmse: 0.220344\n",
      "[3140]\ttraining's rmse: 0.209514\tvalid_1's rmse: 0.220342\n",
      "[3150]\ttraining's rmse: 0.209475\tvalid_1's rmse: 0.220344\n",
      "[3160]\ttraining's rmse: 0.209438\tvalid_1's rmse: 0.220335\n",
      "[3170]\ttraining's rmse: 0.209402\tvalid_1's rmse: 0.220336\n",
      "[3180]\ttraining's rmse: 0.209367\tvalid_1's rmse: 0.220336\n",
      "[3190]\ttraining's rmse: 0.209333\tvalid_1's rmse: 0.220342\n",
      "Early stopping, best iteration is:\n",
      "[3093]\ttraining's rmse: 0.209682\tvalid_1's rmse: 0.220331\n",
      "rmse:0.2202559082518182\n",
      "params: \n",
      "max_depth: 55\n",
      "num_leaves: 78\n",
      "min_data_per_leaf: 74\n",
      "min_child_weight: 0.209292342313\n",
      "subsample: 0.896005148691\n",
      "subsample_freq: 6\n",
      "lambda_l1: 0.203017568113\n",
      "lambda_l2: 0.0933628890453\n",
      "feature_fraction: 0.849689912719\n",
      "learning rate: 0.101397453962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.234758\tvalid_1's rmse: 0.232981\n",
      "[20]\ttraining's rmse: 0.229579\tvalid_1's rmse: 0.22756\n",
      "[30]\ttraining's rmse: 0.227685\tvalid_1's rmse: 0.225775\n",
      "[40]\ttraining's rmse: 0.226545\tvalid_1's rmse: 0.224855\n",
      "[50]\ttraining's rmse: 0.225799\tvalid_1's rmse: 0.224319\n",
      "[60]\ttraining's rmse: 0.225001\tvalid_1's rmse: 0.223748\n",
      "[70]\ttraining's rmse: 0.2244\tvalid_1's rmse: 0.223343\n",
      "[80]\ttraining's rmse: 0.223822\tvalid_1's rmse: 0.22298\n",
      "[90]\ttraining's rmse: 0.223353\tvalid_1's rmse: 0.222726\n",
      "[100]\ttraining's rmse: 0.222936\tvalid_1's rmse: 0.222561\n",
      "[110]\ttraining's rmse: 0.222526\tvalid_1's rmse: 0.222368\n",
      "[120]\ttraining's rmse: 0.222148\tvalid_1's rmse: 0.222223\n",
      "[130]\ttraining's rmse: 0.221761\tvalid_1's rmse: 0.222066\n",
      "[140]\ttraining's rmse: 0.221426\tvalid_1's rmse: 0.221973\n",
      "[150]\ttraining's rmse: 0.221098\tvalid_1's rmse: 0.221868\n",
      "[160]\ttraining's rmse: 0.220766\tvalid_1's rmse: 0.221748\n",
      "[170]\ttraining's rmse: 0.220461\tvalid_1's rmse: 0.221641\n",
      "[180]\ttraining's rmse: 0.220169\tvalid_1's rmse: 0.221548\n",
      "[190]\ttraining's rmse: 0.219914\tvalid_1's rmse: 0.22151\n",
      "[200]\ttraining's rmse: 0.219634\tvalid_1's rmse: 0.22143\n",
      "[210]\ttraining's rmse: 0.219372\tvalid_1's rmse: 0.221397\n",
      "[220]\ttraining's rmse: 0.219117\tvalid_1's rmse: 0.22135\n",
      "[230]\ttraining's rmse: 0.218849\tvalid_1's rmse: 0.221316\n",
      "[240]\ttraining's rmse: 0.218605\tvalid_1's rmse: 0.221268\n",
      "[250]\ttraining's rmse: 0.218362\tvalid_1's rmse: 0.221237\n",
      "[260]\ttraining's rmse: 0.218124\tvalid_1's rmse: 0.221238\n",
      "[270]\ttraining's rmse: 0.217871\tvalid_1's rmse: 0.221179\n",
      "[280]\ttraining's rmse: 0.217664\tvalid_1's rmse: 0.221157\n",
      "[290]\ttraining's rmse: 0.217433\tvalid_1's rmse: 0.22113\n",
      "[300]\ttraining's rmse: 0.217167\tvalid_1's rmse: 0.22108\n",
      "[310]\ttraining's rmse: 0.216968\tvalid_1's rmse: 0.221079\n",
      "[320]\ttraining's rmse: 0.216735\tvalid_1's rmse: 0.221065\n",
      "[330]\ttraining's rmse: 0.216503\tvalid_1's rmse: 0.221054\n",
      "[340]\ttraining's rmse: 0.216289\tvalid_1's rmse: 0.221033\n",
      "[350]\ttraining's rmse: 0.216058\tvalid_1's rmse: 0.221023\n",
      "[360]\ttraining's rmse: 0.215849\tvalid_1's rmse: 0.220986\n",
      "[370]\ttraining's rmse: 0.215645\tvalid_1's rmse: 0.220984\n",
      "[380]\ttraining's rmse: 0.215436\tvalid_1's rmse: 0.220955\n",
      "[390]\ttraining's rmse: 0.215204\tvalid_1's rmse: 0.220908\n",
      "[400]\ttraining's rmse: 0.214979\tvalid_1's rmse: 0.220873\n",
      "[410]\ttraining's rmse: 0.214768\tvalid_1's rmse: 0.220831\n",
      "[420]\ttraining's rmse: 0.214556\tvalid_1's rmse: 0.220821\n",
      "[430]\ttraining's rmse: 0.214365\tvalid_1's rmse: 0.220799\n",
      "[440]\ttraining's rmse: 0.214161\tvalid_1's rmse: 0.220768\n",
      "[450]\ttraining's rmse: 0.213946\tvalid_1's rmse: 0.22077\n",
      "[460]\ttraining's rmse: 0.213732\tvalid_1's rmse: 0.220758\n",
      "[470]\ttraining's rmse: 0.213503\tvalid_1's rmse: 0.220728\n",
      "[480]\ttraining's rmse: 0.213301\tvalid_1's rmse: 0.220708\n",
      "[490]\ttraining's rmse: 0.213097\tvalid_1's rmse: 0.220684\n",
      "[500]\ttraining's rmse: 0.212891\tvalid_1's rmse: 0.220659\n",
      "[510]\ttraining's rmse: 0.212696\tvalid_1's rmse: 0.220638\n",
      "[520]\ttraining's rmse: 0.212515\tvalid_1's rmse: 0.220642\n",
      "[530]\ttraining's rmse: 0.212305\tvalid_1's rmse: 0.220643\n",
      "[540]\ttraining's rmse: 0.212133\tvalid_1's rmse: 0.220619\n",
      "[550]\ttraining's rmse: 0.211968\tvalid_1's rmse: 0.220605\n",
      "[560]\ttraining's rmse: 0.21176\tvalid_1's rmse: 0.220579\n",
      "[570]\ttraining's rmse: 0.211563\tvalid_1's rmse: 0.220565\n",
      "[580]\ttraining's rmse: 0.211374\tvalid_1's rmse: 0.22056\n",
      "[590]\ttraining's rmse: 0.211182\tvalid_1's rmse: 0.220549\n",
      "[600]\ttraining's rmse: 0.210971\tvalid_1's rmse: 0.220534\n",
      "[610]\ttraining's rmse: 0.21079\tvalid_1's rmse: 0.220524\n",
      "[620]\ttraining's rmse: 0.210595\tvalid_1's rmse: 0.220511\n",
      "[630]\ttraining's rmse: 0.210415\tvalid_1's rmse: 0.2205\n",
      "[640]\ttraining's rmse: 0.210219\tvalid_1's rmse: 0.220499\n",
      "[650]\ttraining's rmse: 0.210017\tvalid_1's rmse: 0.220479\n",
      "[660]\ttraining's rmse: 0.209827\tvalid_1's rmse: 0.220481\n",
      "[670]\ttraining's rmse: 0.209647\tvalid_1's rmse: 0.220474\n",
      "[680]\ttraining's rmse: 0.209463\tvalid_1's rmse: 0.220464\n",
      "[690]\ttraining's rmse: 0.20927\tvalid_1's rmse: 0.22046\n",
      "[700]\ttraining's rmse: 0.209103\tvalid_1's rmse: 0.220461\n",
      "[710]\ttraining's rmse: 0.208913\tvalid_1's rmse: 0.220448\n",
      "[720]\ttraining's rmse: 0.208735\tvalid_1's rmse: 0.220437\n",
      "[730]\ttraining's rmse: 0.208558\tvalid_1's rmse: 0.220444\n",
      "[740]\ttraining's rmse: 0.208378\tvalid_1's rmse: 0.220444\n",
      "[750]\ttraining's rmse: 0.208192\tvalid_1's rmse: 0.220437\n",
      "[760]\ttraining's rmse: 0.208019\tvalid_1's rmse: 0.220436\n",
      "[770]\ttraining's rmse: 0.207841\tvalid_1's rmse: 0.220441\n",
      "[780]\ttraining's rmse: 0.207667\tvalid_1's rmse: 0.220443\n",
      "[790]\ttraining's rmse: 0.207505\tvalid_1's rmse: 0.220448\n",
      "[800]\ttraining's rmse: 0.207329\tvalid_1's rmse: 0.220441\n",
      "[810]\ttraining's rmse: 0.207142\tvalid_1's rmse: 0.220433\n",
      "[820]\ttraining's rmse: 0.206976\tvalid_1's rmse: 0.220441\n",
      "[830]\ttraining's rmse: 0.206806\tvalid_1's rmse: 0.220427\n",
      "[840]\ttraining's rmse: 0.206621\tvalid_1's rmse: 0.220399\n",
      "[850]\ttraining's rmse: 0.206441\tvalid_1's rmse: 0.220393\n",
      "[860]\ttraining's rmse: 0.206262\tvalid_1's rmse: 0.220384\n",
      "[870]\ttraining's rmse: 0.206102\tvalid_1's rmse: 0.220372\n",
      "[880]\ttraining's rmse: 0.205924\tvalid_1's rmse: 0.220371\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-fe527f3909d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m res_gp = gp_minimize(objective2, space, n_calls=50,\n\u001b[0;32m----> 2\u001b[0;31m                      random_state=0,n_random_starts=10)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-db77f4fb125b>\u001b[0m in \u001b[0;36mobjective2\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    199\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1523\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1524\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res_gp = gp_minimize(objective2, space, n_calls=50,\n",
    "                     random_state=0,n_random_starts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# space = {\n",
    "#         'max_depth': hp.uniform(\"x_max_depth\", 10, 70),\n",
    "#         'min_child_weight': hp.uniform('x_min_child', 0.05, 2),\n",
    "#         'min_data_per_leaf': hp.uniform('min_data_per_leaf', 20, 80),\n",
    "#         'num_leaves': hp.uniform('num_leaves', 20, 80),\n",
    "#         'subsample': hp.uniform('x_subsample', 0.7, 0.95),\n",
    "#         'subsample_freq': hp.uniform('x_subsample_freq', 1, 6),\n",
    "#         'lambda_l1': hp.uniform('x_lambda_l1', 0, 8),\n",
    "#         'lambda_l2': hp.uniform('x_lambda_l2', 0, 8),\n",
    "#         'feature_fraction': hp.uniform('x_feature_fraction', 0.2, 0.8),\n",
    "#         'learning_rate': hp.uniform('x_learning_rate', 0.07, 0.13),\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trials = Trials()\n",
    "# best = fmin(fn=objective_lgb,\n",
    "#             space=space,\n",
    "#             algo=tpe.suggest,\n",
    "#             max_evals=40,\n",
    "#             trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axel/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-d22951faf389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     }\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[0;32m-> 1303\u001b[0;31m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m                 ctypes.byref(self.handle)))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m                                 \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                                 \u001b[0mpredictor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0mcategorical_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_data_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_categorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_label_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_has_header\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_data_from_pandas\u001b[0;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"DataFrame.dtypes for data must be int, float or bool. Did not expect the data types in fields \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mvalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3270\u001b[0m         \u001b[0mwill\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mflot64\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3271\u001b[0m         \"\"\"\n\u001b[0;32m-> 3272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3274\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mas_matrix\u001b[0;34m(self, columns)\u001b[0m\n\u001b[1;32m   3251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3253\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mas_matrix\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m   3448\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3449\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3450\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3452\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_interleave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3475\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3476\u001b[0m             \u001b[0mrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3477\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3478\u001b[0m             \u001b[0mitemmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget_values\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \"\"\"\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"n_estimators\": 2400,\n",
    "        \"max_depth\": 64,\n",
    "        \"min_data_per_leaf\": 57,\n",
    "        \"min_child_weight\": 1.4985683305030175,\n",
    "        \"num_leaves\" : 61,\n",
    "        \"learning_rate\" : 0.07,\n",
    "        \"subsample\" : 0.94,\n",
    "        \"subsample_freq\" : 2,\n",
    "        \"lambda_l1\": 5,\n",
    "        \"lambda_l2\": 4,\n",
    "        \"feature_fraction\" : 0.7, #0.28\n",
    "        \"bagging_seed\" : 16,\n",
    "        \"device\" : \"gpu\",\n",
    "        \"max_bin\": 64,\n",
    "        \"num_threads\": 1\n",
    "    }\n",
    "\n",
    "model = lgb.train(params, train, valid_sets=[train, val], early_stopping_rounds=100, verbose_eval=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "y_pred = np.clip(y_pred, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21962435186683915"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.2214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gain = model.feature_importance('gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f46d1a3ed30>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = pd.DataFrame({'feature':model.feature_name(), 'split':model.feature_importance('split'), 'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=True)\n",
    "plt.figure()\n",
    "ft[['feature','gain']].head(25).plot(kind='barh', x='feature', y='gain', legend=False, figsize=(10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46b436d438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAARiCAYAAABBHPozAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZLUWv8fLCt0AbLbInQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46cfd82208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show(); plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate_feature_importance(model, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_pred = clf.predict(X_test)\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "y_pred = np.clip(y_pred, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission['deal_probability'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission[['item_id','deal_probability']].to_csv('../../../submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
